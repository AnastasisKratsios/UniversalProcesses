{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Conditional Laws for Random-Fields - via:\n",
    "\n",
    "## Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model $\\mathcal{NN}_{1_{\\mathbb{R}^n},\\mathcal{D}}^{\\sigma:\\star}$.\n",
    "\n",
    "---\n",
    "\n",
    "By: [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---\n",
    "\n",
    "## What does this code do?\n",
    "Described in GUI file..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Parameter Dump (easy access for debugging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True\n",
    "\n",
    "problem_dim = 3\n",
    "\n",
    "\n",
    "train_test_ratio = .2\n",
    "N_train_size = 10**2\n",
    "\n",
    "\n",
    "## Monte-Carlo\n",
    "N_Monte_Carlo_Samples = 10**2\n",
    "\n",
    "\n",
    "# Hyper-parameters of Cover\n",
    "delta = 0.01\n",
    "Proportion_per_cluster = .5\n",
    "\n",
    "\n",
    "# Random DNN\n",
    "f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# Random DNN internal noise\n",
    "# f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "Depth_Bayesian_DNN = 2\n",
    "width = 20\n",
    "\n",
    "# Random Dropout applied to trained DNN\n",
    "# f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "Dropout_rate = 0.1\n",
    "\n",
    "# Rough SDE (time 1)\n",
    "# f_unknown_mode = \"Rough_SDE\"\n",
    "\n",
    "# GD with Randomized Input\n",
    "# f_unknown_mode = \"GD_with_randomized_input\"\n",
    "GD_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Algorithm:\n",
    "---\n",
    "- Random $\\delta$-bounded partition on input space,\n",
    "- Train deep classifier on infered classes.\n",
    "---\n",
    "---\n",
    "---\n",
    "## Notes - Why the procedure is so computationally efficient?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auxiliaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Architecture Builder\n",
    "exec(open('Benchmarks_Model_Builder.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\"\n",
    "\n",
    "\n",
    "### Set Seed\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test_size = int(np.round(N_train_size*train_test_ratio,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try initial sampling-type implementation!  It worked nicely..i.e.: centers were given!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Set\n",
    "X_train = np.random.uniform(size=np.array([N_train_size,problem_dim]),low=.5,high=1.5)\n",
    "\n",
    "# Get Testing Set\n",
    "test_set_indices = np.random.choice(range(X_train.shape[0]),N_test_size)\n",
    "X_test = X_train[test_set_indices,]\n",
    "X_test = X_test + np.random.uniform(low=-(delta/np.sqrt(problem_dim)), \n",
    "                                    high = -(delta/np.sqrt(problem_dim)),\n",
    "                                    size = X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate from: $Y=f(X,W)$ \n",
    "- Random DNN (internal noise): \n",
    "    - $f(X,W) = f_{\\text{unknown}}(X+U)$\n",
    "- Random DNN: \n",
    "    - $f(X,W) = f_{\\text{unknown}}(X)+W$\n",
    "    \n",
    "*Non-linear dependance on exhaugenous noise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedastic Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    # Define Simulator\n",
    "    def Simulator(x_in):\n",
    "        var = np.sqrt(np.sum(x_in**2))\n",
    "        # Pushforward\n",
    "        f_x = f_unknown(x_in)\n",
    "        # Apply Noise After\n",
    "        noise = np.random.laplace(0,var,N_Monte_Carlo_Samples)\n",
    "        f_x_noise = np.cos(f_x) + noise\n",
    "        return f_x_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Random_Weights\":\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,) \n",
    "        # Feature Map Layer\n",
    "        W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "    #     Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Bayesian_Dropout\":\n",
    "    # Initialize Drouput Parameters\n",
    "    N_Dropout = int(np.maximum(1,round(width*Dropout_rate)))\n",
    "    \n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            # Apply Random Dropout\n",
    "            random_mask_coordinates_i = np.random.choice(range(width),N_Dropout)\n",
    "            random_mask_coordinates_j = np.random.choice(range(width),N_Dropout)\n",
    "            W_internal[random_mask_coordinates_i,random_mask_coordinates_j] = 0\n",
    "            # Apply Dropped-out layer\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fSDEs\n",
    "Lean the conditional law of $I_{X_1 \\in Ball(0,1)}$ where $X_t$ solves an SDE with fBM driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    #-------------------#\n",
    "    # Build DNN (Drift) #\n",
    "    #-------------------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([problem_dim,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown_drift(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "    \n",
    "    #-----------------#\n",
    "    # Build DNN (Vol) #\n",
    "    #-----------------#\n",
    "    W_feature_vol = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout_vol = np.random.uniform(size=np.array([problem_dim,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop_vol = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list_vol = [W_hidden_loop_vol]\n",
    "        else:\n",
    "            W_hidden_list_vol.append(W_hidden_loop_vol)\n",
    "    def f_unknown_vol(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        x_internal = np.outer(x_internal,x_internal)\n",
    "        x_internal = np.tanh(x_internal)\n",
    "        return x_internal\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        # Random Radius\n",
    "#         rand_radius = np.random.exponential(1)\n",
    "#         left_random_ball_Q = 0\n",
    "        exit_time = 0\n",
    "        # Get fBM path\n",
    "        for d in range(problem_dim):\n",
    "            fBM_gen_loop = (((FBM(n=N_Euler_Steps, hurst=Hurst_Exponent, length=1, method='daviesharte')).fbm())[1:]).reshape(-1,1)\n",
    "            if d == 0:\n",
    "                fBM_gen = fBM_gen_loop\n",
    "            else:\n",
    "                fBM_gen = np.append(fBM_gen,fBM_gen_loop,axis=-1)\n",
    "        # Perform Integral\n",
    "        for t in range(N_Euler_Steps):\n",
    "            drift_update = f_unknown_drift(x_internal)/N_Euler_Steps\n",
    "            vol_update = f_unknown_vol(x_internal)\n",
    "            x_internal = x_internal + drift_update + np.matmul(vol_update,fBM_gen[t,])\n",
    "            \n",
    "            # Test if the process has left the ball\n",
    "            #left_random_ball_Q = max(left_random_ball_Q,(np.sqrt(np.sum((x_internal - x.reshape(-1,))**2)) <= rand_radius)*1)        \n",
    "            if np.sqrt(np.sum((x_internal - x.reshape(-1,))**2)) <= 1:\n",
    "                exit_time = t/N_Euler_Steps\n",
    "            else:\n",
    "                break\n",
    "        return exit_time\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide on Problem's output dimension\n",
    "(This is only relevant for SDE Example which is multi-dimensional in the output space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "    output_dim = 1\n",
    "else: \n",
    "    output_dim = problem_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with random initialization:\n",
    "$$\n",
    "Y_x\\triangleq \\hat{f}_{\\theta_T}(x),\\qquad \\theta_{t+1} \\triangleq \\theta_t - \\nabla \\sum_{x\\in \\mathbb{X}} \\|\\hat{f}_{\\theta_t}(x)-f(x)\\|, \\qquad \\theta_0 \\sim N_d(0,1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"GD_with_randomized_input\":\n",
    "    # Auxiliary Initialization(s)\n",
    "    Train_step_proportion = 1-train_test_ratio\n",
    "\n",
    "    #--------------#\n",
    "    # Prepare Data #\n",
    "    #--------------#\n",
    "    # Read Dataset\n",
    "    crypto_data = pd.read_csv('inputs/data/cryptocurrencies/Cryptos_All_in_one.csv')\n",
    "    # Format Date-Time\n",
    "    crypto_data['Date'] = pd.to_datetime(crypto_data['Date'],infer_datetime_format=True)\n",
    "    crypto_data.set_index('Date', drop=True, inplace=True)\n",
    "    crypto_data.index.names = [None]\n",
    "\n",
    "    # Remove Missing Data\n",
    "    crypto_data = crypto_data[crypto_data.isna().any(axis=1)==False]\n",
    "\n",
    "    # Get Returns\n",
    "    crypto_returns = crypto_data.diff().iloc[1:]\n",
    "\n",
    "    # Parse Regressors from Targets\n",
    "    ## Get Regression Targets\n",
    "    crypto_target_data = pd.DataFrame({'BITCOIN-closing':crypto_returns['BITCOIN-Close']})\n",
    "    ## Get Regressors\n",
    "    crypto_data_returns = crypto_returns.drop('BITCOIN-Close', axis=1)  \n",
    "\n",
    "    #-------------#\n",
    "    # Subset Data #\n",
    "    #-------------#\n",
    "    # Get indices\n",
    "    N_train_step = int(round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    N_test_set = int(crypto_data_returns.shape[0] - round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "    # # Get Datasets\n",
    "    X_train = crypto_data_returns[:N_train_step]\n",
    "    X_test = crypto_data_returns[-N_test_set:]\n",
    "\n",
    "    ## Coerce into format used in benchmark model(s)\n",
    "    data_x = X_train\n",
    "    data_x_test = X_test\n",
    "    # Get Targets \n",
    "    data_y = crypto_target_data[:N_train_step]\n",
    "    data_y_test = crypto_target_data[-N_test_set:]\n",
    "\n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    data_x = scaler.fit_transform(data_x)\n",
    "    data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "    # # Update User\n",
    "    print('#================================================#')\n",
    "    print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "    print('#================================================#')\n",
    "\n",
    "    # # Set First Run to Off\n",
    "    First_run = False\n",
    "\n",
    "    #-----------#\n",
    "    # Plot Data #\n",
    "    #-----------#\n",
    "    fig = crypto_data_returns.plot(figsize=(16, 16))\n",
    "    fig.get_legend().remove()\n",
    "    plt.title(\"Crypto_Market Returns\")\n",
    "\n",
    "    # SAVE Figure to .eps\n",
    "    plt.savefig('./outputs/plots/Crypto_Data_returns.pdf', format='pdf')\n",
    "\n",
    "    # Redefine Meta-Parameters #\n",
    "    #--------------------------#\n",
    "    # Redefine Training Set inputs and ys to train DNN:\n",
    "    data_y_to_train_DNN_on = (data_y.to_numpy()).reshape(-1,)\n",
    "    X_train = data_x\n",
    "    X_test = data_x_test\n",
    "    problem_dim=data_x.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize Target Function #\n",
    "    #----------------------------#\n",
    "    # Initialize DNN to train\n",
    "    f_model = get_ffNN(width, Depth_Bayesian_DNN, 0.001, problem_dim, 1)\n",
    "\n",
    "    # Define Stochastic Prediction Function:\n",
    "    def f_unknown():\n",
    "        f_model.fit(data_x,data_y_to_train_DNN_on,epochs = GD_epochs)\n",
    "        f_x_trained_with_random_initialization_x_train = f_model.predict(X_train)\n",
    "        f_x_trained_with_random_initialization_x_test = f_model.predict(X_test)\n",
    "        return f_x_trained_with_random_initialization_x_train, f_x_trained_with_random_initialization_x_test\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9916.78it/s]\n"
     ]
    }
   ],
   "source": [
    "if f_unknown_mode != \"GD_with_randomized_input\":\n",
    "    for i in tqdm(range(X_train.shape[0])):\n",
    "        # Put Datum\n",
    "        x_loop = X_train[i,]\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_train = y_loop\n",
    "            Y_train_mean_emp = np.mean(y_loop)\n",
    "    #         Y_train_var_emp = np.mean((y_loop - np.mean(y_loop))**2)\n",
    "        else:\n",
    "            Y_train = np.append(Y_train,y_loop,axis=0)\n",
    "            Y_train_mean_emp = np.append(Y_train_mean_emp,np.mean(y_loop))\n",
    "    #         Y_train_var_emp = np.append(Y_train_var_emp,np.mean((y_loop - np.mean(y_loop))**2))\n",
    "    # Join mean and Variance Training Data\n",
    "    # Y_train_var_emp = np.append(Y_train_mean_emp.reshape(-1,1),Y_train_var_emp.reshape(-1,1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 5241.57it/s]\n"
     ]
    }
   ],
   "source": [
    "if f_unknown_mode != \"GD_with_randomized_input\":\n",
    "    # Start Timer\n",
    "    Test_Set_PredictionTime_MC = time.time()\n",
    "\n",
    "    # Generate Data\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        # Put Datum\n",
    "        x_loop = X_test[i,]\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_test = y_loop\n",
    "        else:\n",
    "            Y_test = np.append(Y_test,y_loop,axis=0)\n",
    "\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do it differently for *\"GD_with_randomized_input\"* Example.\n",
    "This variant is more efficient in the case of the gradient-descent with randomized initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implemention of the GD algorithm is more efficient (but this only holds for the GD Monte-Carlo method):\n",
    "if f_unknown_mode == \"GD_with_randomized_input\":\n",
    "    # Start Timer\n",
    "    Test_Set_PredictionTime_MC = time.time()\n",
    "    for j_MC in range(N_Monte_Carlo_Samples):\n",
    "        # MC of SGD\n",
    "        Y_train_loop,Y_test_loop = f_unknown()\n",
    "        # Update Dataset\n",
    "        if j_MC == 0:\n",
    "            Y_train = Y_train_loop\n",
    "            Y_test = Y_test_loop\n",
    "        else:\n",
    "            Y_train = np.append(Y_train,Y_train_loop,axis=1)\n",
    "            Y_test = np.append(Y_test,Y_test_loop,axis=1)\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC\n",
    "    \n",
    "## Get means for mean-prediction models\n",
    "    ## Training\n",
    "    for i in tqdm(range(X_train.shape[0])):\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = Y_train[i,]\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_train_mean_emp = np.mean(y_loop)\n",
    "        else:\n",
    "            Y_train_mean_emp = np.append(Y_train_mean_emp,np.mean(y_loop))\n",
    "    ## Testing\n",
    "    ### Continue Timer\n",
    "    Test_Set_PredictionTime_MC2 = time.time()\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop_test = Y_test[i,]\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_test_mean_emp = np.mean(y_loop_test)\n",
    "        else:\n",
    "            Y_test_mean_emp = np.append(Y_test_mean_emp,np.mean(y_loop_test))\n",
    "    \n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = (time.time() - Test_Set_PredictionTime_MC2) + Test_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the measures $\\hat{\\mu}_n$ via Barycenters...*aka \"K-Means\"*!\n",
    "- We first identify N-balls in the input space (which is equivalent to identifying N balls in the output space by uniform continuity)\n",
    "- We then project each of the centers of these balls onto the nearest element of the training set.\n",
    "- The corresponing (observed) $f(x)\\in \\mathcal{P}_1(\\mathbb{R})$ are our $\\hat{\\mu}_n$ (for $n=1,\\dots,N$).\n",
    "\n",
    "\n",
    "**NB:** *This is essentially what is done in the proof, exect there, we have access to the correct N and the optimal balls (outside the training dataset)...which we clearly do not here...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize k_means\n",
    "N_Quantizers_to_parameterize = int(np.maximum(2,round(Proportion_per_cluster*X_train.shape[0])))\n",
    "kmeans = KMeans(n_clusters=N_Quantizers_to_parameterize, random_state=0).fit(X_train)\n",
    "# Get Classes\n",
    "Train_classes = np.array(pd.get_dummies(kmeans.labels_))\n",
    "# Get Center Measures\n",
    "Barycenters_Array_x = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 15466.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(Barycenters_Array_x.shape[0])):\n",
    "    # Identify Nearest Datapoint to a ith Barycenter\n",
    "    #------------------------------------------------------------------------------------------------------#\n",
    "    ## Get Barycenter \"out of sample\" in X (NB there is no data-leakage since we know nothing about Y!)\n",
    "    Bar_x_loop = Barycenters_Array_x[i,]\n",
    "    ## Project Barycenter onto testset\n",
    "    distances = np.sum(np.abs(X_train-Bar_x_loop.reshape(-1,)),axis=1)\n",
    "    Bar_x_loop = X_train[np.argmin(distances),]\n",
    "    #------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "    # Product Monte-Carlo Sample for Input\n",
    "    Bar_y_loop = (np.array(Y_train[np.argmin(distances),],dtype=float)).reshape(-1,1)\n",
    "\n",
    "    # Update Dataset\n",
    "    if i == 0:\n",
    "        Barycenters_Array = Bar_y_loop\n",
    "    else:\n",
    "        Barycenters_Array = np.append(Barycenters_Array,Bar_y_loop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "Type_A_timer_Begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train a deep (feed-forward) classifier:\n",
    "$$\n",
    "\\hat{f}\\triangleq \\operatorname{Softmax}_N\\circ W_J\\circ \\sigma \\bullet \\dots \\sigma \\bullet W_1,\n",
    "$$\n",
    "to identify which barycenter we are closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Packages and CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Re-Load Classifier Function(s)\n",
    "exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Training Classifer Portion of Type-A Model\n",
      "==========================================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    3.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9276 - accuracy: 0.0100\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9197 - accuracy: 0.0100\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9131 - accuracy: 0.0200\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.9073 - accuracy: 0.0200\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9013 - accuracy: 0.0200\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8957 - accuracy: 0.0300\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8903 - accuracy: 0.0300\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8848 - accuracy: 0.0300\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8795 - accuracy: 0.0300\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8746 - accuracy: 0.0300\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8694 - accuracy: 0.0300\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8642 - accuracy: 0.0500\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8592 - accuracy: 0.0600\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8541 - accuracy: 0.0800\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8492 - accuracy: 0.0800\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8440 - accuracy: 0.0800\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8392 - accuracy: 0.0800\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8342 - accuracy: 0.0800\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8293 - accuracy: 0.0800\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8243 - accuracy: 0.0900\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8191 - accuracy: 0.0900\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.8135 - accuracy: 0.0800\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8081 - accuracy: 0.0800\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8025 - accuracy: 0.0800\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7970 - accuracy: 0.0800\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7918 - accuracy: 0.0800\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7863 - accuracy: 0.0800\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7808 - accuracy: 0.0800\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7750 - accuracy: 0.0700\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7694 - accuracy: 0.0700\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7631 - accuracy: 0.0700\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7572 - accuracy: 0.0700\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7504 - accuracy: 0.0800\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7442 - accuracy: 0.0800\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7377 - accuracy: 0.0800\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7316 - accuracy: 0.0800\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7253 - accuracy: 0.0800\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7190 - accuracy: 0.0800\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.7129 - accuracy: 0.0800\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.7066 - accuracy: 0.0700\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7001 - accuracy: 0.0700\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6939 - accuracy: 0.0700\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6868 - accuracy: 0.0700\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6799 - accuracy: 0.0700\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6731 - accuracy: 0.0700\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6663 - accuracy: 0.0700\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6593 - accuracy: 0.0700\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6521 - accuracy: 0.0700\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.6446 - accuracy: 0.0700\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6375 - accuracy: 0.0500\n",
      "4/4 [==============================] - 0s 819us/step\n",
      "1/1 [==============================] - 0s 859us/step\n",
      "===============================================\n",
      "Training Classifer Portion of Type Model: Done!\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==========================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "param_grid_Deep_Classifier['output_dim'] = [N_Quantizers_to_parameterize]\n",
    "\n",
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier, timer_output = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter = n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = Train_classes,\n",
    "                                                                                                        X_test = X_test)\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"Training Classifer Portion of Type Model: Done!\")\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predicted Quantized Distributions\n",
    "- Each *row* of \"Predicted_Weights\" is the $\\beta\\in \\Delta_N$.\n",
    "- Each *Column* of \"Barycenters_Array\" denotes the $x_1,\\dots,x_N$ making up the points of the corresponding empirical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Empirical Weights\n",
    "empirical_weights = (np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples).reshape(-1,)\n",
    "\n",
    "for i in range(Barycenters_Array.shape[0]):\n",
    "    if i == 0:\n",
    "        points_of_mass = Barycenters_Array[i,]\n",
    "    else:\n",
    "        points_of_mass = np.append(points_of_mass,Barycenters_Array[i,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode != \"GD_with_randomized_input\":\n",
    "    # Get Noisless Mean\n",
    "    direct_facts = np.apply_along_axis(f_unknown, 1, X_train)\n",
    "    direct_facts_test = np.apply_along_axis(f_unknown, 1, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Error(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run Evaluation.ipynb\n",
    "exec(open('Evaluation.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute *Training* Error(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dimensional transport-problem initializations\n",
    "if output_dim == 1:\n",
    "    # Externally Update Empirical Weights for multi-dimensional case\n",
    "    empirical_weights = np.full((N_Monte_Carlo_Samples,),1/N_Monte_Carlo_Samples)\n",
    "    # Also Initialize\n",
    "    Sinkhorn_regularization = 0.01\n",
    "    \n",
    "# Define Transport Solver\n",
    "def transport_dist(x_source,w_source,x_sink,w_sink,output_dim):\n",
    "    # Decide which problem to solve (1D or multi-D)?\n",
    "    if output_dim == 1:\n",
    "        OT_out = ot.emd2_1d(x_source,\n",
    "                            x_sink,\n",
    "                            w_source,\n",
    "                            w_sink)\n",
    "    else:\n",
    "        # COERCSION\n",
    "        ## Update Source Distribution\n",
    "        x_source = points_of_mass.reshape(-1,output_dim)\n",
    "        ## Update Sink Distribution\n",
    "        x_sink = np.array(Y_train[i,]).reshape(-1,output_dim)\n",
    "        \n",
    "        # COMPUTE: Sinkhorn-Regularized Transport Problem of: https://papers.nips.cc/paper/2013/hash/af21d0c97db2e27e13572cbf59eb343d-Abstract.html\n",
    "        OT_out = ot.bregman.empirical_sinkhorn2(X_s = x_source, \n",
    "                                                X_t = x_sink,\n",
    "                                                a = w_source, \n",
    "                                                b = w_sink, \n",
    "                                                reg=0.01, \n",
    "                                                verbose=False)\n",
    "        # COERSION\n",
    "        OT_out = float(OT_out[0])\n",
    "    # Return (regularized?) Transport Distance\n",
    "    return OT_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:00<00:00, 326.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------#\n",
      " Get Training Error(s)\n",
      "#--------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 349.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#--------------------#\")\n",
    "print(\" Get Training Error(s)\")\n",
    "print(\"#--------------------#\")\n",
    "for i in tqdm(range((X_train.shape[0]))):\n",
    "    for j in range(N_Quantizers_to_parameterize):\n",
    "        b_loop = np.repeat(predicted_classes_train[i,j],N_Monte_Carlo_Samples)\n",
    "        if j == 0:\n",
    "            b = b_loop\n",
    "        else:\n",
    "            b = np.append(b,b_loop)\n",
    "        b = b.reshape(-1,1)\n",
    "        b = b\n",
    "    b = np.array(b,dtype=float).reshape(-1,)\n",
    "    b = b/N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Compute Error(s)\n",
    "    ## W1\n",
    "    W1_loop = transport_dist(x_source = points_of_mass,\n",
    "               w_source = b,\n",
    "               x_sink = np.array(Y_train[i,]).reshape(-1,),\n",
    "               w_sink = empirical_weights,\n",
    "               output_dim = output_dim)\n",
    "    \n",
    "    ## M1\n",
    "    Mu_hat = np.sum(b*(points_of_mass))\n",
    "    Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Mu = direct_facts[i,]\n",
    "    else:\n",
    "        Mu = Mu_MC\n",
    "        \n",
    "    ### Error(s)\n",
    "    Mean_loop = (Mu_hat-Mu)\n",
    "    Mean_loop_MC = (Mu_hat-Mu_MC)\n",
    "    \n",
    "    ## Variance\n",
    "    Var_hat = np.sum(((points_of_mass-Mu_hat)**2)*b)\n",
    "    Var_MC = np.mean(np.array(Y_train[i]-Mu_MC)**2)\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Var = 2*np.sum(X_train[i,]**2)\n",
    "    else:\n",
    "        Var = Var_MC     \n",
    "    ### Error(s)\n",
    "    Var_loop = np.abs(Var_hat-Var)\n",
    "    Var_loop_MC = np.abs(Var_MC-Var)\n",
    "        \n",
    "    # Skewness\n",
    "    Skewness_hat = np.sum((((points_of_mass-Mu_hat)/Var_hat)**3)*b)\n",
    "    Skewness_MC = np.mean((np.array(Y_train[i]-Mu_MC)/Var_MC)**3)\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Skewness = 0\n",
    "    else:\n",
    "        Skewness = Skewness_MC\n",
    "    ### Error(s)\n",
    "    Skewness_loop = np.abs(Skewness_hat-Skewness)\n",
    "    Skewness_loop_MC = np.abs(Skewness_MC-Skewness)\n",
    "    \n",
    "    # Skewness\n",
    "    Ex_Kurtosis_hat = np.sum((((points_of_mass-Mu_hat)/Var_hat)**4)*b) - 3\n",
    "    Ex_Kurtosis_MC = np.mean((np.array(Y_train[i]-Mu_MC)/Var_MC)**4) - 3\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Ex_Kurtosis = 3\n",
    "    else:\n",
    "        Ex_Kurtosis = Ex_Kurtosis_MC\n",
    "    ### Error(s)\n",
    "    Ex_Kurtosis_loop = np.abs(Ex_Kurtosis-Ex_Kurtosis_hat)\n",
    "    Ex_Kurtosis_loop_MC = np.abs(Ex_Kurtosis-Ex_Kurtosis_MC)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get Higher Moments Loss\n",
    "    Higher_momentserrors_loop,Higher_MC_momentserrors_loop = Higher_Moments_Loss(b,np.array(Y_train[i,]))\n",
    "    Higher_Moments_Errors_loop = np.abs(Higher_momentserrors_loop-Higher_MC_momentserrors_loop)\n",
    "    \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_errors = W1_loop\n",
    "        # Moments\n",
    "        ## DNM\n",
    "        Mean_errors =  Mean_loop\n",
    "        Var_errors = Var_loop\n",
    "        Skewness_errors = Skewness_loop\n",
    "        Ex_Kurtosis_errors = Ex_Kurtosis_loop\n",
    "        ## Monte-Carlo\n",
    "        Mean_errors_MC =  Mean_loop_MC\n",
    "        Var_errors_MC = Var_loop_MC\n",
    "        Skewness_errors_MC = Skewness_loop_MC\n",
    "        Ex_Kurtosis_errors_MC = Ex_Kurtosis_loop_MC\n",
    "        # Higher Moments\n",
    "        Higher_Moments_Errors = Higher_Moments_Errors_loop\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_errors = np.append(W1_errors,W1_loop)\n",
    "        # Moments\n",
    "        ## DNM\n",
    "        Mean_errors =  np.append(Mean_errors,Mean_loop)\n",
    "        Var_errors = np.append(Var_errors,Var_loop)\n",
    "        Skewness_errors = np.append(Skewness_errors,Skewness_loop)\n",
    "        Ex_Kurtosis_errors = np.append(Ex_Kurtosis_errors,Ex_Kurtosis_loop)\n",
    "        ## Monte-Carlo\n",
    "        Mean_errors_MC =  np.append(Mean_errors_MC,Mean_loop_MC)\n",
    "        Var_errors_MC = np.append(Var_errors_MC,Var_loop_MC)\n",
    "        Skewness_errors_MC = np.append(Skewness_errors_MC,Skewness_loop_MC)\n",
    "        Ex_Kurtosis_errors_MC = np.append(Ex_Kurtosis_errors_MC,Ex_Kurtosis_loop_MC)\n",
    "        # Higher Moments\n",
    "        Higher_Moments_Errors = np.append(Higher_Moments_Errors,Higher_Moments_Errors_loop)\n",
    "        \n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute *Testing* Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 211.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------#\n",
      " Get Test Error(s)\n",
      "#----------------#\n",
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#----------------#\")\n",
    "print(\" Get Test Error(s)\")\n",
    "print(\"#----------------#\")\n",
    "for i in tqdm(range((X_test.shape[0]))):\n",
    "    for j in range(N_Quantizers_to_parameterize):\n",
    "        b_loop_test = np.repeat(predicted_classes_test[i,j],N_Monte_Carlo_Samples)\n",
    "        if j == 0:\n",
    "            b_test = b_loop_test\n",
    "        else:\n",
    "            b_test = np.append(b,b_loop)\n",
    "        b_test = b_test.reshape(-1,1)\n",
    "    b_test = np.array(b,dtype=float).reshape(-1,)\n",
    "    b_test = b/N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Compute Error(s)\n",
    "    ## W1\n",
    "    W1_loop_test = transport_dist(x_source = points_of_mass,\n",
    "                                  w_source = b,\n",
    "                                  x_sink = np.array(Y_test[i,]).reshape(-1,),\n",
    "                                  w_sink = empirical_weights,\n",
    "                                  output_dim = output_dim)\n",
    "    \n",
    "    ## M1\n",
    "    Mu_hat_test = np.sum(b_test*(points_of_mass))\n",
    "    Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Mu_test = direct_facts_test[i,]\n",
    "    else:\n",
    "        Mu_test = Mu_MC_test\n",
    "    ### Error(s)\n",
    "    Mean_loop_test = (Mu_hat_test-Mu_test)\n",
    "    Mean_loop_MC_test = (Mu_hat_test-Mu_MC_test)\n",
    "    \n",
    "    ## M2\n",
    "    Var_hat_test = np.sum(((points_of_mass-Mu_hat_test)**2)*b)\n",
    "    Var_MC_test = np.mean(np.array(Y_test[i]-Mu_MC)**2)\n",
    "    if f_unknown_mode == \"Rough_SDE\":\n",
    "        Var_test = 2*np.sum(X_test[i,]**2)\n",
    "    else:\n",
    "        Var_test = Var_MC\n",
    "    \n",
    "    ### Error(s)\n",
    "    Var_loop_test = np.abs(Var_hat_test-Var_test)\n",
    "    Var_loop_MC_test = np.abs(Var_MC_test-Var_test)\n",
    "    \n",
    "    # Skewness\n",
    "    Skewness_hat_test = np.sum((((points_of_mass-Mu_hat_test)/Var_hat_test)**3)*b)\n",
    "    Skewness_MC_test = np.mean((np.array(Y_test[i]-Mu_MC_test)/Var_MC_test)**3)\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Skewness_test = 0\n",
    "    else:\n",
    "        Skewness_test = Skewness_MC_test\n",
    "    ### Error(s)\n",
    "    Skewness_loop_test = np.abs(Skewness_hat_test-Skewness_test)\n",
    "    Skewness_loop_MC_test = np.abs(Skewness_MC_test-Skewness_test)\n",
    "    \n",
    "    # Skewness\n",
    "    Ex_Kurtosis_hat_test = np.sum((((points_of_mass-Mu_hat_test)/Var_hat_test)**4)*b) - 3\n",
    "    Ex_Kurtosis_MC_test = np.mean((np.array(Y_test[i]-Mu_MC_test)/Var_MC_test)**4) - 3\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Ex_Kurtosis_test = 3\n",
    "    else:\n",
    "        Ex_Kurtosis_test = Ex_Kurtosis_MC_test\n",
    "    ### Error(s)\n",
    "    Ex_Kurtosis_loop_test = np.abs(Ex_Kurtosis_test-Ex_Kurtosis_hat_test)\n",
    "    Ex_Kurtosis_loop_MC_test = np.abs(Ex_Kurtosis_test-Ex_Kurtosis_MC_test)\n",
    "    \n",
    "    \n",
    "    # Get Higher Moments Loss\n",
    "    Higher_momentserrors_test_loop,Higher_MC_momentserrors_test_loop = Higher_Moments_Loss(b,np.array(Y_test[i,]))\n",
    "    Higher_Moments_Errors_test_loop = np.abs(Higher_momentserrors_test_loop-Higher_MC_momentserrors_test_loop)\n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_errors_test = W1_loop_test\n",
    "        # Moments\n",
    "        ## DNM\n",
    "        Mean_errors_test =  Mean_loop_test\n",
    "        Var_errors_test = Var_loop_test\n",
    "        Skewness_errors_test = Skewness_loop_test\n",
    "        Ex_Kurtosis_errors_test = Ex_Kurtosis_loop_test\n",
    "        ## Monte-Carlo\n",
    "        Mean_errors_MC_test =  Mean_loop_MC_test\n",
    "        Var_errors_MC_test = Var_loop_MC_test\n",
    "        Skewness_errors_MC_test = Skewness_loop_MC_test\n",
    "        Ex_Kurtosis_errors_MC_test = Ex_Kurtosis_loop_MC_test\n",
    "        # Higher Moments\n",
    "        Higher_Moments_test_Errors = Higher_Moments_Errors_test_loop\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_errors_test = np.append(W1_errors_test,W1_loop_test)\n",
    "        # Moments\n",
    "        ## DNM\n",
    "        Mean_errors_test =  np.append(Mean_errors_test,Mean_loop_test)\n",
    "        Var_errors_test = np.append(Var_errors_test,Var_loop_test)\n",
    "        Skewness_errors_test = np.append(Skewness_errors_test,Skewness_loop_test)\n",
    "        Ex_Kurtosis_errors_test = np.append(Ex_Kurtosis_errors_test,Ex_Kurtosis_loop_test)\n",
    "        ## Monte-Carlo\n",
    "        Mean_errors_MC_test =  np.append(Mean_errors_MC_test,Mean_loop_MC_test)\n",
    "        Var_errors_MC_test = np.append(Var_errors_MC_test,Var_loop_MC_test)\n",
    "        Skewness_errors_MC_test = np.append(Skewness_errors_MC_test,Skewness_loop_MC_test)\n",
    "        Ex_Kurtosis_errors_MC_test = np.append(Ex_Kurtosis_errors_MC_test,Ex_Kurtosis_loop_MC_test)\n",
    "        # Higher Moments\n",
    "        Higher_Moments_test_Errors = np.append(Higher_Moments_test_Errors,Higher_Moments_Errors_test_loop)\n",
    "        \n",
    "        \n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Type_A_timer_end = time.time()\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Time_Lapse_Model_A = Type_A_timer_end - Type_A_timer_Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
