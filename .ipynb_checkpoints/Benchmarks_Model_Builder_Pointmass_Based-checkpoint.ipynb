{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### DEGUBBING MENU:\n",
    "\n",
    "\n",
    "# trial_run = True\n",
    "# N_train_size= 5\n",
    "# train_test_ratio = .5\n",
    "# N_Monte_Carlo_Samples = 100\n",
    "# # # Random DNN\n",
    "# # f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# # # Random DNN internal noise\n",
    "# # # f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "# Depth_Bayesian_DNN = 2\n",
    "# width = 50\n",
    "\n",
    "# # # Random Dropout applied to trained DNN\n",
    "# f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "# Dropout_rate = 0.1\n",
    "\n",
    "# # GD with Randomized Input\n",
    "# # f_unknown_mode = \"GD_with_randomized_input\"\n",
    "# GD_epochs = 100\n",
    "\n",
    "# # SDE with fractional Driver\n",
    "# f_unknown_mode = \"Rough_SDE\"\n",
    "# N_Euler_Steps = 10**1\n",
    "# Hurst_Exponent = 0.5\n",
    "# problem_dim = 4\n",
    "\n",
    "# # Hyper-parameters of Cover\n",
    "# delta = 0.01\n",
    "# Proportion_per_cluster = .75\n",
    "\n",
    "# # %run Loader.ipynb\n",
    "# exec(open('Loader.py').read())\n",
    "# # Load Packages/Modules\n",
    "# exec(open('Init_Dump.py').read())\n",
    "# import time as time #<- Note sure why...but its always seems to need 'its own special loading...'\n",
    "\n",
    "# # %run Data_Simulator_and_Parser.ipynb\n",
    "# exec(open('Data_Simulator_and_Parser.py').read())\n",
    "\n",
    "# print(\"------------------------------\")\n",
    "# print(\"Running script for main model!\")\n",
    "# print(\"------------------------------\")\n",
    "# # %run Universal_Measure_Valued_Networks_Backend.ipynb\n",
    "# exec(open('Universal_Measure_Valued_Networks_Backend.py').read())\n",
    "\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"Done: Running script for main model!\")\n",
    "# print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================#\n",
    "# Elastic Net Version #\n",
    "#=====================#\n",
    "# Block warnings that spam when performing coordinate descent (by default) in 1-d.\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Initialize Elastic Net Regularization Model\n",
    "ENET_reg = ElasticNetCV(cv=5, random_state=0, alphas = np.linspace(0,(10**2),(10**2)),\n",
    "                           l1_ratio=np.linspace(0,1,(10**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ffNN Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "    # Deep Feature Network\n",
    "    ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = ffNN_CVer.predict(X_train)\n",
    "    \n",
    "    eval_time_ffNN = time.time()\n",
    "    y_hat_test = ffNN_CVer.predict(X_test)\n",
    "    eval_time_ffNN = time.time() - eval_time_ffNN\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = ffNN_CVer.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    \n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test, N_params_best_ffNN, eval_time_ffNN\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Deep Feature Builder - Ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GBRF(X_train,X_test,y_train):\n",
    "\n",
    "    # Run Random Forest Util\n",
    "    rand_forest_model_grad_boosted = GradientBoostingRegressor()\n",
    "\n",
    "    # Grid-Search CV\n",
    "    Random_Forest_GridSearch = RandomizedSearchCV(estimator = rand_forest_model_grad_boosted,\n",
    "                                                  n_iter=n_iter_trees,\n",
    "                                                  cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                                  param_distributions=Rand_Forest_Grid,\n",
    "                                                  return_train_score=True,\n",
    "                                                  random_state=2020,\n",
    "                                                  verbose=10,\n",
    "                                                  n_jobs=n_jobs)\n",
    "\n",
    "    random_forest_trained = Random_Forest_GridSearch.fit(X_train,y_train)\n",
    "    random_forest_trained = random_forest_trained.best_estimator_\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Write: Model, Results, and Best Hyper-Parameters #\n",
    "    #--------------------------------------------------#\n",
    "\n",
    "    # Save Model\n",
    "    # pickle.dump(random_forest_trained, open('./outputs/models/Gradient_Boosted_Tree/Gradient_Boosted_Tree_Best.pkl','wb'))\n",
    "\n",
    "    # Save Readings\n",
    "    cur_path = os.path.expanduser('./outputs/tables/best_params_Gradient_Boosted_Tree.txt')\n",
    "    with open(cur_path, \"w\") as f:\n",
    "        f.write(str(Random_Forest_GridSearch.best_params_))\n",
    "\n",
    "    best_params_table_tree = pd.DataFrame({'N Estimators': [Random_Forest_GridSearch.best_params_['n_estimators']],\n",
    "                                        'Min Samples Leaf': [Random_Forest_GridSearch.best_params_['min_samples_leaf']],\n",
    "                                        'Learning Rate': [Random_Forest_GridSearch.best_params_['learning_rate']],\n",
    "                                        'Max Depth': [Random_Forest_GridSearch.best_params_['max_depth']],\n",
    "                                        })\n",
    "    \n",
    "    # Count Number of Parameters in Random Forest Regressor\n",
    "    N_tot_params_per_tree = [ (x[0].tree_.node_count)*random_forest_trained.n_features_ for x in random_forest_trained.estimators_]\n",
    "    N_tot_params_in_forest = sum(N_tot_params_per_tree)\n",
    "    best_params_table_tree['N_parameters'] = [N_tot_params_in_forest]\n",
    "    # Write Best Parameter(s)\n",
    "    best_params_table_tree.to_latex('./outputs/tables/Best_params_table_Gradient_Boosted_Tree.txt')\n",
    "    #---------------------------------------------#\n",
    "    \n",
    "    # Generate Prediction(s) #\n",
    "    #------------------------#\n",
    "    y_train_hat_random_forest_Gradient_boosting = random_forest_trained.predict(X_train)\n",
    "    eval_time_GBRF = time.time()\n",
    "    y_test_hat_random_forest_Gradient_boosting = random_forest_trained.predict(X_test)\n",
    "    eval_time_GBRF = time.time() - eval_time_GBRF\n",
    "    \n",
    "    # Return Predictions #\n",
    "    #--------------------#\n",
    "    return y_train_hat_random_forest_Gradient_boosting, y_test_hat_random_forest_Gradient_boosting, random_forest_trained, N_tot_params_in_forest, eval_time_GBRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Kernel_Ridge_Regressor(data_x_in,data_x_test_in,data_y_in):\n",
    "    # Imports\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "    # Initialize Randomized Gridsearch\n",
    "    kernel_ridge_CVer = RandomizedSearchCV(estimator = KernelRidge(),\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                           param_distributions=param_grid_kernel_Ridge,\n",
    "                                           n_iter=n_iter,\n",
    "                                           return_train_score=True,\n",
    "                                           random_state=2020,\n",
    "                                           verbose=10)\n",
    "    kernel_ridge_CVer.fit(data_x_in,data_y_in)\n",
    "\n",
    "    # Get best Kernel ridge regressor\n",
    "    best_kernel_ridge_model = kernel_ridge_CVer.best_estimator_\n",
    "\n",
    "    # Get Predictions\n",
    "    f_hat_kernel_ridge_train = best_kernel_ridge_model.predict(data_x_in)\n",
    "    eval_time_kr = time.time()\n",
    "    f_hat_kernel_ridge_test = best_kernel_ridge_model.predict(data_x_test_in)\n",
    "    eval_time_kr = time.time() - eval_time_kr\n",
    "\n",
    "    # Count Parameters of best model\n",
    "    N_params_kR = len(best_kernel_ridge_model.get_params()) + 2*problem_dim\n",
    "    \n",
    "    Path('./outputs/models/Kernel_Ridge/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(kernel_ridge_CVer.best_params_,orient='index').to_latex(\"./outputs/models/Kernel_Ridge/Best_Parameters.tex\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return\n",
    "    return f_hat_kernel_ridge_train, f_hat_kernel_ridge_test, best_kernel_ridge_model, N_params_kR, eval_time_kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models and Get Prediction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run Evaluation.ipynb\n",
    "exec(open('Evaluation.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Training: ENET\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:09<00:27,  9.05s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ENET_predict_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-445054eece49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mENET_eval_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENET_eval_time_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mENET_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENET_predict_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mENET_predict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mENET_predict_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENET_predict_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mENET_predict_loop_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mENET_N_Params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENET_N_Params\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENET_predict_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Start Timer\n",
    "Timer_ENET = time.time()\n",
    "\n",
    "print(\"--------------\")\n",
    "print(\"Training: ENET\")\n",
    "print(\"--------------\")\n",
    "\n",
    "if output_dim == 1:\n",
    "    # Fit Elastic Net Model\n",
    "    ENET_reg.fit(X_train,Y_train_mean_emp)\n",
    "\n",
    "    # Get Predictions\n",
    "    ENET_predict = ENET_reg.predict(X_train)\n",
    "    ENET_eval_time = time.time()\n",
    "    ENET_predict_test = ENET_reg.predict(X_test)\n",
    "    ENET_eval_time = time.time() - ENET_eval_time\n",
    "    ENET_N_Params = X_train.shape[0]*2\n",
    "    \n",
    "else:\n",
    "    for d in tqdm(range(output_dim)):\n",
    "        # Fit Elastic Net Model\n",
    "        ENET_reg.fit(X_train,Y_train_mean_emp[:,d])\n",
    "\n",
    "        # Get Predictions\n",
    "        ENET_predict_loop = ENET_reg.predict(X_train)\n",
    "        ENET_eval_time_loop = time.time()\n",
    "        ENET_predict_loop_test = ENET_reg.predict(X_test)\n",
    "        ENET_eval_time_loop = time.time() - ENET_eval_time_loop\n",
    "    \n",
    "        if d == 0:\n",
    "            ENET_predict = ENET_predict_loop.reshape(-1,1)\n",
    "            ENET_predict_test = ENET_predict_loop_test.reshape(-1,1)\n",
    "            ENET_N_Params = X_train.shape[0]*2\n",
    "            ENET_eval_time = ENET_eval_time_loop\n",
    "        else:\n",
    "            ENET_predict = np.append(ENET_predict,ENET_predict_loop.reshape(-1,1),axis=1)\n",
    "            ENET_predict_test = np.append(ENET_predict_test,ENET_predict_loop_test.reshape(-1,1),axis=1)\n",
    "            ENET_N_Params = ENET_N_Params + X_train.shape[0]*2\n",
    "            ENET_eval_time = ENET_eval_time + ENET_eval_time_loop\n",
    "\n",
    "# Stop Timer\n",
    "Timer_ENET = time.time() - Timer_ENET\n",
    "print(\"---------------------\")\n",
    "print(\"Training: ENET - Done\")\n",
    "print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Prediction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "ENET_errors_W1, ENET_errors_M1 = get_deterministic_errors(X_train,\n",
    "                                                          ENET_predict,\n",
    "                                                          Y_train,\n",
    "                                                          N_Bootstraps=N_Boostraps_BCA)\n",
    "## Test\n",
    "ENET_errors_W1_test, ENET_errors_M1_test = get_deterministic_errors(X_test,\n",
    "                                                                    ENET_predict_test,\n",
    "                                                                    Y_test,\n",
    "                                                                    N_Bootstraps=N_Boostraps_BCA)\n",
    "# Stop Timer\n",
    "Timer_ENET = time.time() - Timer_ENET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "Summary_pred_Qual_models[\"ENET\"] = pd.Series(np.append(np.append(ENET_errors_W1,\n",
    "                                                                ENET_errors_M1),\n",
    "                                                         np.array([ENET_N_Params,\n",
    "                                                                   Timer_ENET,\n",
    "                                                                   (ENET_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "# Test\n",
    "Summary_pred_Qual_models_test[\"ENET\"] = pd.Series(np.append(np.append(ENET_errors_W1_test,\n",
    "                                                                     ENET_errors_M1_test),\n",
    "                                                           np.array([ENET_N_Params,\n",
    "                                                                     Timer_ENET,\n",
    "                                                                     (ENET_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "\n",
    "print(\"Updated DataFrame\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Timer_kRidge = time.time()\n",
    "\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(\"Training: K-Ridge\")\n",
    "print(\"-----------------\")\n",
    "\n",
    "Xhat_Kridge, Xhat_Kridge_test , relic, kRidge_N_params, KRidge_eval_time = get_Kernel_Ridge_Regressor(X_train,X_test,Y_train_mean_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "kRidge_errors_W1, kRidge_errors_M1 = get_deterministic_errors(X_train,Xhat_Kridge,Y_train,N_Bootstraps=N_Boostraps_BCA)\n",
    "## Test\n",
    "kRidge_errors_W1_test, kRidge_errors_M1_test = get_deterministic_errors(X_test,Xhat_Kridge_test,Y_test,N_Bootstraps=N_Boostraps_BCA)\n",
    "# Stop Timer\n",
    "Timer_kRidge = time.time() - Timer_kRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "Summary_pred_Qual_models[\"KRidge\"] = pd.Series(np.append(np.append(kRidge_errors_W1,\n",
    "                                                                kRidge_errors_M1),\n",
    "                                                         np.array([0,\n",
    "                                                                   Timer_kRidge,\n",
    "                                                                   (KRidge_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "# Test\n",
    "Summary_pred_Qual_models_test[\"KRidge\"] = pd.Series(np.append(np.append(kRidge_errors_W1_test,\n",
    "                                                                     kRidge_errors_M1_test),\n",
    "                                                           np.array([0,\n",
    "                                                                     Timer_kRidge,\n",
    "                                                                     (KRidge_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "\n",
    "print(\"Updated DataFrame\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "Timer_GBRF = time.time()\n",
    "\n",
    "print(\"--------------\")\n",
    "print(\"Training: GBRF\")\n",
    "print(\"--------------\")\n",
    "\n",
    "if output_dim == 1:\n",
    "    GBRF_y_hat_train, GBRF_y_hat_test, GBRF_model, GBRF_N_Params, GBRF_eval_time = get_GBRF(X_train,\n",
    "                                                                                            X_test,\n",
    "                                                                                            Y_train_mean_emp)\n",
    "else:\n",
    "    for d in range(output_dim):\n",
    "        GBRF_y_hat_train_loop, GBRF_y_hat_test_loop, GBRF_model, GBRF_N_Params_loop, GBRF_eval_time_loop = get_GBRF(X_train,\n",
    "                                                                                                                X_test,\n",
    "                                                                                                                Y_train_mean_emp[:,d])\n",
    "        if d == 0:\n",
    "            GBRF_y_hat_train = GBRF_y_hat_train_loop.reshape(-1,1)\n",
    "            GBRF_y_hat_test = GBRF_y_hat_test_loop.reshape(-1,1)\n",
    "            GBRF_N_Params = GBRF_N_Params_loop\n",
    "            GBRF_eval_time = GBRF_eval_time_loop\n",
    "        else:\n",
    "            GBRF_y_hat_train = np.append(GBRF_y_hat_train,GBRF_y_hat_train_loop.reshape(-1,1),axis=1)\n",
    "            GBRF_y_hat_test = np.append(GBRF_y_hat_test,GBRF_y_hat_test_loop.reshape(-1,1),axis=1)\n",
    "            GBRF_N_Params = GBRF_N_Params + GBRF_N_Params_loop\n",
    "            GBRF_eval_time = GBRF_eval_time + GBRF_eval_time_loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "GBRF_errors_W1, GBRF_errors_M1 = get_deterministic_errors(X_train,GBRF_y_hat_train,Y_train,N_Bootstraps=N_Boostraps_BCA)\n",
    "## Test\n",
    "GBRF_errors_W1_test, GBRF_errors_M1_test = get_deterministic_errors(X_test,GBRF_y_hat_test,Y_test,N_Bootstraps=N_Boostraps_BCA)\n",
    "\n",
    "\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Timer_GBRF = time.time() - Timer_GBRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "Summary_pred_Qual_models[\"GBRF\"] = pd.Series(np.append(np.append(GBRF_errors_W1,\n",
    "                                                                GBRF_errors_M1),\n",
    "                                                         np.array([GBRF_N_Params,\n",
    "                                                                   Timer_GBRF,\n",
    "                                                                   (GBRF_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "# Test\n",
    "Summary_pred_Qual_models_test[\"GBRF\"] = pd.Series(np.append(np.append(GBRF_errors_W1_test,\n",
    "                                                                     GBRF_errors_M1_test),\n",
    "                                                           np.array([GBRF_N_Params,\n",
    "                                                                     Timer_GBRF,\n",
    "                                                                     (GBRF_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "\n",
    "print(\"Updated DataFrame\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Timer_ffNN = time.time()\n",
    "print(\"-------------\")\n",
    "print(\"Training: DNN\")\n",
    "print(\"-------------\")\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "param_grid_Deep_Classifier['output_dim'] = [output_dim]\n",
    "\n",
    "YHat_ffNN, YHat_ffNN_test, ffNN_N_Params, ffNN_eval_time = build_ffNN(n_folds = CV_folds,\n",
    "                                                                      n_jobs = n_jobs, \n",
    "                                                                      n_iter = n_iter,\n",
    "                                                                      param_grid_in = param_grid_Deep_Classifier,  \n",
    "                                                                      X_train = X_train,\n",
    "                                                                      y_train = Y_train_mean_emp,\n",
    "                                                                      X_test = X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "ffNN_errors_W1,ffNN_errors_M1 = get_deterministic_errors(X_train,YHat_ffNN,Y_train,N_Bootstraps=N_Boostraps_BCA)\n",
    "## Test\n",
    "ffNN_errors_W1_test,ffNN_errors_M1_test = get_deterministic_errors(X_test,YHat_ffNN_test,Y_test,N_Bootstraps=N_Boostraps_BCA)\n",
    "\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Timer_ffNN =  time.time() - Timer_ffNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "Summary_pred_Qual_models[\"DNN\"] = pd.Series(np.append(np.append(ffNN_errors_W1,\n",
    "                                                                ffNN_errors_M1),\n",
    "                                                         np.array([ffNN_N_Params,\n",
    "                                                                   Timer_ffNN,\n",
    "                                                                   (ffNN_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "# Test\n",
    "Summary_pred_Qual_models_test[\"DNN\"] = pd.Series(np.append(np.append(ffNN_errors_W1_test,\n",
    "                                                                     ffNN_errors_M1_test),\n",
    "                                                           np.array([ffNN_N_Params,\n",
    "                                                                     Timer_ffNN,\n",
    "                                                                     (ffNN_eval_time/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "\n",
    "print(\"Updated DataFrame\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
