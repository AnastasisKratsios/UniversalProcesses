{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model (Type A)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Procedure:\n",
    "---\n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "## 1) Generate Data:\n",
    "Generates the empirical measure $\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$ of $X_T$ conditional on $X_0=x_0\\in \\mathbb{R}$ *($x_0$ and $T>0$ are user-provided)*.\n",
    "\n",
    "## 2) Get \"Sample Barycenters\":\n",
    "Let $\\{\\mu_n\\}_{n=1}^N\\subset\\mathcal{P}_1(\\mathbb{R}^d)$.  Then, the *sample barycenter* is defined by:\n",
    "1. $\\mathcal{M}^{(0)}\\triangleq \\left\\{\\hat{\\mu}_n\\right\\}_{n=1}^N$,\n",
    "2. For $1\\leq n\\leq \\mbox{N sample barycenters}$: \n",
    "    - $\n",
    "\\mu^{\\star}\\in \\underset{\\tilde{\\mu}\\in \\mathcal{M}^{(n)}}{\\operatorname{argmin}}\\, \\sum_{n=1}^N \\mathcal{W}_1\\left(\\mu^{\\star},\\mu_n\\right),\n",
    "$\n",
    "    - $\\mathcal{M}^{(n)}\\triangleq \\mathcal{M}^{(n-1)} - \\{\\mu^{\\star}\\},$\n",
    "*i.e., the closest generated measure form the random sample to all other elements of the random sample.*\n",
    "\n",
    "---\n",
    "**Note:** *We simplify the computational burden of getting the correct classes by putting this right into this next loop.*\n",
    "\n",
    "## 3) Train Deep Classifier:\n",
    "$\\hat{f}\\in \\operatorname{argmin}_{f \\in \\mathcal{NN}_{d:N}^{\\star}} \n",
    "\\sum_{x \\in \\mathbb{X}}\n",
    "\\, \n",
    "\\mathbb{H}\n",
    "\\left(\n",
    "    \\operatorname{Softmax}_N\\circ f(x)_n| I\\left\\{W_1(\\hat{\\mu}_n,\\mu_x),\\inf_{m\\leq N} W_1(\\hat{\\mu}_m,\\mu_x)\\right\\}\n",
    "\\right);\n",
    "$\n",
    "where $\\mathbb{H}$ is the categorical cross-entropy.  \n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many random polulations to visualize:\n",
    "Visualization_Size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "*This hyperparameter describes the proportion of the data used as sample-barycenters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantization_Proportion = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Euler_Maruyama_Steps = 2\n",
    "N_Monte_Carlo_Samples = 10000\n",
    "N_Monte_Carlo_Samples_Test = 1000 # How many MC-samples to draw from test-set?\n",
    "T_end = 1\n",
    "Direct_Sampling = False #This hyperparameter determines if we use a Euler-Maryama scheme or if we use something else.  \n",
    "\n",
    "## Grid\n",
    "N_Grid_Finess = 200\n",
    "Max_Grid = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Setting *N_Quantizers_to_parameterize* prevents any barycenters and sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm\n",
    "---\n",
    "Given a set of training inputs $\\mathbb{X}$ and a stochastic process $(X_t)_{t\\geq 0}$ which we can sample from:\n",
    "1. **For:** x in $\\mathbb{X}$:\n",
    "    - *Simulate:* $\\{x\\mapsto X_T(\\omega_n)\\}_{n=1}^N$\n",
    "    - *Set*: $\\hat{\\nu}_{x,T}\\triangleq \\frac1{N}\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$\n",
    "2. **Learn:** Wasserstein Barycenters $\\hat{\\mu}_1,\\dots,\\hat{\\mu}_N\n",
    "    \\in \\underset{{\\hat{\\mu}_n\\in\\mathscr{P}_{N}(\\mathbb{R}^d)}}{\\operatorname{argmin}}\n",
    "    \\, \\sum_{n=1}^N W_1(\\hat{\\mu_n},\\hat{\\nu}_{x,T})$\n",
    "3. **Train Classifier:** $\\hat{f}:x\\mapsto \\operatorname{n\\leq N}\\, W_1(\\hat{\\mu_n},\\hat{\\nu}_{x,T})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Load Helper Function(s)\n",
    "%run ParaGAN_Backend.ipynb\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Internal (Hyper)-Parameter(s)\n",
    "*Initialize the hyperparameters which are fully-specified by the user-provided hyperparameter(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Grid\n",
    "This is $\\mathbb{X}$ and it represents the grid of initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Grid Instances:  200 and : 150  Testing instances.\n"
     ]
    }
   ],
   "source": [
    "# Get Input Data\n",
    "#----------------------------------------------------------#\n",
    "## Train\n",
    "x_Grid = np.arange(start=-Max_Grid,\n",
    "                   stop=Max_Grid,\n",
    "                   step=(2*Max_Grid/N_Grid_Finess))\n",
    "## Get Number of Instances in Grid: Training\n",
    "N_Grid_Instances = len(x_Grid)\n",
    "\n",
    "#----------------------------------------------------------#\n",
    "## Test\n",
    "x_Grid_test = np.sort(np.random.uniform(low=-Max_Grid,\n",
    "                                        high=Max_Grid,\n",
    "                                        size = round(N_Grid_Instances*test_size_ratio)))\n",
    "# Get Number of Instances in Grid: Test\n",
    "N_Grid_Instances_test = len(x_Grid_test)\n",
    "#----------------------------------------------------------#\n",
    "\n",
    "# Updater User\n",
    "print(\"\\u2022 Grid Instances: \", N_Grid_Instances, \"and :\",N_Grid_Instances_test,\" Testing instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Counting Parameters\n",
    "Initialize the \"conting\" type parameters which will help us to determine the length of loops and to intialize object's size later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ 200  Centers will be produced; from a total datasize of:  200 !  (That's  1  percent).\n",
      "â€¢ Each Wasserstein-1 Ball should contain:  1 elements from the training set.\n"
     ]
    }
   ],
   "source": [
    "# Get Internal (Counting) Parameters\n",
    "N_Quantizers_to_parameterize = round(Quantization_Proportion*N_Grid_Finess)\n",
    "N_Elements_Per_Cluster = int(round(N_Grid_Instances/N_Quantizers_to_parameterize))\n",
    "\n",
    "# Update User\n",
    "print(\"\\u2022\",N_Quantizers_to_parameterize,\" Centers will be produced; from a total datasize of: \",N_Grid_Finess,\n",
    "      \"!  (That's \",Quantization_Proportion,\n",
    "      \" percent).\")\n",
    "print(\"\\u2022 Each Wasserstein-1 Ball should contain: \",\n",
    "      N_Elements_Per_Cluster, \n",
    "      \"elements from the training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Path\n",
    "$d X_t = \\alpha(t,x)dt + \\beta(t,x)dW_t ;\\qquad X_0 =x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(t,x):\n",
    "    return np.sin(math.pi*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(t,x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sampler - Data-Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates the empirical measure $\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$ of $X_T$ conditional on $X_0=x_0\\in \\mathbb{R}$ *($x_0$ and $T>0$ are user-provided)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euler_Maruyama_Generator(x_0,\n",
    "                             N_Euler_Maruyama_Steps = 100,\n",
    "                             N_Monte_Carlo_Samples = 100,\n",
    "                             T = 1): \n",
    "    \n",
    "    #----------------------------#    \n",
    "    # DEFINE INTERNAL PARAMETERS #\n",
    "    #----------------------------#\n",
    "    # Initialize Empirical Measure\n",
    "    X_T_Empirical = np.zeros(N_Monte_Carlo_Samples)\n",
    "\n",
    "\n",
    "    # Internal Initialization(s)\n",
    "    ## Initialize current state\n",
    "    n_sample = 0\n",
    "    ## Initialize Incriments\n",
    "    dt = T/N_Euler_Maruyama_Steps\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "\n",
    "    #-----------------------------#    \n",
    "    # Generate Monte-Carlo Sample #\n",
    "    #-----------------------------#\n",
    "    while n_sample < N_Monte_Carlo_Samples:\n",
    "        # Reset Step Counter\n",
    "        t = 1\n",
    "        # Initialize Current State \n",
    "        X_current = x_0\n",
    "        # Perform Euler-Maruyama Simulation\n",
    "        while t<N_Euler_Maruyama_Steps:\n",
    "            # Update Internal Parameters\n",
    "            ## Get Current Time\n",
    "            t_current = t*(T/N_Euler_Maruyama_Steps)\n",
    "\n",
    "            # Update Generated Path\n",
    "            X_current = X_current + alpha(t_current,X_current)*dt + beta(t_current,X_current)*np.random.normal(0,sqrt_dt)\n",
    "\n",
    "            # Update Counter (EM)\n",
    "            t = t+1\n",
    "\n",
    "        # Update Empirical Measure\n",
    "        X_T_Empirical[n_sample] = X_current\n",
    "\n",
    "        # Update Counter (MC)\n",
    "        n_sample = n_sample + 1\n",
    "\n",
    "    return X_T_Empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize List of Barycenters\n",
    "Wasserstein_Barycenters = []\n",
    "# Initialize Terminal-Time Empirical Measures\n",
    "## Training Outputs\n",
    "measures_locations_list = []\n",
    "measures_weights_list = []\n",
    "## Testing Outputs\n",
    "measures_locations_test_list = []\n",
    "measures_weights_test_list = []\n",
    "\n",
    "# Initialize (Empirical) Weight(s)\n",
    "measure_weights = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples\n",
    "measure_weights_test = np.ones(N_Monte_Carlo_Samples_Test)/N_Monte_Carlo_Samples_Test\n",
    "# Initialize Quantizer\n",
    "Init_Quantizer_generic = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate $\\{\\hat{\\nu}^{N}_{T,x}\\}_{x \\in \\mathbb{X}}$ Build Wasserstein Cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Monte-Carlo Step:\n",
      "Using Monte-Carlo Sampling directly from measure at time-T of X_T.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:12<00:00, 16.42it/s]\n",
      "  9%|â–‰         | 14/150 [00:00<00:01, 132.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Simulation Step (Train Set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 168.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Simulation Step (Test Set)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Update User\n",
    "print(\"Current Monte-Carlo Step:\")\n",
    "if Direct_Sampling == True:\n",
    "    print(\"Using Euler-Maruyama distritization + Monte-Carlo Sampling.\")\n",
    "else:\n",
    "    print(\"Using Monte-Carlo Sampling directly from measure at time-T of X_T.\")\n",
    "\n",
    "# Perform Monte-Carlo Data Generation\n",
    "for i in tqdm(range(N_Grid_Instances)):\n",
    "    # Get Terminal Distribution Shape\n",
    "    ###\n",
    "    \n",
    "    if Direct_Sampling == True:\n",
    "        # DIRECT SAMPLING\n",
    "        measures_locations_loop = (np.random.normal(alpha(1,x_Grid[i]),\n",
    "                                                    beta(1,x_Grid[i]),\n",
    "                                                    N_Monte_Carlo_Samples).reshape(-1,))/N_Monte_Carlo_Samples\n",
    "    else:\n",
    "        measures_locations_loop = Euler_Maruyama_Generator(x_0=x_Grid[i],\n",
    "                                                           N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                                           N_Monte_Carlo_Samples = N_Monte_Carlo_Samples,\n",
    "                                                           T = T_end)\n",
    "    \n",
    "    # Append to List\n",
    "    measures_locations_list.append(measures_locations_loop.reshape(-1,1))\n",
    "    measures_weights_list.append(measure_weights)\n",
    "    \n",
    "# Update User\n",
    "print(\"Done Simulation Step (Train Set)\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Perform Monte-Carlo Data Generation\n",
    "for i in tqdm(range(N_Grid_Instances_test)):\n",
    "    # Get Terminal Distribution Shape\n",
    "    ###\n",
    "    \n",
    "     \n",
    "    if Direct_Sampling == True:\n",
    "        # DIRECT SAMPLING\n",
    "        measures_locations_test_loop = (np.random.normal(alpha(1,x_Grid[i]),\n",
    "                                                    beta(1,x_Grid[i]),\n",
    "                                                    N_Monte_Carlo_Samples_Test).reshape(-1,))/N_Monte_Carlo_Samples_Test\n",
    "    else:\n",
    "        measures_locations_test_loop = Euler_Maruyama_Generator(x_0=x_Grid[i],\n",
    "                                                           N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                                           N_Monte_Carlo_Samples = N_Monte_Carlo_Samples_Test,\n",
    "                                                           T = T_end)\n",
    "    \n",
    "    \n",
    "    # Append to List\n",
    "    measures_locations_test_list.append(measures_locations_test_loop.reshape(-1,1))\n",
    "    measures_weights_test_list.append(measure_weights_test)\n",
    "    \n",
    "# Update User\n",
    "print(\"Done Simulation Step (Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"Sample Barycenters\":\n",
    "Let $\\{\\mu_n\\}_{n=1}^N\\subset\\mathcal{P}_1(\\mathbb{R}^d)$.  Then, the *sample barycenter* is defined by:\n",
    "1. $\\mathcal{M}^{(0)}\\triangleq \\left\\{\\hat{\\mu}_n\\right\\}_{n=1}^N$,\n",
    "2. For $1\\leq n\\leq \\mbox{N sample barycenters}$: \n",
    "    - $\n",
    "\\mu^{\\star}\\in \\underset{\\tilde{\\mu}\\in \\mathcal{M}^{(n)}}{\\operatorname{argmin}}\\, \\sum_{n=1}^N \\mathcal{W}_1\\left(\\mu^{\\star},\\mu_n\\right),\n",
    "$\n",
    "    - $\\mathcal{M}^{(n)}\\triangleq \\mathcal{M}^{(n-1)} - \\{\\mu^{\\star}\\},$\n",
    "*i.e., the closest generated measure form the random sample to all other elements of the random sample.*\n",
    "\n",
    "---\n",
    "**Note:** *We simplify the computational burden of getting the correct classes by putting this right into this next loop.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dissimilarity (Distance) Matrix\n",
    "*In this step we build a dissimularity matrix of the dataset on the Wasserstein-1 space.  Namely:*\n",
    "$$\n",
    "\\operatorname{Mat}_{\\# \\mathbb{X},\\# \\mathbb{X}}\\left(\\mathbb{R}\\right)\\ni D; \\text{ where}\\qquad \\, D_{i,j}\\triangleq \\mathcal{W}_1\\left(f(x_i),f(x_j)\\right)\n",
    ";\n",
    "$$\n",
    "*where $f\\in C\\left((\\mathcal{X},\\mathcal{P}_1(\\mathcal{Y})\\right)$ is the \"target\" function we are learning.*\n",
    "\n",
    "**Note**: *Computing the dissimularity matrix is the most costly part of the entire algorithm with a complexity of at-most $\\mathcal{O}\\left(E_{W} \\# \\mathbb{X})^2\\right)$ where $E_W$ denotes the complexity of a single Wasserstein-1 evaluation between two elements of the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜š  Begin Building Distance Matrix  ðŸ˜š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:25<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€  Done Building Distance Matrix ðŸ˜€ !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Disimilarity Matrix\n",
    "Dissimilarity_matrix_ot = np.zeros([N_Grid_Instances,N_Grid_Instances])\n",
    "\n",
    "\n",
    "# Update User\n",
    "print(\"\\U0001F61A\",\" Begin Building Distance Matrix\",\" \\U0001F61A\")\n",
    "# Build Disimilarity Matrix\n",
    "for i in tqdm(range(N_Grid_Instances)):\n",
    "    for j in range(N_Grid_Instances):\n",
    "        Dissimilarity_matrix_ot[i,j] = ot.emd2_1d(measures_locations_list[j],\n",
    "                                                  measures_locations_list[i])\n",
    "# Update User\n",
    "print(\"\\U0001F600\",\" Done Building Distance Matrix\",\"\\U0001F600\",\"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Quantities to Loop Over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get \"Sample Barycenters\" and Generate Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Locations Matrix (Internal to Loop)\n",
    "measures_locations_list_current = copy.copy(measures_locations_list)\n",
    "Dissimilarity_matrix_ot_current = copy.copy(Dissimilarity_matrix_ot)\n",
    "\n",
    "# Initialize masker vector\n",
    "masker = np.ones(N_Grid_Instances)\n",
    "\n",
    "# Initialize Sorting Reference Vector (This helps us efficiently scroll through the disimularity matrix to identify the barycenter without having to re-compute the dissimultarity matrix of a sub-saple at every iteration (which is the most costly part of the algorithm!))\n",
    "Distances_Loop = Dissimilarity_matrix_ot_current.sum(axis=1)\n",
    "\n",
    "# Initialize Classes (In-Sample)\n",
    "Classifer_Wasserstein_Centers = np.zeros([N_Quantizers_to_parameterize,N_Grid_Instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/usr/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in multiply\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [00:00<00:00, 619.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜š  Begin Identifying Sample Barycenters  ðŸ˜š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 313.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ˜€  Done Identifying Sample Barycenters ðŸ˜€ !\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Update User\n",
    "print(\"\\U0001F61A\",\" Begin Identifying Sample Barycenters\",\" \\U0001F61A\")\n",
    "\n",
    "# Identify Sample Barycenters\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):    \n",
    "    # GET BARYCENTER #\n",
    "    #----------------#\n",
    "    ## Identify row with minimum total distance\n",
    "    Barycenter_index = int(Distances_Loop.argsort()[:1][0])\n",
    "    ## Get Barycenter\n",
    "    ## Update Barycenters Array ##\n",
    "    #----------------------------#\n",
    "    ### Get next Barycenter\n",
    "    new_barycenter_loop = measures_locations_list_current[Barycenter_index].reshape(-1,1)\n",
    "    ### Update Array of Barycenters\n",
    "    if i == 0:\n",
    "        # Initialize Barycenters Array\n",
    "        Barycenters_Array = new_barycenter_loop\n",
    "    else:\n",
    "        # Populate Barycenters Array\n",
    "        Barycenters_Array = np.append(Barycenters_Array,new_barycenter_loop,axis=-1)\n",
    "\n",
    "    # GET CLUSTER #\n",
    "    #-------------#\n",
    "    # Identify Cluster for this barycenter (which elements are closest to it)\n",
    "    Cluster_indices = (masker*Dissimilarity_matrix_ot_current[:,Barycenter_index]).argsort()[:N_Elements_Per_Cluster]\n",
    "    ## UPDATES Set  M^{(n)}  ##\n",
    "    #-------------------------#\n",
    "    Dissimilarity_matrix_ot_current[Cluster_indices,:] = 0\n",
    "    # Distance-Based Sorting\n",
    "    Distances_Loop[Cluster_indices] = math.inf\n",
    "\n",
    "    # Update Cluster\n",
    "    masker[Cluster_indices] = math.inf\n",
    "    \n",
    "    # Update Classes\n",
    "    Classifer_Wasserstein_Centers[i,Cluster_indices] = 1\n",
    "#     print(Cluster_indices)\n",
    "\n",
    "# Update User\n",
    "print(\"\\U0001F600\",\" Done Identifying Sample Barycenters\",\"\\U0001F600\",\"!\")\n",
    "print(Classifer_Wasserstein_Centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train a deep (feed-forward) classifier:\n",
    "$$\n",
    "\\hat{f}\\triangleq \\operatorname{Softmax}_N\\circ W_J\\circ \\sigma \\bullet \\dots \\sigma \\bullet W_1,\n",
    "$$\n",
    "to identify which barycenter we are closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Classifier\n",
    "Prepare Labels/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Elapsed Training Deep Classifier\n",
    "Type_A_timer_Begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('Grid_Enhanced_Network.py').read())\n",
    "# Re-Load Classifier Function(s)\n",
    "exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    4.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.3027 - accuracy: 0.0050\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 5.2884 - accuracy: 0.0150\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2773 - accuracy: 0.0150\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.2636 - accuracy: 0.0150\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.2446 - accuracy: 0.0100\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.2176 - accuracy: 0.0100\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.1813 - accuracy: 0.0050\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.1323 - accuracy: 0.0050\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 5.0718 - accuracy: 0.0100\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 4.9999 - accuracy: 0.0100\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 4.9249 - accuracy: 0.0100\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 4.8539 - accuracy: 0.0050\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.7932 - accuracy: 0.0050\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.7442 - accuracy: 0.0050\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.7044 - accuracy: 0.0050\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6725 - accuracy: 0.0100\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6455 - accuracy: 0.0150\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.6222 - accuracy: 0.0150\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.6003 - accuracy: 0.0100\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.5783 - accuracy: 0.0150\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 4.5577 - accuracy: 0.0100\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.5369 - accuracy: 0.0150\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5152 - accuracy: 0.0150\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.4911 - accuracy: 0.0150\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4664 - accuracy: 0.0100\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.4428 - accuracy: 0.0200\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.4132 - accuracy: 0.0250\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3872 - accuracy: 0.0250\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.3562 - accuracy: 0.0300\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.3274 - accuracy: 0.0350\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.2969 - accuracy: 0.0350\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.2652 - accuracy: 0.0350\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.2379 - accuracy: 0.0300\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.2116 - accuracy: 0.0350\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1842 - accuracy: 0.0200\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1569 - accuracy: 0.0150\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.1293 - accuracy: 0.0200\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.1046 - accuracy: 0.0200\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.0753 - accuracy: 0.0300\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.0527 - accuracy: 0.0450\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 4.0298 - accuracy: 0.0450\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 4.0037 - accuracy: 0.0450\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9789 - accuracy: 0.0300\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9574 - accuracy: 0.0450\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.9340 - accuracy: 0.0400\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.9161 - accuracy: 0.0300\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8947 - accuracy: 0.0500\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 3.8764 - accuracy: 0.0400\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 3.8541 - accuracy: 0.0350\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.8399 - accuracy: 0.0450\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.8199 - accuracy: 0.0400\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.8022 - accuracy: 0.0500\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7860 - accuracy: 0.0600\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7639 - accuracy: 0.0600\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7451 - accuracy: 0.0550\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.7331 - accuracy: 0.0550\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.7145 - accuracy: 0.0550\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6932 - accuracy: 0.0550\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 3.6537 - accuracy: 0.0000e+ - 0s 3ms/step - loss: 3.6815 - accuracy: 0.0550\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.6682 - accuracy: 0.0600\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6555 - accuracy: 0.0500\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6489 - accuracy: 0.0450\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6302 - accuracy: 0.0550\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.6105 - accuracy: 0.0650\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.5950 - accuracy: 0.0650\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.5818 - accuracy: 0.0550\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.5696 - accuracy: 0.0600\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5565 - accuracy: 0.0550\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5499 - accuracy: 0.0450\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5458 - accuracy: 0.0500\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5261 - accuracy: 0.0750\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5057 - accuracy: 0.0750\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.5030 - accuracy: 0.0550\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4865 - accuracy: 0.0750\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4736 - accuracy: 0.0650\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4573 - accuracy: 0.0650\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4566 - accuracy: 0.0450\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4462 - accuracy: 0.0700\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.4348 - accuracy: 0.0700\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.4181 - accuracy: 0.0900\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4096 - accuracy: 0.0700\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4042 - accuracy: 0.0750\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.4017 - accuracy: 0.0700\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3849 - accuracy: 0.0550\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3761 - accuracy: 0.0600\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3651 - accuracy: 0.0650\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3622 - accuracy: 0.0550\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 3.3532 - accuracy: 0.0650\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3425 - accuracy: 0.0650\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3365 - accuracy: 0.0800\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3197 - accuracy: 0.0850\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3100 - accuracy: 0.0700\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3164 - accuracy: 0.0600\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.3019 - accuracy: 0.0800\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2801 - accuracy: 0.0750\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2779 - accuracy: 0.0700\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2692 - accuracy: 0.0750\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2574 - accuracy: 0.0850\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2613 - accuracy: 0.0650\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 3.2572 - accuracy: 0.0750\n",
      "7/7 [==============================] - 0s 858us/step\n",
      "5/5 [==============================] - 0s 953us/step\n"
     ]
    }
   ],
   "source": [
    "# Redefine (Dimension-related) Elements of Grid\n",
    "# param_grid_Deep_Classifier['input_dim'] = [1]\n",
    "param_grid_Deep_Classifier['output_dim'] = [N_Quantizers_to_parameterize]\n",
    "\n",
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter = n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = x_Grid, \n",
    "                                                                                                        y_train = Classifer_Wasserstein_Centers.T,\n",
    "                                                                                                        X_test = x_Grid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predicted Quantized Distributions\n",
    "- Each *row* of \"Predicted_Weights\" is the $\\beta\\in \\Delta_N$.\n",
    "- Each *Column* of \"Barycenters_Array\" denotes the $x_1,\\dots,x_N$ making up the points of the corresponding empirical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 5/200 [00:00<00:04, 46.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#---------------------------------------#\n",
      "Building Training Set (Regression): START\n",
      "#---------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:07<00:00,  2.96it/s]\n",
      "  4%|â–Ž         | 7/200 [00:00<00:03, 57.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------------------#\n",
      "Building Training Set (Regression): END\n",
      "#-------------------------------------#\n",
      "#-------------------------------------#\n",
      "Building Test Set (Predictions): START\n",
      "#-------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [00:28<00:15,  3.02it/s]"
     ]
    }
   ],
   "source": [
    "# Format Weights\n",
    "## Train\n",
    "print(\"#---------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): START\")\n",
    "print(\"#---------------------------------------#\")\n",
    "Predicted_Weights = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):    \n",
    "    b = np.repeat(np.array(predicted_classes_train[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b = b/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights = b\n",
    "    else:\n",
    "        Predicted_Weights = np.append(Predicted_Weights,b,axis=1)\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): END\")\n",
    "print(\"#-------------------------------------#\")\n",
    "\n",
    "## Test\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): START\")\n",
    "print(\"#-------------------------------------#\")\n",
    "Predicted_Weights_test = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):\n",
    "    b_test = np.repeat(np.array(predicted_classes_test[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b_test = b_test/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights_test = b_test\n",
    "    else:\n",
    "        Predicted_Weights_test = np.append(Predicted_Weights_test,b_test,axis=1)\n",
    "print(\"#-----------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): END\")\n",
    "print(\"#-----------------------------------#\")\n",
    "        \n",
    "# Format Points of Mass\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: START\")\n",
    "print(\"#-----------------------------#\")\n",
    "Barycenters_Array = Barycenters_Array.T.reshape(-1,)\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: END\")\n",
    "print(\"#-----------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Moment Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Training Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors = np.array([])\n",
    "Mean_errors = np.array([])\n",
    "Var_errors = np.array([])\n",
    "Skewness_errors = np.array([])\n",
    "Kurtosis_errors = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_list)-1)):    \n",
    "    # Get Laws\n",
    "    W1_loop = ot.emd2_1d(Barycenters_Array,\n",
    "                         measures_locations_list[x_i].reshape(-1,),\n",
    "                         Predicted_Weights[x_i,].reshape(-1,),\n",
    "                         measures_weights_list[x_i].reshape(-1,))\n",
    "    W1_errors = np.append(W1_errors,W1_loop)\n",
    "    # Get Means\n",
    "    Mu_hat = np.sum((Predicted_Weights[x_i])*(Barycenters_Array))\n",
    "    Mu = np.mean(measures_locations_list[x_i])\n",
    "    Mean_errors =  np.append(Mean_errors,(Mu_hat-Mu))\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat = np.sum((Barycenters_Array**2)*(Predicted_Weights[x_i]))\n",
    "    Var = np.mean(measures_locations_list[x_i]**2)\n",
    "    Var_errors = np.append(Var_errors,(Var_hat-Var)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat = np.sum((Barycenters_Array**3)*(Predicted_Weights[x_i]))\n",
    "    Skewness = np.mean(measures_locations_list[x_i]**3)\n",
    "    Skewness_errors = np.append(Skewness_errors,(abs(Skewness_hat-Skewness))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat = np.sum((Barycenters_Array**4)*(Predicted_Weights[x_i]))\n",
    "    Kurtosis = np.mean(measures_locations_list[x_i]**4)\n",
    "    Kurtosis_errors = np.append(Kurtosis_errors,(abs(Kurtosis_hat-Kurtosis))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance = np.array([np.mean(np.abs(W1_errors)),np.mean(W1_errors**2)])\n",
    "Mean_prediction_Performance = np.array([np.mean(np.abs(Mean_errors)),np.mean(Mean_errors**2)])\n",
    "Var_prediction_Performance = np.array([np.mean(np.abs(Var_errors)),np.mean(Var_errors**2)])\n",
    "Skewness_prediction_Performance = np.array([np.mean(np.abs(Skewness_errors)),np.mean(Skewness_errors**2)])\n",
    "Kurtosis_prediction_Performance = np.array([np.mean(np.abs(Kurtosis_errors)),np.mean(Kurtosis_errors**2)])\n",
    "\n",
    "Type_A_Prediction = pd.DataFrame({\"W1\":W1_Performance,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance},index=[\"MAE\",\"MSE\"])\n",
    "\n",
    "# Write Performance\n",
    "Type_A_Prediction.to_latex((results_tables_path+\"Type_A_Prediction.tex\"))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Update User\n",
    "print(Type_A_Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Test Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors_test = np.array([])\n",
    "Mean_errors_test = np.array([])\n",
    "Var_errors_test = np.array([])\n",
    "Skewness_errors_test = np.array([])\n",
    "Kurtosis_errors_test = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_test_list)-1)):    \n",
    "    # Get Laws\n",
    "    W1_loop_test = ot.emd2_1d(Barycenters_Array,\n",
    "                         measures_locations_test_list[x_i].reshape(-1,),\n",
    "                         Predicted_Weights_test[x_i,].reshape(-1,),\n",
    "                         measures_weights_test_list[x_i].reshape(-1,))\n",
    "    W1_errors_test = np.append(W1_errors_test,W1_loop_test)\n",
    "    # Get Means\n",
    "    Mu_hat_test = np.sum((Predicted_Weights_test[x_i])*(Barycenters_Array))\n",
    "    Mu_test = np.mean(measures_locations_test_list[x_i])\n",
    "    Mean_errors_test =  np.append(Mean_errors_test,(Mu_hat_test-Mu_test))\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat_test = np.sum((Barycenters_Array**2)*(Predicted_Weights_test[x_i]))\n",
    "    Var_test = np.mean(measures_locations_test_list[x_i]**2)\n",
    "    Var_errors_test = np.append(Var_errors_test,(Var_hat_test-Var_test)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat_test = np.sum((Barycenters_Array**3)*(Predicted_Weights_test[x_i]))\n",
    "    Skewness_test = np.mean(measures_locations_test_list[x_i]**3)\n",
    "    Skewness_errors_test = np.append(Skewness_errors_test,(abs(Skewness_hat_test-Skewness_test))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat_test = np.sum((Barycenters_Array**4)*(Predicted_Weights_test[x_i]))\n",
    "    Kurtosis_test = np.mean(measures_locations_test_list[x_i]**4)\n",
    "    Kurtosis_errors_test = np.append(Kurtosis_errors_test,(abs(Kurtosis_hat_test-Kurtosis_test))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance_test = np.array([np.mean(np.abs(W1_errors_test)),np.mean(W1_errors_test**2)])\n",
    "Mean_prediction_Performance_test = np.array([np.mean(np.abs(Mean_errors_test)),np.mean(Mean_errors_test**2)])\n",
    "Var_prediction_Performance_test = np.array([np.mean(np.abs(Var_errors_test)),np.mean(Var_errors_test**2)])\n",
    "Skewness_prediction_Performance_test = np.array([np.mean(np.abs(Skewness_errors_test)),np.mean(Skewness_errors_test**2)])\n",
    "Kurtosis_prediction_Performance_test = np.array([np.mean(np.abs(Kurtosis_errors_test)),np.mean(Kurtosis_errors_test**2)])\n",
    "\n",
    "Type_A_Prediction_test = pd.DataFrame({\"W1\":W1_Performance_test,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance_test,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance_test,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance_test,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance_test},index=[\"MAE\",\"MSE\"])\n",
    "\n",
    "# Write Performance\n",
    "Type_A_Prediction_test.to_latex((results_tables_path+\"Type_A_Prediction_test.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#----------------------#\")\n",
    "print(\"Training-Set Performance\")\n",
    "print(\"#----------------------#\")\n",
    "print(Type_A_Prediction_test)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_A_Prediction_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#------------------#\")\n",
    "print(\"Test-Set Performance\")\n",
    "print(\"#------------------#\")\n",
    "print(Type_A_Prediction_test)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_A_Prediction_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Performance\n",
    "Randomly subsample from output space and visualize empirical measures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust if is number of plots to visualizes is larger than number of output distributions (But only if there is not enough data!)\n",
    "# if N_Grid_Instances <= Visualization_Size**2:\n",
    "#         Visualization_Size = int(round(np.sqrt(min(N_Grid_Instances,Visualization_Size**2)))-1)\n",
    "\n",
    "\n",
    "# # Initialize Random Sample of input-output pairs to visualize\n",
    "# plotting_distribution_indices = random.sample(range(N_Grid_Instances), (Visualization_Size)**2)\n",
    "\n",
    "# # Generate Plot\n",
    "# f, axarr = plt.subplots(Visualization_Size,Visualization_Size,figsize=(6, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.suptitle(\"Sample of Predictions\")\n",
    "# for i in range(Visualization_Size):\n",
    "#     for j in range(Visualization_Size):\n",
    "#         # Get Current (Randomly chosen (uniformly)) Index\n",
    "#         current_index = (i*Visualization_Size + j)\n",
    "#         current_random_index = plotting_distribution_indices[current_index]\n",
    "#         # Generate Current Plot\n",
    "#         axarr[i,j].bar(Barycenters_Array,(Predicted_Weights[current_random_index].reshape(-1,)), alpha=0.5,label=\"Prediction\",color=\"chartreuse\")\n",
    "#         axarr[i,j].bar(measures_locations_list[current_random_index].reshape(-1,),measures_weights_list[current_random_index], alpha=0.5,label=\"Target\",color=\"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
