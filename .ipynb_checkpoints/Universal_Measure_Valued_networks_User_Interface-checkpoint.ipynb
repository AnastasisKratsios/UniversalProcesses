{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Universal Regular Conditional Expectations:\n",
    "\n",
    "---\n",
    "This implements the universal deep neural model of $\\mathcal{NN}_{1_{\\mathbb{R}^n},\\mathcal{D}}^{\\sigma:\\star}$ [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---\n",
    "\n",
    "## What does this code do?\n",
    "1. Learn Heteroskedastic Non-Linear Regression Problem\n",
    "     - $Y\\sim f_{\\text{unkown}}(x) + \\epsilon$ where $f$ is an known function and $\\epsilon\\sim Laplace(0,\\|x\\|)$\n",
    "2. Learn Random Bayesian Network's Law:\n",
    "    - $Y = W_J Y^{J-1}, \\qquad Y^{j}\\triangleq \\sigma\\bullet A^{j}Y^{j-1} + b^{j}, \\qquad Y^0\\triangleq x$\n",
    "\n",
    "3. In the above example if $A_j = M_j\\odot \\tilde{A_j}$ where $\\tilde{A}_j$ is a deterministic matrix and $M_j$ is a \"mask\", that is, a random matrix with binary entries and $\\odot$ is the Hadamard product then we recover the dropout framework.\n",
    "4. Learn the probability distribution that the unique strong solution to the rough SDE with uniformly Lipschitz drivers driven by a factional Brownian motion with Hurst exponent $H \\in [\\frac1{2},1)$:\n",
    "$$\n",
    "X_t^x = x + \\int_0^t \\alpha(s,X_s^x)ds + \\int_0^t \\beta(s,X_s^x)dB_s^H\n",
    "$$\n",
    "belongs, at time $t=1$, to a ball about the initial point $x$ of random radius given by an independant exponential random-variable with shape parameter $\\lambda=2$\n",
    "5. Train a DNN to predict the returns of bitcoin with GD.  Since this has random initialization then each prediction of a given $x$ is stochastic...We learn the distribution of this conditional RV (conditioned on x in the input space).\n",
    "$$\n",
    "Y_x \\triangleq \\hat{f}_{\\theta_{T}}(x), \\qquad \\theta_{(t+1)}\\triangleq \\theta_{(t)} + \\lambda \\sum_{x \\in \\mathbb{X}} \\nabla_{\\theta}\\|\\hat{f}_{\\theta_t}(x) - f(x)\\|, \\qquad \\theta_0 \\sim N_d(0,1);\n",
    "$$\n",
    "$T\\in \\mathbb{N}$ is a fixed number of \"SGD\" iterations (typically identified by cross-validation on a single SGD trajectory for a single initialization) and where $\\theta \\in \\mathbb{R}^{(d_{J}+1)+\\sum_{j=0}^{J-1} (d_{j+1}d_j + 1)}$ and $d_j$ is the dimension of the \"bias\" vector $b_j$ defining each layer of the DNN with layer dimensions:\n",
    "$$\n",
    "\\hat{f}_{\\theta}(x)\\triangleq A^{(J)}x^{(J)} + b^{(J)},\\qquad x^{(j+1)}\\triangleq \\sigma\\bullet A^{j}x^{(j)} + b^{j},\\qquad x^{(0)}\\triangleq x\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode:\n",
    "Software/Hardware Testing or Real-Deal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random DNN\n",
    "# f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# Random DNN internal noise\n",
    "# f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "Depth_Bayesian_DNN = 1\n",
    "width = 5\n",
    "\n",
    "# Random Dropout applied to trained DNN\n",
    "# f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "Dropout_rate = 0.1\n",
    "\n",
    "# GD with Randomized Input\n",
    "# f_unknown_mode = \"GD_with_randomized_input\"\n",
    "GD_epochs = 2\n",
    "\n",
    "# SDE with fractional Driver\n",
    "f_unknown_mode = \"Rough_SDE\"\n",
    "N_Euler_Steps = 10**2\n",
    "Hurst_Exponent = 0.75\n",
    "\n",
    "# f_unknown_mode = \"Rough_SDE_Vanilla\"\n",
    "## Define Process' dynamics in (2) cell(s) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla fractional SDE:\n",
    "If f_unknown_mode == \"Rough_SDE_Vanilla\" is selected, then we can specify the process's dynamics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------#\n",
    "# Define Process' Dynamics #\n",
    "#--------------------------#\n",
    "# Define DNN Applier\n",
    "def f_unknown_drift_vanilla(x):\n",
    "    x_internal = x.reshape(-1,)\n",
    "    x_internal = drift_constant*x_internal\n",
    "    return x_internal\n",
    "def f_unknown_vol_vanilla(x):\n",
    "    x_internal = volatility_constant*diag(problem_dim)\n",
    "    return x_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: *Why the procedure is so computationally efficient*?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)\n",
    "- Ratio $\\frac{\\text{Testing Datasize}}{\\text{Training Datasize}}$.\n",
    "- Number of Training Points to Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = .1\n",
    "N_train_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Monte_Carlo_Samples = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial radis of $\\delta$-bounded random partition of $\\mathcal{X}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.01\n",
    "Proportion_per_cluster = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Auxiliary Script(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# %run Loader.ipynb\n",
    "exec(open('Loader.py').read())\n",
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "import time as time #<- Note sure why...but its always seems to need 'its own special loading...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate or Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Beginning Data-Parsing/Simulation Phase\n",
      "---------------------------------------\n",
      "Deciding on Which Simulator/Parser To Load\n",
      "Setting/Defining: Internal Parameters\n",
      "Deciding on Which Type of Data to Get/Simulate\n",
      "Simulating Output Data for given input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.40s/it]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Done Data-Parsing/Simulation Phase\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run Data_Simulator_and_Parser.ipynb\n",
    "exec(open('Data_Simulator_and_Parser.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Running script for main model!\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 8540.31it/s]\n",
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "==========================================\n",
      "Training Classifer Portion of Type-A Model\n",
      "==========================================\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Done   1 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=40)]: Done   6 out of  40 | elapsed: 16.3min remaining: 92.6min\n",
      "[Parallel(n_jobs=40)]: Done  11 out of  40 | elapsed: 20.4min remaining: 53.8min\n",
      "[Parallel(n_jobs=40)]: Done  16 out of  40 | elapsed: 22.0min remaining: 33.1min\n",
      "[Parallel(n_jobs=40)]: Done  21 out of  40 | elapsed: 37.2min remaining: 33.7min\n",
      "[Parallel(n_jobs=40)]: Done  26 out of  40 | elapsed: 40.7min remaining: 21.9min\n",
      "[Parallel(n_jobs=40)]: Done  31 out of  40 | elapsed: 46.6min remaining: 13.5min\n",
      "[Parallel(n_jobs=40)]: Done  36 out of  40 | elapsed: 55.6min remaining:  6.2min\n",
      "[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 60.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 7.3251 - accuracy: 0.0000e+00\n",
      "Epoch 2/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 7.2292 - accuracy: 0.0010\n",
      "Epoch 3/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 6.8449 - accuracy: 0.0020\n",
      "Epoch 4/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 6.3836 - accuracy: 0.0055\n",
      "Epoch 5/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 5.9467 - accuracy: 0.0090\n",
      "Epoch 6/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 5.5721 - accuracy: 0.0095\n",
      "Epoch 7/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 5.2152 - accuracy: 0.0160\n",
      "Epoch 8/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 4.9273 - accuracy: 0.0170\n",
      "Epoch 9/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 4.7083 - accuracy: 0.0225\n",
      "Epoch 10/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 4.5255 - accuracy: 0.0230\n",
      "Epoch 11/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 4.3736 - accuracy: 0.0285\n",
      "Epoch 12/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 4.2515 - accuracy: 0.0365\n",
      "Epoch 13/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 4.1348 - accuracy: 0.0350\n",
      "Epoch 14/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 4.0442 - accuracy: 0.0425\n",
      "Epoch 15/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.9517 - accuracy: 0.0380\n",
      "Epoch 16/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.8682 - accuracy: 0.0465\n",
      "Epoch 17/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.7905 - accuracy: 0.0465\n",
      "Epoch 18/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 3.7124 - accuracy: 0.0590\n",
      "Epoch 19/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.6692 - accuracy: 0.0595\n",
      "Epoch 20/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.5934 - accuracy: 0.0560\n",
      "Epoch 21/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.5291 - accuracy: 0.0635\n",
      "Epoch 22/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.4974 - accuracy: 0.0690\n",
      "Epoch 23/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.4482 - accuracy: 0.0630\n",
      "Epoch 24/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.3971 - accuracy: 0.0740\n",
      "Epoch 25/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.3422 - accuracy: 0.0875\n",
      "Epoch 26/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.3068 - accuracy: 0.0740\n",
      "Epoch 27/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.2828 - accuracy: 0.0810\n",
      "Epoch 28/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.2528 - accuracy: 0.0765\n",
      "Epoch 29/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.2115 - accuracy: 0.0790\n",
      "Epoch 30/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.1634 - accuracy: 0.0865\n",
      "Epoch 31/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.1349 - accuracy: 0.0870\n",
      "Epoch 32/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.1160 - accuracy: 0.0970\n",
      "Epoch 33/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.0699 - accuracy: 0.1070\n",
      "Epoch 34/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 3.0470 - accuracy: 0.0990\n",
      "Epoch 35/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.0202 - accuracy: 0.0970\n",
      "Epoch 36/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 3.0073 - accuracy: 0.1040\n",
      "Epoch 37/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.9686 - accuracy: 0.1065\n",
      "Epoch 38/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.9402 - accuracy: 0.1130\n",
      "Epoch 39/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.9197 - accuracy: 0.1160\n",
      "Epoch 40/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.9084 - accuracy: 0.1050\n",
      "Epoch 41/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.8823 - accuracy: 0.1060\n",
      "Epoch 42/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.8459 - accuracy: 0.1150\n",
      "Epoch 43/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.8385 - accuracy: 0.1185\n",
      "Epoch 44/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.8163 - accuracy: 0.1240\n",
      "Epoch 45/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.8059 - accuracy: 0.1125\n",
      "Epoch 46/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.7769 - accuracy: 0.1170\n",
      "Epoch 47/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.7327 - accuracy: 0.1230\n",
      "Epoch 48/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.7047 - accuracy: 0.1415\n",
      "Epoch 49/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.7097 - accuracy: 0.1310\n",
      "Epoch 50/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.6998 - accuracy: 0.1290\n",
      "Epoch 51/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.6768 - accuracy: 0.1410\n",
      "Epoch 52/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.6596 - accuracy: 0.1435\n",
      "Epoch 53/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.6424 - accuracy: 0.1430\n",
      "Epoch 54/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.6100 - accuracy: 0.1470\n",
      "Epoch 55/250\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 2.6020 - accuracy: 0.1405\n",
      "Epoch 56/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.5882 - accuracy: 0.1400\n",
      "Epoch 57/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.5673 - accuracy: 0.1420\n",
      "Epoch 58/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.5579 - accuracy: 0.1415\n",
      "Epoch 59/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.5572 - accuracy: 0.1505\n",
      "Epoch 60/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.5284 - accuracy: 0.1575\n",
      "Epoch 61/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.5187 - accuracy: 0.1515\n",
      "Epoch 62/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.5172 - accuracy: 0.1460\n",
      "Epoch 63/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.4771 - accuracy: 0.1655\n",
      "Epoch 64/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.4600 - accuracy: 0.1575\n",
      "Epoch 65/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.4596 - accuracy: 0.1560\n",
      "Epoch 66/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.4483 - accuracy: 0.1655\n",
      "Epoch 67/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.4269 - accuracy: 0.1735\n",
      "Epoch 68/250\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 2.4163 - accuracy: 0.1695\n",
      "Epoch 69/250\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.4524 - accuracy: 0.1545\n",
      "Epoch 70/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3680 - accuracy: 0.1970\n",
      "Epoch 71/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3941 - accuracy: 0.1655\n",
      "Epoch 72/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3658 - accuracy: 0.1705\n",
      "Epoch 73/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3483 - accuracy: 0.1880\n",
      "Epoch 74/250\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 2.3429 - accuracy: 0.1805\n",
      "Epoch 75/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3338 - accuracy: 0.1870\n",
      "Epoch 76/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.3102 - accuracy: 0.1895\n",
      "Epoch 77/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.2966 - accuracy: 0.1920\n",
      "Epoch 78/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.2822 - accuracy: 0.2035\n",
      "Epoch 79/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.2899 - accuracy: 0.1890\n",
      "Epoch 80/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.2736 - accuracy: 0.1985\n",
      "Epoch 81/250\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 2.2508 - accuracy: 0.1950\n",
      "Epoch 82/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 8ms/step - loss: 2.2753 - accuracy: 0.1980\n",
      "Epoch 83/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.2501 - accuracy: 0.1935\n",
      "Epoch 84/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.2175 - accuracy: 0.2080\n",
      "Epoch 85/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.2200 - accuracy: 0.2045\n",
      "Epoch 86/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.2212 - accuracy: 0.2065\n",
      "Epoch 87/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1822 - accuracy: 0.2140\n",
      "Epoch 88/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1831 - accuracy: 0.2075\n",
      "Epoch 89/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1739 - accuracy: 0.2065\n",
      "Epoch 90/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.1772 - accuracy: 0.2015\n",
      "Epoch 91/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1511 - accuracy: 0.2240\n",
      "Epoch 92/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1606 - accuracy: 0.2105\n",
      "Epoch 93/250\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 2.1522 - accuracy: 0.2150\n",
      "Epoch 94/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1328 - accuracy: 0.2085\n",
      "Epoch 95/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1332 - accuracy: 0.2235\n",
      "Epoch 96/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1306 - accuracy: 0.2205\n",
      "Epoch 97/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 2.1083 - accuracy: 0.2230\n",
      "Epoch 98/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.1019 - accuracy: 0.2225\n",
      "Epoch 99/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0784 - accuracy: 0.2360\n",
      "Epoch 100/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0811 - accuracy: 0.2240\n",
      "Epoch 101/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0714 - accuracy: 0.2335\n",
      "Epoch 102/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0794 - accuracy: 0.2150\n",
      "Epoch 103/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0570 - accuracy: 0.2335\n",
      "Epoch 104/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0519 - accuracy: 0.2370\n",
      "Epoch 105/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0551 - accuracy: 0.2210\n",
      "Epoch 106/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0617 - accuracy: 0.2180\n",
      "Epoch 107/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0239 - accuracy: 0.2425\n",
      "Epoch 108/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0134 - accuracy: 0.2500\n",
      "Epoch 109/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0621 - accuracy: 0.2335\n",
      "Epoch 110/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 2.0327 - accuracy: 0.2375\n",
      "Epoch 111/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9995 - accuracy: 0.2350\n",
      "Epoch 112/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9994 - accuracy: 0.2375\n",
      "Epoch 113/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9965 - accuracy: 0.2555\n",
      "Epoch 114/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.9840 - accuracy: 0.2420\n",
      "Epoch 115/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9612 - accuracy: 0.2530\n",
      "Epoch 116/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9704 - accuracy: 0.2475\n",
      "Epoch 117/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9515 - accuracy: 0.2590\n",
      "Epoch 118/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.9798 - accuracy: 0.2355\n",
      "Epoch 119/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9329 - accuracy: 0.2650\n",
      "Epoch 120/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9475 - accuracy: 0.2475\n",
      "Epoch 121/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9405 - accuracy: 0.2525\n",
      "Epoch 122/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9264 - accuracy: 0.2595\n",
      "Epoch 123/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9327 - accuracy: 0.2375\n",
      "Epoch 124/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9198 - accuracy: 0.2630\n",
      "Epoch 125/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9057 - accuracy: 0.2655\n",
      "Epoch 126/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.9075 - accuracy: 0.2620\n",
      "Epoch 127/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.8820 - accuracy: 0.2590\n",
      "Epoch 128/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.8739 - accuracy: 0.2700\n",
      "Epoch 129/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8718 - accuracy: 0.2730\n",
      "Epoch 130/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.9118 - accuracy: 0.2400\n",
      "Epoch 131/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8848 - accuracy: 0.2680\n",
      "Epoch 132/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8499 - accuracy: 0.2800\n",
      "Epoch 133/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8522 - accuracy: 0.2835\n",
      "Epoch 134/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8550 - accuracy: 0.2690\n",
      "Epoch 135/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.8385 - accuracy: 0.2875\n",
      "Epoch 136/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8388 - accuracy: 0.2825\n",
      "Epoch 137/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8318 - accuracy: 0.2720\n",
      "Epoch 138/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8327 - accuracy: 0.2830\n",
      "Epoch 139/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.8326 - accuracy: 0.2745\n",
      "Epoch 140/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8267 - accuracy: 0.2765\n",
      "Epoch 141/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8111 - accuracy: 0.2820\n",
      "Epoch 142/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8003 - accuracy: 0.2860\n",
      "Epoch 143/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7979 - accuracy: 0.2820\n",
      "Epoch 144/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8008 - accuracy: 0.2860\n",
      "Epoch 145/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7613 - accuracy: 0.3040\n",
      "Epoch 146/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.8085 - accuracy: 0.2795\n",
      "Epoch 147/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7859 - accuracy: 0.2910\n",
      "Epoch 148/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7572 - accuracy: 0.3045\n",
      "Epoch 149/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7513 - accuracy: 0.3020\n",
      "Epoch 150/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7572 - accuracy: 0.2910\n",
      "Epoch 151/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7689 - accuracy: 0.2940\n",
      "Epoch 152/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7816 - accuracy: 0.2840\n",
      "Epoch 153/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7628 - accuracy: 0.2935\n",
      "Epoch 154/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7650 - accuracy: 0.3015\n",
      "Epoch 155/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7475 - accuracy: 0.2955\n",
      "Epoch 156/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7225 - accuracy: 0.3165\n",
      "Epoch 157/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7292 - accuracy: 0.3105\n",
      "Epoch 158/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7174 - accuracy: 0.3035\n",
      "Epoch 159/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7365 - accuracy: 0.2970\n",
      "Epoch 160/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7020 - accuracy: 0.3035\n",
      "Epoch 161/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7098 - accuracy: 0.3130\n",
      "Epoch 162/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.7060 - accuracy: 0.3100\n",
      "Epoch 163/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6988 - accuracy: 0.3120\n",
      "Epoch 164/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7427 - accuracy: 0.3035\n",
      "Epoch 165/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.7071 - accuracy: 0.3160\n",
      "Epoch 166/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6741 - accuracy: 0.3260\n",
      "Epoch 167/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6809 - accuracy: 0.3105\n",
      "Epoch 168/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6845 - accuracy: 0.3050\n",
      "Epoch 169/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6484 - accuracy: 0.3220\n",
      "Epoch 170/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.6744 - accuracy: 0.3170\n",
      "Epoch 171/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6716 - accuracy: 0.3135\n",
      "Epoch 172/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6871 - accuracy: 0.3140\n",
      "Epoch 173/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6682 - accuracy: 0.3095\n",
      "Epoch 174/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6286 - accuracy: 0.3290\n",
      "Epoch 175/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6483 - accuracy: 0.3215\n",
      "Epoch 176/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6374 - accuracy: 0.3295\n",
      "Epoch 177/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.6128 - accuracy: 0.3435\n",
      "Epoch 178/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6442 - accuracy: 0.3335\n",
      "Epoch 179/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6192 - accuracy: 0.3325\n",
      "Epoch 180/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6172 - accuracy: 0.3395\n",
      "Epoch 181/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.6062 - accuracy: 0.3315\n",
      "Epoch 182/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6110 - accuracy: 0.3490\n",
      "Epoch 183/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6096 - accuracy: 0.3345\n",
      "Epoch 184/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6243 - accuracy: 0.3275\n",
      "Epoch 185/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6280 - accuracy: 0.3240\n",
      "Epoch 186/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6006 - accuracy: 0.3375\n",
      "Epoch 187/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6073 - accuracy: 0.3355\n",
      "Epoch 188/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5879 - accuracy: 0.3370\n",
      "Epoch 189/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5628 - accuracy: 0.3550\n",
      "Epoch 190/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5810 - accuracy: 0.3560\n",
      "Epoch 191/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5797 - accuracy: 0.3440\n",
      "Epoch 192/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.6202 - accuracy: 0.3250\n",
      "Epoch 193/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5895 - accuracy: 0.3410\n",
      "Epoch 194/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5758 - accuracy: 0.3470\n",
      "Epoch 195/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5478 - accuracy: 0.3675\n",
      "Epoch 196/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5726 - accuracy: 0.3380\n",
      "Epoch 197/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5940 - accuracy: 0.3295\n",
      "Epoch 198/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5589 - accuracy: 0.3520\n",
      "Epoch 199/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5321 - accuracy: 0.3670\n",
      "Epoch 200/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5268 - accuracy: 0.3650\n",
      "Epoch 201/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5591 - accuracy: 0.3395\n",
      "Epoch 202/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5277 - accuracy: 0.3425\n",
      "Epoch 203/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5512 - accuracy: 0.3620\n",
      "Epoch 204/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5289 - accuracy: 0.3565\n",
      "Epoch 205/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5111 - accuracy: 0.3645\n",
      "Epoch 206/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5110 - accuracy: 0.3640\n",
      "Epoch 207/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5129 - accuracy: 0.3610\n",
      "Epoch 208/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4992 - accuracy: 0.3610\n",
      "Epoch 209/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5096 - accuracy: 0.3620\n",
      "Epoch 210/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5355 - accuracy: 0.3565\n",
      "Epoch 211/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5504 - accuracy: 0.3440\n",
      "Epoch 212/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5214 - accuracy: 0.3565\n",
      "Epoch 213/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4801 - accuracy: 0.3795\n",
      "Epoch 214/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5060 - accuracy: 0.3680\n",
      "Epoch 215/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5506 - accuracy: 0.3390\n",
      "Epoch 216/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4931 - accuracy: 0.3755\n",
      "Epoch 217/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5413 - accuracy: 0.3460\n",
      "Epoch 218/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.5215 - accuracy: 0.3540\n",
      "Epoch 219/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4875 - accuracy: 0.3640\n",
      "Epoch 220/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4897 - accuracy: 0.3615\n",
      "Epoch 221/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4486 - accuracy: 0.3860\n",
      "Epoch 222/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.5007 - accuracy: 0.3805\n",
      "Epoch 223/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4697 - accuracy: 0.3750\n",
      "Epoch 224/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4609 - accuracy: 0.3755\n",
      "Epoch 225/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4694 - accuracy: 0.3710\n",
      "Epoch 226/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4395 - accuracy: 0.3935\n",
      "Epoch 227/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4729 - accuracy: 0.3725\n",
      "Epoch 228/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4456 - accuracy: 0.3825\n",
      "Epoch 229/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4445 - accuracy: 0.3895\n",
      "Epoch 230/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4577 - accuracy: 0.3885\n",
      "Epoch 231/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4742 - accuracy: 0.3745\n",
      "Epoch 232/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4083 - accuracy: 0.4130\n",
      "Epoch 233/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4496 - accuracy: 0.3870\n",
      "Epoch 234/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4157 - accuracy: 0.4035\n",
      "Epoch 235/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4053 - accuracy: 0.4050\n",
      "Epoch 236/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4621 - accuracy: 0.3820\n",
      "Epoch 237/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4288 - accuracy: 0.4040\n",
      "Epoch 238/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4464 - accuracy: 0.3695\n",
      "Epoch 239/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.3985 - accuracy: 0.4160\n",
      "Epoch 240/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4413 - accuracy: 0.3700\n",
      "Epoch 241/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4304 - accuracy: 0.3980\n",
      "Epoch 242/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.4098 - accuracy: 0.3960\n",
      "Epoch 243/250\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 1.3876 - accuracy: 0.4165\n",
      "Epoch 244/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4013 - accuracy: 0.4030\n",
      "Epoch 245/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4133 - accuracy: 0.3910\n",
      "Epoch 246/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4200 - accuracy: 0.3835\n",
      "Epoch 247/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4064 - accuracy: 0.3995\n",
      "Epoch 248/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4068 - accuracy: 0.3940\n",
      "Epoch 249/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4231 - accuracy: 0.3945\n",
      "Epoch 250/250\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 1.4117 - accuracy: 0.3995\n",
      "63/63 [==============================] - 0s 2ms/step\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "===============================================\n",
      "Training Classifer Portion of Type Model: Done!\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------#\n",
      " Get Training Error(s)\n",
      "#--------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [23:28<00:00,  1.42it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "#----------------#\n",
      " Get Test Error(s)\n",
      "#----------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [06:33<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------------#\n",
      " Get Testing Error(s): END\n",
      "#------------------------#\n",
      "                                         DNM  MC-Oracle\n",
      "W1-95L                              0.161423   0.000000\n",
      "W1                                  0.168691   0.000000\n",
      "W1-95R                              0.176543   0.000000\n",
      "M-95L                               0.396704   0.396386\n",
      "M                                   0.404971   0.404971\n",
      "M-95R                               0.413845   0.413161\n",
      "N_Par                          923500.000000   0.000000\n",
      "Train_Time                       5564.811233  96.727152\n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000\n",
      "------------------------------------\n",
      "Done: Running script for main model!\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------\")\n",
    "print(\"Running script for main model!\")\n",
    "print(\"------------------------------\")\n",
    "# %run Universal_Measure_Valued_Networks_Backend.ipynb\n",
    "exec(open('Universal_Measure_Valued_Networks_Backend.py').read())\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Done: Running script for main model!\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Run: All Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) *Pointmass Benchmark(s)*\n",
    "These benchmarks consist of subsets of $C(\\mathbb{R}^d,\\mathbb{R})$ which we lift to models in $C(\\mathbb{R}^d,\\cap_{1\\leq q<\\infty}\\mathscr{P}_{q}(\\mathbb{R}))$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to f(x) \\to \\delta_{f(x)}\\in \\cap_{1\\leq q<\\infty}\\mathcal{P}_{q}(\\mathbb{R}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "--------------\n",
      "Training: ENET\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 403/2000 [00:00<00:00, 4021.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Training: ENET - Done\n",
      "---------------------\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 4110.28it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3887.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle          ENET\n",
      "W1-95L                              0.161423   0.000000  1.092669e+06\n",
      "W1                                  0.168691   0.000000  1.208410e+06\n",
      "W1-95R                              0.176543   0.000000  1.331617e+06\n",
      "M-95L                               0.396704   0.396386  9.694256e+02\n",
      "M                                   0.404971   0.404971  1.022149e+03\n",
      "M-95R                               0.413845   0.413161  1.078668e+03\n",
      "N_Par                          923500.000000   0.000000  4.000000e+03\n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09\n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06\n",
      "-----------------\n",
      "Training: K-Ridge\n",
      "-----------------\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   1 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=40)]: Done   6 out of  40 | elapsed:    6.9s remaining:   39.2s\n",
      "[Parallel(n_jobs=40)]: Done  11 out of  40 | elapsed:    8.5s remaining:   22.4s\n",
      "[Parallel(n_jobs=40)]: Done  16 out of  40 | elapsed:   11.8s remaining:   17.8s\n",
      "[Parallel(n_jobs=40)]: Done  21 out of  40 | elapsed:   12.6s remaining:   11.4s\n",
      "[Parallel(n_jobs=40)]: Done  26 out of  40 | elapsed:   13.0s remaining:    7.0s\n",
      "[Parallel(n_jobs=40)]: Done  31 out of  40 | elapsed:   13.4s remaining:    3.9s\n",
      "[Parallel(n_jobs=40)]: Done  36 out of  40 | elapsed:   14.0s remaining:    1.6s\n",
      "[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed:   15.7s finished\n",
      " 10%|█         | 209/2000 [00:00<00:00, 2088.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 3271.60it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2673.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.161423   0.000000  1.092669e+06   \n",
      "W1                                  0.168691   0.000000  1.208410e+06   \n",
      "W1-95R                              0.176543   0.000000  1.331617e+06   \n",
      "M-95L                               0.396704   0.396386  9.694256e+02   \n",
      "M                                   0.404971   0.404971  1.022149e+03   \n",
      "M-95R                               0.413845   0.413161  1.078668e+03   \n",
      "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
      "\n",
      "                                     KRidge  \n",
      "W1-95L                         1.085188e+06  \n",
      "W1                             1.208125e+06  \n",
      "W1-95R                         1.323333e+06  \n",
      "M-95L                          9.660472e+02  \n",
      "M                              1.021983e+03  \n",
      "M-95R                          1.082549e+03  \n",
      "N_Par                          0.000000e+00  \n",
      "Train_Time                     1.704852e+01  \n",
      "Test_Time/MC-Oracle_Test_Time  1.655043e-03  \n",
      "--------------\n",
      "Training: GBRF\n",
      "--------------\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   1 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=40)]: Done   6 out of  40 | elapsed:   12.3s remaining:  1.2min\n",
      "[Parallel(n_jobs=40)]: Done  11 out of  40 | elapsed:   18.5s remaining:   48.8s\n",
      "[Parallel(n_jobs=40)]: Done  16 out of  40 | elapsed:   26.7s remaining:   40.1s\n",
      "[Parallel(n_jobs=40)]: Done  21 out of  40 | elapsed:   29.2s remaining:   26.4s\n",
      "[Parallel(n_jobs=40)]: Done  26 out of  40 | elapsed:   30.1s remaining:   16.2s\n",
      "[Parallel(n_jobs=40)]: Done  31 out of  40 | elapsed:   31.0s remaining:    9.0s\n",
      "[Parallel(n_jobs=40)]: Done  36 out of  40 | elapsed:   31.6s remaining:    3.5s\n",
      "[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed:   32.1s finished\n",
      " 21%|██        | 423/2000 [00:00<00:00, 4223.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 4155.07it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 4098.32it/s]\n",
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.161423   0.000000  1.092669e+06   \n",
      "W1                                  0.168691   0.000000  1.208410e+06   \n",
      "W1-95R                              0.176543   0.000000  1.331617e+06   \n",
      "M-95L                               0.396704   0.396386  9.694256e+02   \n",
      "M                                   0.404971   0.404971  1.022149e+03   \n",
      "M-95R                               0.413845   0.413161  1.078668e+03   \n",
      "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
      "\n",
      "                                     KRidge          GBRF  \n",
      "W1-95L                         1.085188e+06  9.386122e+05  \n",
      "W1                             1.208125e+06  9.531860e+05  \n",
      "W1-95R                         1.323333e+06  9.688458e+05  \n",
      "M-95L                          9.660472e+02  9.666866e+02  \n",
      "M                              1.021983e+03  9.746349e+02  \n",
      "M-95R                          1.082549e+03  9.821129e+02  \n",
      "N_Par                          0.000000e+00  1.917000e+06  \n",
      "Train_Time                     1.704852e+01  3.574519e+01  \n",
      "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  \n",
      "-------------\n",
      "Training: DNN\n",
      "-------------\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Done   1 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=40)]: Done   6 out of  40 | elapsed:  5.6min remaining: 31.6min\n",
      "[Parallel(n_jobs=40)]: Done  11 out of  40 | elapsed:  9.2min remaining: 24.2min\n",
      "[Parallel(n_jobs=40)]: Done  16 out of  40 | elapsed: 10.2min remaining: 15.3min\n",
      "[Parallel(n_jobs=40)]: Done  21 out of  40 | elapsed: 14.7min remaining: 13.3min\n",
      "[Parallel(n_jobs=40)]: Done  26 out of  40 | elapsed: 16.6min remaining:  8.9min\n",
      "[Parallel(n_jobs=40)]: Done  31 out of  40 | elapsed: 17.3min remaining:  5.0min\n",
      "[Parallel(n_jobs=40)]: Done  36 out of  40 | elapsed: 23.5min remaining:  2.6min\n",
      "[Parallel(n_jobs=40)]: Done  40 out of  40 | elapsed: 23.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 967.7452 - mse: 1047632.2500 - mae: 967.7452 - mape: 99.9959\n",
      "Epoch 2/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 967.6370 - mse: 1047416.7500 - mae: 967.6370 - mape: 99.9834\n",
      "Epoch 3/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 967.5059 - mse: 1047154.1875 - mae: 967.5059 - mape: 99.9686\n",
      "Epoch 4/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 967.3374 - mse: 1046816.4375 - mae: 967.3374 - mape: 99.9495\n",
      "Epoch 5/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 967.1155 - mse: 1046369.8125 - mae: 967.1155 - mape: 99.9245\n",
      "Epoch 6/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 966.8219 - mse: 1045777.2500 - mae: 966.8219 - mape: 99.8915\n",
      "Epoch 7/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 966.4358 - mse: 1045000.5000 - mae: 966.4358 - mape: 99.8479\n",
      "Epoch 8/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 965.9333 - mse: 1043986.1250 - mae: 965.9333 - mape: 99.7915\n",
      "Epoch 9/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 965.2872 - mse: 1042679.9375 - mae: 965.2872 - mape: 99.7191\n",
      "Epoch 10/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 964.4665 - mse: 1041021.8750 - mae: 964.4665 - mape: 99.6272\n",
      "Epoch 11/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 963.4366 - mse: 1038952.1875 - mae: 963.4366 - mape: 99.5114\n",
      "Epoch 12/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 962.1573 - mse: 1036352.7500 - mae: 962.1573 - mape: 99.3689\n",
      "Epoch 13/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 960.5837 - mse: 1033186.3750 - mae: 960.5837 - mape: 99.1927\n",
      "Epoch 14/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 958.6632 - mse: 1029307.5625 - mae: 958.6632 - mape: 98.9791\n",
      "Epoch 15/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 956.3469 - mse: 1024661.0625 - mae: 956.3469 - mape: 98.7193\n",
      "Epoch 16/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 953.5894 - mse: 1019107.5000 - mae: 953.5894 - mape: 98.4133\n",
      "Epoch 17/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 950.3585 - mse: 1012633.0625 - mae: 950.3585 - mape: 98.0526\n",
      "Epoch 18/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 946.6348 - mse: 1005198.3750 - mae: 946.6348 - mape: 97.6385\n",
      "Epoch 19/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 942.4161 - mse: 996859.3750 - mae: 942.4161 - mape: 97.1658\n",
      "Epoch 20/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 937.7080 - mse: 987560.5000 - mae: 937.7080 - mape: 96.6396\n",
      "Epoch 21/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 932.5287 - mse: 977436.3750 - mae: 932.5287 - mape: 96.0580\n",
      "Epoch 22/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 926.8941 - mse: 966517.7500 - mae: 926.8941 - mape: 95.4234\n",
      "Epoch 23/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 920.8120 - mse: 954749.8125 - mae: 920.8120 - mape: 94.7414\n",
      "Epoch 24/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 914.2946 - mse: 942277.3750 - mae: 914.2946 - mape: 94.0062\n",
      "Epoch 25/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 907.3253 - mse: 928997.7500 - mae: 907.3253 - mape: 93.2247\n",
      "Epoch 26/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 899.8904 - mse: 914906.9375 - mae: 899.8904 - mape: 92.3921\n",
      "Epoch 27/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 891.9734 - mse: 900173.9375 - mae: 891.9734 - mape: 91.4955\n",
      "Epoch 28/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 883.5361 - mse: 884371.8750 - mae: 883.5361 - mape: 90.5558\n",
      "Epoch 29/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 874.5536 - mse: 867835.0000 - mae: 874.5536 - mape: 89.5481\n",
      "Epoch 30/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 864.9904 - mse: 850419.0000 - mae: 864.9904 - mape: 88.4753\n",
      "Epoch 31/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 854.8140 - mse: 832063.2500 - mae: 854.8140 - mape: 87.3347\n",
      "Epoch 32/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 843.9899 - mse: 812707.1875 - mae: 843.9899 - mape: 86.1243\n",
      "Epoch 33/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 832.4799 - mse: 792389.9375 - mae: 832.4799 - mape: 84.8393\n",
      "Epoch 34/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 820.2614 - mse: 771222.8750 - mae: 820.2614 - mape: 83.4689\n",
      "Epoch 35/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 807.2925 - mse: 748987.1875 - mae: 807.2925 - mape: 82.0196\n",
      "Epoch 36/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 793.5657 - mse: 725805.3750 - mae: 793.5657 - mape: 80.4883\n",
      "Epoch 37/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 779.0396 - mse: 701744.5000 - mae: 779.0396 - mape: 78.8628\n",
      "Epoch 38/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 763.6968 - mse: 676858.6250 - mae: 763.6968 - mape: 77.1459\n",
      "Epoch 39/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 747.5277 - mse: 651002.7500 - mae: 747.5277 - mape: 75.3430\n",
      "Epoch 40/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 730.4742 - mse: 624389.6250 - mae: 730.4742 - mape: 73.4393\n",
      "Epoch 41/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 712.5405 - mse: 597065.5625 - mae: 712.5405 - mape: 71.4374\n",
      "Epoch 42/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 693.7110 - mse: 569085.0000 - mae: 693.7110 - mape: 69.3373\n",
      "Epoch 43/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 673.9583 - mse: 540538.8125 - mae: 673.9583 - mape: 67.1291\n",
      "Epoch 44/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 653.2745 - mse: 511508.5938 - mae: 653.2745 - mape: 64.8166\n",
      "Epoch 45/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 631.6171 - mse: 482075.9375 - mae: 631.6171 - mape: 62.3976\n",
      "Epoch 46/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 609.0085 - mse: 452435.2812 - mae: 609.0085 - mape: 59.8631\n",
      "Epoch 47/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 585.3987 - mse: 422561.3125 - mae: 585.3987 - mape: 57.2228\n",
      "Epoch 48/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 560.7928 - mse: 392604.6250 - mae: 560.7928 - mape: 54.4731\n",
      "Epoch 49/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 535.1605 - mse: 362739.7188 - mae: 535.1605 - mape: 51.6085\n",
      "Epoch 50/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 508.5003 - mse: 333361.0000 - mae: 508.5003 - mape: 48.6224\n",
      "Epoch 51/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 480.7865 - mse: 303896.4375 - mae: 480.7865 - mape: 45.5419\n",
      "Epoch 52/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 452.1215 - mse: 275510.5312 - mae: 452.1215 - mape: 42.3246\n",
      "Epoch 53/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 423.2348 - mse: 248220.1250 - mae: 423.2348 - mape: 39.1226\n",
      "Epoch 54/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 394.8114 - mse: 222116.2812 - mae: 394.8114 - mape: 36.0564\n",
      "Epoch 55/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 367.4035 - mse: 197855.2188 - mae: 367.4035 - mape: 33.1916\n",
      "Epoch 56/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 341.8304 - mse: 175649.8594 - mae: 341.8304 - mape: 30.6220\n",
      "Epoch 57/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 319.0352 - mse: 156065.2500 - mae: 319.0352 - mape: 28.4587\n",
      "Epoch 58/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 298.8379 - mse: 138757.4219 - mae: 298.8379 - mape: 26.6508\n",
      "Epoch 59/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 281.2049 - mse: 123482.3516 - mae: 281.2049 - mape: 25.2131\n",
      "Epoch 60/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 266.0350 - mse: 110432.1562 - mae: 266.0350 - mape: 24.0856\n",
      "Epoch 61/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 252.8852 - mse: 99053.8594 - mae: 252.8852 - mape: 23.2098\n",
      "Epoch 62/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 241.3998 - mse: 89412.8281 - mae: 241.3998 - mape: 22.5299\n",
      "Epoch 63/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 231.2669 - mse: 80994.4141 - mae: 231.2669 - mape: 22.0186\n",
      "Epoch 64/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 222.5907 - mse: 73880.2812 - mae: 222.5907 - mape: 21.6765\n",
      "Epoch 65/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 215.0808 - mse: 67937.7031 - mae: 215.0808 - mape: 21.4345\n",
      "Epoch 66/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 208.5027 - mse: 62983.1562 - mae: 208.5027 - mape: 21.2956\n",
      "Epoch 67/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 202.7743 - mse: 58928.8789 - mae: 202.7743 - mape: 21.2041\n",
      "Epoch 68/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 198.0371 - mse: 55604.8672 - mae: 198.0371 - mape: 21.2073\n",
      "Epoch 69/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 194.2090 - mse: 53041.6406 - mae: 194.2090 - mape: 21.2557\n",
      "Epoch 70/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 191.1131 - mse: 51134.6172 - mae: 191.1131 - mape: 21.2743\n",
      "Epoch 71/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 188.6501 - mse: 49563.3164 - mae: 188.6501 - mape: 21.3675\n",
      "Epoch 72/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 186.5809 - mse: 48354.5469 - mae: 186.5809 - mape: 21.4042\n",
      "Epoch 73/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 184.9179 - mse: 47343.1719 - mae: 184.9179 - mape: 21.5187\n",
      "Epoch 74/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 183.4059 - mse: 46563.9297 - mae: 183.4059 - mape: 21.4902\n",
      "Epoch 75/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 182.0512 - mse: 45850.9492 - mae: 182.0512 - mape: 21.5403\n",
      "Epoch 76/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 180.7761 - mse: 45226.5898 - mae: 180.7761 - mape: 21.5083\n",
      "Epoch 77/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 179.5816 - mse: 44637.2070 - mae: 179.5816 - mape: 21.5263\n",
      "Epoch 78/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 178.4581 - mse: 44112.8203 - mae: 178.4581 - mape: 21.5616\n",
      "Epoch 79/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 177.3485 - mse: 43602.5859 - mae: 177.3485 - mape: 21.5089\n",
      "Epoch 80/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 176.2732 - mse: 43104.9570 - mae: 176.2732 - mape: 21.4108\n",
      "Epoch 81/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 175.2137 - mse: 42612.6953 - mae: 175.2137 - mape: 21.3405\n",
      "Epoch 82/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 174.1748 - mse: 42149.7852 - mae: 174.1748 - mape: 21.2892\n",
      "Epoch 83/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 173.1341 - mse: 41671.3672 - mae: 173.1341 - mape: 21.2152\n",
      "Epoch 84/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 172.0931 - mse: 41203.3906 - mae: 172.0931 - mape: 21.1506\n",
      "Epoch 85/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 171.0684 - mse: 40716.0156 - mae: 171.0684 - mape: 20.9860\n",
      "Epoch 86/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 170.0702 - mse: 40240.1758 - mae: 170.0702 - mape: 20.8590\n",
      "Epoch 87/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 168.9826 - mse: 39770.3398 - mae: 168.9826 - mape: 20.8413\n",
      "Epoch 88/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 167.9475 - mse: 39286.5664 - mae: 167.9475 - mape: 20.7059\n",
      "Epoch 89/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 166.9117 - mse: 38817.0234 - mae: 166.9117 - mape: 20.6040\n",
      "Epoch 90/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 165.8576 - mse: 38338.1445 - mae: 165.8576 - mape: 20.4984\n",
      "Epoch 91/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 164.8203 - mse: 37843.3828 - mae: 164.8203 - mape: 20.3263\n",
      "Epoch 92/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 163.7525 - mse: 37377.2773 - mae: 163.7525 - mape: 20.2630\n",
      "Epoch 93/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 162.6982 - mse: 36890.9570 - mae: 162.6982 - mape: 20.1051\n",
      "Epoch 94/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 161.6229 - mse: 36417.0781 - mae: 161.6229 - mape: 20.0251\n",
      "Epoch 95/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 160.5452 - mse: 35941.0078 - mae: 160.5452 - mape: 19.9216\n",
      "Epoch 96/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 159.4596 - mse: 35418.8984 - mae: 159.4596 - mape: 19.7200\n",
      "Epoch 97/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 158.3713 - mse: 34948.9336 - mae: 158.3713 - mape: 19.6048\n",
      "Epoch 98/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 157.2632 - mse: 34467.0039 - mae: 157.2632 - mape: 19.4935\n",
      "Epoch 99/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 156.1293 - mse: 33981.1836 - mae: 156.1293 - mape: 19.3826\n",
      "Epoch 100/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 155.0109 - mse: 33491.7695 - mae: 155.0109 - mape: 19.2586\n",
      "Epoch 101/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 153.9034 - mse: 33035.1875 - mae: 153.9034 - mape: 19.1739\n",
      "Epoch 102/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 152.7828 - mse: 32521.9121 - mae: 152.7828 - mape: 18.9701\n",
      "Epoch 103/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 151.5858 - mse: 32033.9668 - mae: 151.5858 - mape: 18.8879\n",
      "Epoch 104/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 150.4286 - mse: 31550.7227 - mae: 150.4286 - mape: 18.7533\n",
      "Epoch 105/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 149.2735 - mse: 31041.5840 - mae: 149.2735 - mape: 18.5656\n",
      "Epoch 106/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 148.1014 - mse: 30557.8965 - mae: 148.1014 - mape: 18.4645\n",
      "Epoch 107/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 146.9044 - mse: 30049.5859 - mae: 146.9044 - mape: 18.2848\n",
      "Epoch 108/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 145.7381 - mse: 29578.8984 - mae: 145.7381 - mape: 18.1603\n",
      "Epoch 109/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 144.4866 - mse: 29097.3047 - mae: 144.4866 - mape: 18.0657\n",
      "Epoch 110/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 143.2774 - mse: 28597.6523 - mae: 143.2774 - mape: 17.8961\n",
      "Epoch 111/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 142.0425 - mse: 28096.8887 - mae: 142.0425 - mape: 17.7401\n",
      "Epoch 112/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 140.8211 - mse: 27609.4160 - mae: 140.8211 - mape: 17.5856\n",
      "Epoch 113/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 139.5652 - mse: 27148.5586 - mae: 139.5652 - mape: 17.4999\n",
      "Epoch 114/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 138.3289 - mse: 26642.5215 - mae: 138.3289 - mape: 17.3215\n",
      "Epoch 115/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 137.1037 - mse: 26188.6191 - mae: 137.1037 - mape: 17.2196\n",
      "Epoch 116/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 135.8240 - mse: 25670.2500 - mae: 135.8240 - mape: 17.0050\n",
      "Epoch 117/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 134.5631 - mse: 25214.7598 - mae: 134.5631 - mape: 16.8978\n",
      "Epoch 118/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 133.3085 - mse: 24720.0801 - mae: 133.3085 - mape: 16.6774\n",
      "Epoch 119/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 131.9710 - mse: 24223.2363 - mae: 131.9710 - mape: 16.5261\n",
      "Epoch 120/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 130.7115 - mse: 23801.8398 - mae: 130.7115 - mape: 16.4755\n",
      "Epoch 121/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 129.4098 - mse: 23282.7617 - mae: 129.4098 - mape: 16.2269\n",
      "Epoch 122/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 128.0793 - mse: 22798.2383 - mae: 128.0793 - mape: 16.0507\n",
      "Epoch 123/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 126.7558 - mse: 22336.1641 - mae: 126.7558 - mape: 15.9157\n",
      "Epoch 124/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 125.4316 - mse: 21863.7578 - mae: 125.4316 - mape: 15.7595\n",
      "Epoch 125/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 124.0686 - mse: 21420.7773 - mae: 124.0686 - mape: 15.6608\n",
      "Epoch 126/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 122.7215 - mse: 20941.4863 - mae: 122.7215 - mape: 15.4685\n",
      "Epoch 127/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 121.3421 - mse: 20458.9355 - mae: 121.3421 - mape: 15.2804\n",
      "Epoch 128/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 119.9694 - mse: 20012.3457 - mae: 119.9694 - mape: 15.1733\n",
      "Epoch 129/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 118.6136 - mse: 19514.8359 - mae: 118.6136 - mape: 14.9043\n",
      "Epoch 130/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 117.1816 - mse: 19080.7129 - mae: 117.1816 - mape: 14.8170\n",
      "Epoch 131/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 115.7201 - mse: 18592.2871 - mae: 115.7201 - mape: 14.6117\n",
      "Epoch 132/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 114.3100 - mse: 18102.3477 - mae: 114.3100 - mape: 14.3544\n",
      "Epoch 133/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 112.8429 - mse: 17670.4727 - mae: 112.8429 - mape: 14.2551\n",
      "Epoch 134/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 111.3743 - mse: 17230.7461 - mae: 111.3743 - mape: 14.1310\n",
      "Epoch 135/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 109.8817 - mse: 16733.2578 - mae: 109.8817 - mape: 13.8711\n",
      "Epoch 136/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 108.3973 - mse: 16300.6943 - mae: 108.3973 - mape: 13.7441\n",
      "Epoch 137/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 106.8672 - mse: 15820.2725 - mae: 106.8672 - mape: 13.5026\n",
      "Epoch 138/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 105.3709 - mse: 15339.9307 - mae: 105.3709 - mape: 13.2333\n",
      "Epoch 139/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 103.8289 - mse: 14917.0391 - mae: 103.8289 - mape: 13.1489\n",
      "Epoch 140/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 102.2749 - mse: 14485.5088 - mae: 102.2749 - mape: 12.9850\n",
      "Epoch 141/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 100.6657 - mse: 14009.7715 - mae: 100.6657 - mape: 12.7336\n",
      "Epoch 142/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 99.0764 - mse: 13595.7783 - mae: 99.0764 - mape: 12.6134\n",
      "Epoch 143/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 97.4275 - mse: 13095.2979 - mae: 97.4275 - mape: 12.2940\n",
      "Epoch 144/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 95.7793 - mse: 12680.2539 - mae: 95.7793 - mape: 12.1691\n",
      "Epoch 145/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 94.1391 - mse: 12216.5615 - mae: 94.1391 - mape: 11.9008\n",
      "Epoch 146/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 92.4159 - mse: 11790.3789 - mae: 92.4159 - mape: 11.7381\n",
      "Epoch 147/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 90.6982 - mse: 11352.8555 - mae: 90.6982 - mape: 11.5518\n",
      "Epoch 148/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 88.9316 - mse: 10901.1914 - mae: 88.9316 - mape: 11.3060\n",
      "Epoch 149/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 87.1850 - mse: 10488.1768 - mae: 87.1850 - mape: 11.1401\n",
      "Epoch 150/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 85.3882 - mse: 10014.8398 - mae: 85.3882 - mape: 10.8228\n",
      "Epoch 151/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 83.5563 - mse: 9585.1230 - mae: 83.5563 - mape: 10.6050\n",
      "Epoch 152/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 81.6754 - mse: 9155.6602 - mae: 81.6754 - mape: 10.3824\n",
      "Epoch 153/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 79.8158 - mse: 8764.0762 - mae: 79.8158 - mape: 10.2347\n",
      "Epoch 154/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 77.8770 - mse: 8289.0586 - mae: 77.8770 - mape: 9.8480\n",
      "Epoch 155/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 75.8923 - mse: 7884.2900 - mae: 75.8923 - mape: 9.6739\n",
      "Epoch 156/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 73.8876 - mse: 7480.6631 - mae: 73.8876 - mape: 9.4618\n",
      "Epoch 157/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 71.9030 - mse: 7049.8496 - mae: 71.9030 - mape: 9.1405\n",
      "Epoch 158/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 69.8037 - mse: 6655.8687 - mae: 69.8037 - mape: 8.9528\n",
      "Epoch 159/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 67.7156 - mse: 6262.0151 - mae: 67.7156 - mape: 8.7022\n",
      "Epoch 160/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 65.6597 - mse: 5838.2749 - mae: 65.6597 - mape: 8.3236\n",
      "Epoch 161/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 63.3920 - mse: 5465.4346 - mae: 63.3920 - mape: 8.1393\n",
      "Epoch 162/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 61.1891 - mse: 5064.2876 - mae: 61.1891 - mape: 7.7929\n",
      "Epoch 163/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 58.9356 - mse: 4703.4966 - mae: 58.9356 - mape: 7.5618\n",
      "Epoch 164/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 56.6843 - mse: 4332.7632 - mae: 56.6843 - mape: 7.2375\n",
      "Epoch 165/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 54.3354 - mse: 3979.2937 - mae: 54.3354 - mape: 6.9674\n",
      "Epoch 166/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 51.9823 - mse: 3648.9958 - mae: 51.9823 - mape: 6.7394\n",
      "Epoch 167/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 49.5455 - mse: 3288.7024 - mae: 49.5455 - mape: 6.3644\n",
      "Epoch 168/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 47.1015 - mse: 2957.2051 - mae: 47.1015 - mape: 6.0272\n",
      "Epoch 169/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 44.6072 - mse: 2657.6865 - mae: 44.6072 - mape: 5.7791\n",
      "Epoch 170/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 42.0813 - mse: 2349.8049 - mae: 42.0813 - mape: 5.4249\n",
      "Epoch 171/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 39.4921 - mse: 2072.2397 - mae: 39.4921 - mape: 5.1352\n",
      "Epoch 172/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 36.9146 - mse: 1788.2583 - mae: 36.9146 - mape: 4.7322\n",
      "Epoch 173/250\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 34.1939 - mse: 1535.5613 - mae: 34.1939 - mape: 4.4342\n",
      "Epoch 174/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 31.4685 - mse: 1295.8922 - mae: 31.4685 - mape: 4.0823\n",
      "Epoch 175/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 28.6357 - mse: 1064.8698 - mae: 28.6357 - mape: 3.6932\n",
      "Epoch 176/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 25.7982 - mse: 866.5289 - mae: 25.7982 - mape: 3.3460\n",
      "Epoch 177/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 22.8945 - mse: 681.9202 - mae: 22.8945 - mape: 2.9866\n",
      "Epoch 178/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 20.0681 - mse: 524.0420 - mae: 20.0681 - mape: 2.5877\n",
      "Epoch 179/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 17.1353 - mse: 386.4983 - mae: 17.1353 - mape: 2.2182\n",
      "Epoch 180/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 14.2822 - mse: 276.9375 - mae: 14.2822 - mape: 1.8597\n",
      "Epoch 181/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 11.6258 - mse: 192.5863 - mae: 11.6258 - mape: 1.5078\n",
      "Epoch 182/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 9.3875 - mse: 134.8107 - mae: 9.3875 - mape: 1.2148\n",
      "Epoch 183/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 7.8602 - mse: 98.8748 - mae: 7.8602 - mape: 1.0416\n",
      "Epoch 184/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.8666 - mse: 76.8487 - mae: 6.8666 - mape: 0.9092\n",
      "Epoch 185/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 5.9610 - mse: 59.2642 - mae: 5.9610 - mape: 0.8019\n",
      "Epoch 186/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.2449 - mse: 46.1369 - mae: 5.2449 - mape: 0.7062\n",
      "Epoch 187/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.6661 - mse: 37.0804 - mae: 4.6661 - mape: 0.6232\n",
      "Epoch 188/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 4.2321 - mse: 30.9728 - mae: 4.2321 - mape: 0.5602\n",
      "Epoch 189/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.9511 - mse: 27.3114 - mae: 3.9511 - mape: 0.5180\n",
      "Epoch 190/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.7548 - mse: 24.6972 - mae: 3.7548 - mape: 0.4874\n",
      "Epoch 191/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.6480 - mse: 23.5106 - mae: 3.6480 - mape: 0.4679\n",
      "Epoch 192/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4863 - mse: 22.0555 - mae: 3.4863 - mape: 0.4437\n",
      "Epoch 193/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3971 - mse: 21.0588 - mae: 3.3971 - mape: 0.4264\n",
      "Epoch 194/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3197 - mse: 20.4995 - mae: 3.3197 - mape: 0.4155\n",
      "Epoch 195/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2752 - mse: 19.8106 - mae: 3.2752 - mape: 0.4077\n",
      "Epoch 196/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2655 - mse: 19.3633 - mae: 3.2655 - mape: 0.4015\n",
      "Epoch 197/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2004 - mse: 18.6353 - mae: 3.2004 - mape: 0.3931\n",
      "Epoch 198/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1456 - mse: 18.4000 - mae: 3.1456 - mape: 0.3853\n",
      "Epoch 199/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0859 - mse: 17.6014 - mae: 3.0859 - mape: 0.3774\n",
      "Epoch 200/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0718 - mse: 17.4911 - mae: 3.0718 - mape: 0.3737\n",
      "Epoch 201/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0419 - mse: 17.1534 - mae: 3.0419 - mape: 0.3707\n",
      "Epoch 202/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0314 - mse: 17.0699 - mae: 3.0314 - mape: 0.3673\n",
      "Epoch 203/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9981 - mse: 16.5423 - mae: 2.9981 - mape: 0.3637\n",
      "Epoch 204/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9948 - mse: 16.4906 - mae: 2.9948 - mape: 0.3630\n",
      "Epoch 205/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9649 - mse: 16.1060 - mae: 2.9649 - mape: 0.3593\n",
      "Epoch 206/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9610 - mse: 15.9103 - mae: 2.9610 - mape: 0.3554\n",
      "Epoch 207/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9155 - mse: 15.6273 - mae: 2.9155 - mape: 0.3522\n",
      "Epoch 208/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8870 - mse: 15.2832 - mae: 2.8870 - mape: 0.3481\n",
      "Epoch 209/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8646 - mse: 15.3455 - mae: 2.8646 - mape: 0.3460\n",
      "Epoch 210/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8448 - mse: 15.0742 - mae: 2.8448 - mape: 0.3440\n",
      "Epoch 211/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8915 - mse: 15.0840 - mae: 2.8915 - mape: 0.3424\n",
      "Epoch 212/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8442 - mse: 14.5238 - mae: 2.8442 - mape: 0.3406\n",
      "Epoch 213/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7905 - mse: 14.5049 - mae: 2.7905 - mape: 0.3331\n",
      "Epoch 214/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8259 - mse: 14.2677 - mae: 2.8259 - mape: 0.3363\n",
      "Epoch 215/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7981 - mse: 14.7687 - mae: 2.7981 - mape: 0.3350\n",
      "Epoch 216/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7629 - mse: 14.0504 - mae: 2.7629 - mape: 0.3305\n",
      "Epoch 217/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7614 - mse: 13.7608 - mae: 2.7614 - mape: 0.3303\n",
      "Epoch 218/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7331 - mse: 13.8283 - mae: 2.7331 - mape: 0.3265\n",
      "Epoch 219/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7186 - mse: 13.6243 - mae: 2.7186 - mape: 0.3280\n",
      "Epoch 220/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6870 - mse: 13.3349 - mae: 2.6870 - mape: 0.3230\n",
      "Epoch 221/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6994 - mse: 13.6511 - mae: 2.6994 - mape: 0.3217\n",
      "Epoch 222/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6739 - mse: 13.0901 - mae: 2.6739 - mape: 0.3207\n",
      "Epoch 223/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6627 - mse: 13.0847 - mae: 2.6627 - mape: 0.3203\n",
      "Epoch 224/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6541 - mse: 12.9533 - mae: 2.6541 - mape: 0.3179\n",
      "Epoch 225/250\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6299 - mse: 12.8326 - mae: 2.6299 - mape: 0.3162\n",
      "Epoch 226/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6268 - mse: 12.7546 - mae: 2.6268 - mape: 0.3127\n",
      "Epoch 227/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5987 - mse: 12.5482 - mae: 2.5987 - mape: 0.3102\n",
      "Epoch 228/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6035 - mse: 12.6055 - mae: 2.6035 - mape: 0.3105\n",
      "Epoch 229/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5897 - mse: 12.4032 - mae: 2.5897 - mape: 0.3094\n",
      "Epoch 230/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6337 - mse: 12.4752 - mae: 2.6337 - mape: 0.3125\n",
      "Epoch 231/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5682 - mse: 12.1515 - mae: 2.5682 - mape: 0.3053\n",
      "Epoch 232/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5601 - mse: 12.0090 - mae: 2.5601 - mape: 0.3048\n",
      "Epoch 233/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5451 - mse: 12.0986 - mae: 2.5451 - mape: 0.3027\n",
      "Epoch 234/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5501 - mse: 11.9224 - mae: 2.5501 - mape: 0.3025\n",
      "Epoch 235/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5179 - mse: 11.7690 - mae: 2.5179 - mape: 0.2991\n",
      "Epoch 236/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5330 - mse: 11.6704 - mae: 2.5330 - mape: 0.3014\n",
      "Epoch 237/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4841 - mse: 11.6484 - mae: 2.4841 - mape: 0.2953\n",
      "Epoch 238/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4827 - mse: 11.2974 - mae: 2.4827 - mape: 0.2959\n",
      "Epoch 239/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4745 - mse: 11.4830 - mae: 2.4745 - mape: 0.2932\n",
      "Epoch 240/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4947 - mse: 11.4255 - mae: 2.4947 - mape: 0.2954\n",
      "Epoch 241/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4567 - mse: 11.2314 - mae: 2.4567 - mape: 0.2930\n",
      "Epoch 242/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4463 - mse: 11.1441 - mae: 2.4463 - mape: 0.2911\n",
      "Epoch 243/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4406 - mse: 11.0274 - mae: 2.4406 - mape: 0.2896\n",
      "Epoch 244/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4126 - mse: 10.8784 - mae: 2.4126 - mape: 0.2852\n",
      "Epoch 245/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4083 - mse: 10.5965 - mae: 2.4083 - mape: 0.2858\n",
      "Epoch 246/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3906 - mse: 10.7217 - mae: 2.3906 - mape: 0.2842\n",
      "Epoch 247/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3823 - mse: 10.4616 - mae: 2.3823 - mape: 0.2826\n",
      "Epoch 248/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3834 - mse: 10.4777 - mae: 2.3834 - mape: 0.2830\n",
      "Epoch 249/250\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3758 - mse: 10.4672 - mae: 2.3758 - mape: 0.2797\n",
      "Epoch 250/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3415 - mse: 10.2497 - mae: 2.3415 - mape: 0.2788\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "#------------#"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 3369.79it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3762.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.161423   0.000000  1.092669e+06   \n",
      "W1                                  0.168691   0.000000  1.208410e+06   \n",
      "W1-95R                              0.176543   0.000000  1.331617e+06   \n",
      "M-95L                               0.396704   0.396386  9.694256e+02   \n",
      "M                                   0.404971   0.404971  1.022149e+03   \n",
      "M-95R                               0.413845   0.413161  1.078668e+03   \n",
      "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
      "\n",
      "                                     KRidge          GBRF           DNN  \n",
      "W1-95L                         1.085188e+06  9.386122e+05  1.089221e+06  \n",
      "W1                             1.208125e+06  9.531860e+05  1.204600e+06  \n",
      "W1-95R                         1.323333e+06  9.688458e+05  1.327178e+06  \n",
      "M-95L                          9.660472e+02  9.666866e+02  9.639629e+02  \n",
      "M                              1.021983e+03  9.746349e+02  1.020487e+03  \n",
      "M-95R                          1.082549e+03  9.821129e+02  1.080823e+03  \n",
      "N_Par                          0.000000e+00  1.917000e+06  3.224010e+05  \n",
      "Train_Time                     1.704852e+01  3.574519e+01  1.493983e+03  \n",
      "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  5.394637e-03  \n"
     ]
    }
   ],
   "source": [
    "exec(open('CV_Grid.py').read())\n",
    "# Notebook Mode:\n",
    "# %run Evaluation.ipynb\n",
    "# %run Benchmarks_Model_Builder_Pointmass_Based.ipynb\n",
    "# Terminal Mode (Default):\n",
    "exec(open('Evaluation.py').read())\n",
    "exec(open('Benchmarks_Model_Builder_Pointmass_Based.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Point-Mass Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.000128   0.000000  1.018309e+06   \n",
      "W1                                  0.000185   0.000000  1.045608e+06   \n",
      "W1-95R                              0.000307   0.000000  1.075119e+06   \n",
      "M-95L                               0.002600   0.002595  9.517059e+02   \n",
      "M                                   0.002797   0.002797  9.668141e+02   \n",
      "M-95R                               0.003027   0.003022  9.814793e+02   \n",
      "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
      "\n",
      "                                     KRidge          GBRF           DNN  \n",
      "W1-95L                         1.014195e+06  9.329218e+05  1.013288e+06  \n",
      "W1                             1.045607e+06  9.368544e+05  1.044346e+06  \n",
      "W1-95R                         1.072116e+06  9.408291e+05  1.074491e+06  \n",
      "M-95L                          9.527161e+02  9.646336e+02  9.526977e+02  \n",
      "M                              9.668135e+02  9.668141e+02  9.662117e+02  \n",
      "M-95R                          9.816957e+02  9.687405e+02  9.812708e+02  \n",
      "N_Par                          0.000000e+00  1.917000e+06  3.224010e+05  \n",
      "Train_Time                     1.704852e+01  3.574519e+01  1.493983e+03  \n",
      "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  5.394637e-03  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.018309e+06</td>\n",
       "      <td>1.014195e+06</td>\n",
       "      <td>9.329218e+05</td>\n",
       "      <td>1.013288e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.045608e+06</td>\n",
       "      <td>1.045607e+06</td>\n",
       "      <td>9.368544e+05</td>\n",
       "      <td>1.044346e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.075119e+06</td>\n",
       "      <td>1.072116e+06</td>\n",
       "      <td>9.408291e+05</td>\n",
       "      <td>1.074491e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>9.517059e+02</td>\n",
       "      <td>9.527161e+02</td>\n",
       "      <td>9.646336e+02</td>\n",
       "      <td>9.526977e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>9.668141e+02</td>\n",
       "      <td>9.668135e+02</td>\n",
       "      <td>9.668141e+02</td>\n",
       "      <td>9.662117e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>9.814793e+02</td>\n",
       "      <td>9.816957e+02</td>\n",
       "      <td>9.687405e+02</td>\n",
       "      <td>9.812708e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>923500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.917000e+06</td>\n",
       "      <td>3.224010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>5564.811233</td>\n",
       "      <td>96.727152</td>\n",
       "      <td>1.619966e+09</td>\n",
       "      <td>1.704852e+01</td>\n",
       "      <td>3.574519e+01</td>\n",
       "      <td>1.493983e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.006567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.935642e-06</td>\n",
       "      <td>1.655043e-03</td>\n",
       "      <td>9.877405e-04</td>\n",
       "      <td>5.394637e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                              0.000128   0.000000  1.018309e+06   \n",
       "W1                                  0.000185   0.000000  1.045608e+06   \n",
       "W1-95R                              0.000307   0.000000  1.075119e+06   \n",
       "M-95L                               0.002600   0.002595  9.517059e+02   \n",
       "M                                   0.002797   0.002797  9.668141e+02   \n",
       "M-95R                               0.003027   0.003022  9.814793e+02   \n",
       "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
       "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
       "\n",
       "                                     KRidge          GBRF           DNN  \n",
       "W1-95L                         1.014195e+06  9.329218e+05  1.013288e+06  \n",
       "W1                             1.045607e+06  9.368544e+05  1.044346e+06  \n",
       "W1-95R                         1.072116e+06  9.408291e+05  1.074491e+06  \n",
       "M-95L                          9.527161e+02  9.646336e+02  9.526977e+02  \n",
       "M                              9.668135e+02  9.668141e+02  9.662117e+02  \n",
       "M-95R                          9.816957e+02  9.687405e+02  9.812708e+02  \n",
       "N_Par                          0.000000e+00  1.917000e+06  3.224010e+05  \n",
       "Train_Time                     1.704852e+01  3.574519e+01  1.493983e+03  \n",
       "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  5.394637e-03  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.161423   0.000000  1.092669e+06   \n",
      "W1                                  0.168691   0.000000  1.208410e+06   \n",
      "W1-95R                              0.176543   0.000000  1.331617e+06   \n",
      "M-95L                               0.396704   0.396386  9.694256e+02   \n",
      "M                                   0.404971   0.404971  1.022149e+03   \n",
      "M-95R                               0.413845   0.413161  1.078668e+03   \n",
      "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
      "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
      "\n",
      "                                     KRidge          GBRF           DNN  \n",
      "W1-95L                         1.085188e+06  9.386122e+05  1.089221e+06  \n",
      "W1                             1.208125e+06  9.531860e+05  1.204600e+06  \n",
      "W1-95R                         1.323333e+06  9.688458e+05  1.327178e+06  \n",
      "M-95L                          9.660472e+02  9.666866e+02  9.639629e+02  \n",
      "M                              1.021983e+03  9.746349e+02  1.020487e+03  \n",
      "M-95R                          1.082549e+03  9.821129e+02  1.080823e+03  \n",
      "N_Par                          0.000000e+00  1.917000e+06  3.224010e+05  \n",
      "Train_Time                     1.704852e+01  3.574519e+01  1.493983e+03  \n",
      "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  5.394637e-03  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.161423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.092669e+06</td>\n",
       "      <td>1.085188e+06</td>\n",
       "      <td>9.386122e+05</td>\n",
       "      <td>1.089221e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.168691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.208410e+06</td>\n",
       "      <td>1.208125e+06</td>\n",
       "      <td>9.531860e+05</td>\n",
       "      <td>1.204600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.331617e+06</td>\n",
       "      <td>1.323333e+06</td>\n",
       "      <td>9.688458e+05</td>\n",
       "      <td>1.327178e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.396704</td>\n",
       "      <td>0.396386</td>\n",
       "      <td>9.694256e+02</td>\n",
       "      <td>9.660472e+02</td>\n",
       "      <td>9.666866e+02</td>\n",
       "      <td>9.639629e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.404971</td>\n",
       "      <td>0.404971</td>\n",
       "      <td>1.022149e+03</td>\n",
       "      <td>1.021983e+03</td>\n",
       "      <td>9.746349e+02</td>\n",
       "      <td>1.020487e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.413845</td>\n",
       "      <td>0.413161</td>\n",
       "      <td>1.078668e+03</td>\n",
       "      <td>1.082549e+03</td>\n",
       "      <td>9.821129e+02</td>\n",
       "      <td>1.080823e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>923500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.917000e+06</td>\n",
       "      <td>3.224010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>5564.811233</td>\n",
       "      <td>96.727152</td>\n",
       "      <td>1.619966e+09</td>\n",
       "      <td>1.704852e+01</td>\n",
       "      <td>3.574519e+01</td>\n",
       "      <td>1.493983e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.006567</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.935642e-06</td>\n",
       "      <td>1.655043e-03</td>\n",
       "      <td>9.877405e-04</td>\n",
       "      <td>5.394637e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                              0.161423   0.000000  1.092669e+06   \n",
       "W1                                  0.168691   0.000000  1.208410e+06   \n",
       "W1-95R                              0.176543   0.000000  1.331617e+06   \n",
       "M-95L                               0.396704   0.396386  9.694256e+02   \n",
       "M                                   0.404971   0.404971  1.022149e+03   \n",
       "M-95R                               0.413845   0.413161  1.078668e+03   \n",
       "N_Par                          923500.000000   0.000000  4.000000e+03   \n",
       "Train_Time                       5564.811233  96.727152  1.619966e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.006567   1.000000  8.935642e-06   \n",
       "\n",
       "                                     KRidge          GBRF           DNN  \n",
       "W1-95L                         1.085188e+06  9.386122e+05  1.089221e+06  \n",
       "W1                             1.208125e+06  9.531860e+05  1.204600e+06  \n",
       "W1-95R                         1.323333e+06  9.688458e+05  1.327178e+06  \n",
       "M-95L                          9.660472e+02  9.666866e+02  9.639629e+02  \n",
       "M                              1.021983e+03  9.746349e+02  1.020487e+03  \n",
       "M-95R                          1.082549e+03  9.821129e+02  1.080823e+03  \n",
       "N_Par                          0.000000e+00  1.917000e+06  3.224010e+05  \n",
       "Train_Time                     1.704852e+01  3.574519e+01  1.493983e+03  \n",
       "Test_Time/MC-Oracle_Test_Time  1.655043e-03  9.877405e-04  5.394637e-03  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) *Gaussian Benchmarks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bencharm 1: [Gaussian Process Regressor](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "- Benchmark 2: Deep Gaussian Networks:\n",
    "These models train models which assume Gaussianity.  We may view these as models in $\\mathcal{P}_2(\\mathbb{R})$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to (\\hat{\\mu}(x),\\hat{\\Sigma}(x)\\hat{\\Sigma}^{\\top})\\triangleq f(x) \\in \\mathbb{R}\\times [0,\\infty) \\to \n",
    "(2\\pi)^{-\\frac{d}{2}}\\det(\\hat{\\Sigma}(x))^{-\\frac{1}{2}} \\, e^{ -\\frac{1}{2}(\\cdot - \\hat{\\mu}(x))^{{{\\!\\mathsf{T}}}} \\hat{\\Sigma}(x)^{-1}(\\cdot - \\hat{\\mu}(x)) } \\mu \\in \\mathcal{G}_d\\subset \\mathcal{P}_2(\\mathbb{R});\n",
    "$$\n",
    "where $\\mathcal{G}_1$ is the set of Gaussian measures on $\\mathbb{R}$ equipped with the relative Wasserstein-1 topology.\n",
    "\n",
    "Examples of this type of architecture are especially prevalent in uncertainty quantification; see ([Deep Ensembles](https://arxiv.org/abs/1612.01474)] or [NOMU: Neural Optimization-based Model Uncertainty](https://arxiv.org/abs/2102.13640).  Moreover, their universality in $C(\\mathbb{R}^d,\\mathcal{G}_2)$ is known, and has been shown in [Corollary 4.7](https://arxiv.org/abs/2101.05390)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   4 out of  20 | elapsed:   45.8s remaining:  3.1min\n",
      "[Parallel(n_jobs=40)]: Done   7 out of  20 | elapsed:  2.1min remaining:  3.9min\n",
      "[Parallel(n_jobs=40)]: Done  10 out of  20 | elapsed:  5.1min remaining:  5.1min\n",
      "[Parallel(n_jobs=40)]: Done  13 out of  20 | elapsed:  5.8min remaining:  3.1min\n",
      "[Parallel(n_jobs=40)]: Done  16 out of  20 | elapsed:  7.2min remaining:  1.8min\n",
      "[Parallel(n_jobs=40)]: Done  20 out of  20 | elapsed:  7.7min finished\n",
      "  0%|          | 2/2000 [00:00<02:07, 15.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering Parameters for Deep Gaussian Network to train on!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:46<00:00, 43.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Getting Parameters for Deep Gaussian Network!\n",
      "===============================\n",
      "Training Deep Gaussian Network!\n",
      "===============================\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   1 tasks      | elapsed:  2.3min\n"
     ]
    }
   ],
   "source": [
    "# %run Benchmarks_Model_Builder_Mean_Var.ipynb\n",
    "exec(open('Benchmarks_Model_Builder_Mean_Var.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction Quality (Updated): Test\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction Quality (Updated): Train\")\n",
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) The natural Universal Benchmark: [Bishop's Mixture Density Network](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)\n",
    "\n",
    "This implementation is as follows:\n",
    "- For every $x$ in the trainingdata-set we fit a GMM $\\hat{\\nu}_x$, using the [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), with the same number of centers as the deep neural model in $\\mathcal{NN}_{1_{\\mathbb{R}^d},\\mathcal{D}}^{\\sigma:\\star}$ which we are evaluating.  \n",
    "- A Mixture density network is then trained to predict the infered parameters; given any $x \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dim == 1:\n",
    "    # %run Mixture_Density_Network.ipynb\n",
    "    exec(open('Mixture_Density_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Outputs\n",
    "Now we piece together all the numerical experiments and report a nice summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Training-Set Result(s)\")\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Test-Set Result(s)\")\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Terminal Runner(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Terminal Running\n",
    "print(\"============================\")\n",
    "print(\"Training Predictive Quality:\")\n",
    "print(\"============================\")\n",
    "print(Summary_pred_Qual_models)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"===========================\")\n",
    "print(\"Testing Predictive Quality:\")\n",
    "print(\"===========================\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "print(\"================================\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Kernel_Used_in_GPR: \"+str(GPR_trash.kernel))\n",
    "print(\"🙃🙃 Have a wonderful day! 🙃🙃\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
