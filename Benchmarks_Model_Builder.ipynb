{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================#\n",
    "# Elastic Net Version #\n",
    "#=====================#\n",
    "# Block warnings that spam when performing coordinate descent (by default) in 1-d.\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Initialize Elastic Net Regularization Model\n",
    "Lin_reg = ElasticNetCV(cv=5, random_state=0, alphas = np.linspace(0,(10**2),(10**2)),\n",
    "                           l1_ratio=np.linspace(0,1,(10**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ffNN Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "    #----------------------------#\n",
    "    # Maximally Interacting Layer #\n",
    "    #-----------------------------#\n",
    "    # Initialize Inputs\n",
    "    input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "   \n",
    "    \n",
    "    #------------------#\n",
    "    #   Core Layers    #\n",
    "    #------------------#\n",
    "    core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "    # Activation\n",
    "    core_layers = tf.nn.swish(core_layers)\n",
    "    # Train additional Depth?\n",
    "    if depth>1:\n",
    "        # Add additional deep layer(s)\n",
    "        for depth_i in range(1,depth):\n",
    "            core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "            # Activation\n",
    "            core_layers = tf.nn.swish(core_layers)\n",
    "    \n",
    "    #------------------#\n",
    "    #  Readout Layers  #\n",
    "    #------------------# \n",
    "    # Affine (Readout) Layer (Dense Fully Connected)\n",
    "    output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "    # Define Input/Output Relationship (Arch.)\n",
    "    trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "    \n",
    "    \n",
    "    #----------------------------------#\n",
    "    # Define Optimizer & Compile Archs.\n",
    "    #----------------------------------#\n",
    "    opt = Adam(lr=learning_rate)\n",
    "    trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "    return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "    # Update Dictionary\n",
    "    param_grid_in_internal = param_grid_in\n",
    "    param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "    \n",
    "    # Deep Feature Network\n",
    "    ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                            verbose=True)\n",
    "    \n",
    "    # Randomized CV\n",
    "    ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                    n_jobs=n_jobs,\n",
    "                                    cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                    param_distributions=param_grid_in_internal,\n",
    "                                    n_iter=n_iter,\n",
    "                                    return_train_score=True,\n",
    "                                    random_state=2020,\n",
    "                                    verbose=10)\n",
    "    \n",
    "    # Fit Model #\n",
    "    #-----------#\n",
    "    ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "    # Write Predictions #\n",
    "    #-------------------#\n",
    "    y_hat_train = ffNN_CVer.predict(X_train)\n",
    "    y_hat_test = ffNN_CVer.predict(X_test)\n",
    "    \n",
    "    # Counter number of parameters #\n",
    "    #------------------------------#\n",
    "    # Extract Best Model\n",
    "    best_model = ffNN_CVer.best_estimator_\n",
    "    # Count Number of Parameters\n",
    "    N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "    \n",
    "    \n",
    "    # Return Values #\n",
    "    #---------------#\n",
    "    return y_hat_train, y_hat_test\n",
    "\n",
    "# Update User\n",
    "#-------------#\n",
    "print('Deep Feature Builder - Ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Boosted Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GBRF(X_train,X_test,y_train):\n",
    "\n",
    "    # Run Random Forest Util\n",
    "    rand_forest_model_grad_boosted = GradientBoostingRegressor()\n",
    "\n",
    "    # Grid-Search CV\n",
    "    Random_Forest_GridSearch = RandomizedSearchCV(estimator = rand_forest_model_grad_boosted,\n",
    "                                                  n_iter=n_iter_trees,\n",
    "                                                  cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                                  param_distributions=Rand_Forest_Grid,\n",
    "                                                  return_train_score=True,\n",
    "                                                  random_state=2020,\n",
    "                                                  verbose=10,\n",
    "                                                  n_jobs=n_jobs)\n",
    "\n",
    "    random_forest_trained = Random_Forest_GridSearch.fit(X_train,y_train)\n",
    "    random_forest_trained = random_forest_trained.best_estimator_\n",
    "\n",
    "    #--------------------------------------------------#\n",
    "    # Write: Model, Results, and Best Hyper-Parameters #\n",
    "    #--------------------------------------------------#\n",
    "\n",
    "    # Save Model\n",
    "    # pickle.dump(random_forest_trained, open('./outputs/models/Gradient_Boosted_Tree/Gradient_Boosted_Tree_Best.pkl','wb'))\n",
    "\n",
    "    # Save Readings\n",
    "    cur_path = os.path.expanduser('./outputs/tables/best_params_Gradient_Boosted_Tree.txt')\n",
    "    with open(cur_path, \"w\") as f:\n",
    "        f.write(str(Random_Forest_GridSearch.best_params_))\n",
    "\n",
    "    best_params_table_tree = pd.DataFrame({'N Estimators': [Random_Forest_GridSearch.best_params_['n_estimators']],\n",
    "                                        'Min Samples Leaf': [Random_Forest_GridSearch.best_params_['min_samples_leaf']],\n",
    "                                        'Learning Rate': [Random_Forest_GridSearch.best_params_['learning_rate']],\n",
    "                                        'Max Depth': [Random_Forest_GridSearch.best_params_['max_depth']],\n",
    "                                        })\n",
    "    \n",
    "    # Count Number of Parameters in Random Forest Regressor\n",
    "    N_tot_params_per_tree = [ (x[0].tree_.node_count)*random_forest_trained.n_features_ for x in random_forest_trained.estimators_]\n",
    "    N_tot_params_in_forest = sum(N_tot_params_per_tree)\n",
    "    best_params_table_tree['N_parameters'] = [N_tot_params_in_forest]\n",
    "    # Write Best Parameter(s)\n",
    "    best_params_table_tree.to_latex('./outputs/tables/Best_params_table_Gradient_Boosted_Tree.txt')\n",
    "    #---------------------------------------------#\n",
    "    \n",
    "    # Generate Prediction(s) #\n",
    "    #------------------------#\n",
    "    y_train_hat_random_forest_Gradient_boosting = random_forest_trained.predict(X_train)\n",
    "    y_test_hat_random_forest_Gradient_boosting = random_forest_trained.predict(X_test)\n",
    "    \n",
    "    # Return Predictions #\n",
    "    #--------------------#\n",
    "    return y_train_hat_random_forest_Gradient_boosting, y_test_hat_random_forest_Gradient_boosting, random_forest_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Kernel_Ridge_Regressor(data_x_in,data_x_test_in,data_y_in):\n",
    "    # Imports\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "    # Initialize Randomized Gridsearch\n",
    "    kernel_ridge_CVer = RandomizedSearchCV(estimator = KernelRidge(),\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           cv=KFold(CV_folds, random_state=2020, shuffle=True),\n",
    "                                           param_distributions=param_grid_kernel_Ridge,\n",
    "                                           n_iter=n_iter,\n",
    "                                           return_train_score=True,\n",
    "                                           random_state=2020,\n",
    "                                           verbose=10)\n",
    "    kernel_ridge_CVer.fit(data_x_in,data_y_in)\n",
    "\n",
    "    # Get best Kernel ridge regressor\n",
    "    best_kernel_ridge_model = kernel_ridge_CVer.best_estimator_\n",
    "\n",
    "    # Get Predictions\n",
    "    f_hat_kernel_ridge_train = best_kernel_ridge_model.predict(data_x_in)\n",
    "    f_hat_kernel_ridge_test = best_kernel_ridge_model.predict(data_x_test_in)\n",
    "\n",
    "\n",
    "    Path('./outputs/models/Kernel_Ridge/').mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame.from_dict(kernel_ridge_CVer.best_params_,orient='index').to_latex(\"./outputs/models/Kernel_Ridge/Best_Parameters.tex\")\n",
    "    \n",
    "    # Return\n",
    "    return f_hat_kernel_ridge_train, f_hat_kernel_ridge_test, best_kernel_ridge_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
