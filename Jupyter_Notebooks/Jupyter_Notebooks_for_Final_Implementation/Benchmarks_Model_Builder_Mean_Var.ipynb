{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Model(s)\n",
    "\n",
    "**Note:** *NB, this means that this script *must* be run after the point-mass benchmarks script!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run Debug_Menu.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GPR(X_train_in,X_test_in,y_means_in):\n",
    "    # Initialize Cross-Vlidator of GPR\n",
    "    CV_GPR = RandomizedSearchCV(estimator=GaussianProcessRegressor(),\n",
    "                                n_jobs=n_jobs,\n",
    "                                cv=KFold(2, random_state=2020, shuffle=True),\n",
    "                                param_distributions=param_grid_GAUSSIAN,\n",
    "                                n_iter=n_iter,\n",
    "                                return_train_score=True,\n",
    "                                random_state=2021,\n",
    "                                verbose=10)\n",
    "\n",
    "    CV_GPR.fit(X_train,Y_train_mean_emp)\n",
    "    # Get Best Model\n",
    "    best_GPR = CV_GPR.best_estimator_\n",
    "\n",
    "    # Get Training-Set Prediction\n",
    "    GPR_means = best_GPR.predict(X_train,return_std=True)[0]\n",
    "    GPR_vars = (best_GPR.predict(X_train,return_std=True)[1])**2\n",
    "\n",
    "    # Get Test-Set Predictions\n",
    "    GPR_test_time_prediction = time.time()\n",
    "    GPR_means_test = best_GPR.predict(X_test,return_std=True)[0]\n",
    "    GPR_vars_test = (best_GPR.predict(X_test,return_std=True)[1])**2\n",
    "    GPR_test_time_prediction = time.time() - GPR_test_time_prediction\n",
    "    \n",
    "    # Return Trained Predictions + Model\n",
    "    GPR_means_test = best_GPR.predict(X_test,return_std=True)[0]\n",
    "    return GPR_means,GPR_vars, GPR_means_test, GPR_vars_test, best_GPR, GPR_test_time_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Gaussian DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps $\\varrho:\\mathbb{R}^d\\ni \\to (\\hat{\\mu},\\sigma)\\in \\mathbb{R}\\times (0,\\infty)$.  \n",
    "\n",
    "Implictly:\n",
    "$\n",
    "\\rho:\\mathbb{R}^d\\ni \\to \\nu\\circ \\varrho(x)\\in \\mathcal{P}_2(\\mathbb{R})\n",
    ".\n",
    "$\n",
    "\n",
    "The universal approximation theorem for this architecture is given in [Corollary 7: Quantitative Rates and Fundamental Obstructions to Non-EuclideanUniversal Approximation with Deep Narrow Feed-Forward Networks](https://arxiv.org/pdf/2101.05390.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dim == 1:\n",
    "    # Affine Readout post-composed with UAP-preserving readout map to G_d\n",
    "    class Gaussian_Splitter(tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, units=16, input_dim=32):\n",
    "            super(Gaussian_Splitter, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                     shape=(input_shape[-1], self.units),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True)\n",
    "            self.b = self.add_weight(name='bias_ffNN',\n",
    "                                     shape=(self.units,),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            parameters = tf.matmul(inputs, self.w) + self.b\n",
    "            mean_and_cov = tf.concat([parameters,tf.math.exp(parameters)],-1)\n",
    "            return mean_and_cov\n",
    "else:\n",
    "    # Affine Readout post-composed with UAP-preserving readout map to G_d\n",
    "    class Gaussian_Splitter(tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, units=16, input_dim=32):\n",
    "            super(Gaussian_Splitter, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                     shape=(input_shape[-1], self.units),\n",
    "                                     initializer='random_normal',\n",
    "                                     trainable=True)\n",
    "            self.b = self.add_weight(name='bias_ffNN',\n",
    "                                     shape=(self.units,),\n",
    "                                     initializer='random_normal',\n",
    "                                     trainable=True)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            parameters = tf.matmul(inputs, self.w) + self.b\n",
    "            mean_and_cov = tf.concat([parameters,tf.math.exp(parameters)],-1)\n",
    "            return mean_and_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "if output_dim == 1:\n",
    "    def get_ffNN_Gaussian(height, depth, learning_rate, input_dim, output_dim):\n",
    "        #----------------------------#\n",
    "        # Maximally Interacting Layer #\n",
    "        #-----------------------------#\n",
    "        # Initialize Inputs\n",
    "        input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "\n",
    "        #------------------#\n",
    "        #   Core Layers    #\n",
    "        #------------------#\n",
    "        core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "        # Activation\n",
    "        core_layers = tf.nn.swish(core_layers)\n",
    "        # Train additional Depth?\n",
    "        if depth>1:\n",
    "            # Add additional deep layer(s)\n",
    "            for depth_i in range(1,depth):\n",
    "                core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "                # Activation\n",
    "                core_layers = tf.nn.swish(core_layers)\n",
    "\n",
    "        #------------------#\n",
    "        #  Readout Layers  #\n",
    "        #------------------# \n",
    "        # Gaussian Splitter Layer\n",
    "        output_layers = Gaussian_Splitter(output_dim)(core_layers)  \n",
    "        # Define Input/Output Relationship (Arch.)\n",
    "        trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "\n",
    "\n",
    "        #----------------------------------#\n",
    "        # Define Optimizer & Compile Archs.\n",
    "        #----------------------------------#\n",
    "        opt = Adam(lr=learning_rate)\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "        return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "    def build_ffNN_Gaussian(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "        # Update Dictionary\n",
    "        param_grid_in_internal = param_grid_in\n",
    "        param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "        # Deep Feature Network\n",
    "        ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN_Gaussian, \n",
    "                                                                verbose=True)\n",
    "\n",
    "        # Randomized CV\n",
    "        ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                        n_jobs=n_jobs,\n",
    "                                        cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                        param_distributions=param_grid_in_internal,\n",
    "                                        n_iter=n_iter,\n",
    "                                        return_train_score=True,\n",
    "                                        random_state=2020,\n",
    "                                        verbose=10)\n",
    "\n",
    "        # Fit Model #\n",
    "        #-----------#\n",
    "        ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "        # Write Predictions #\n",
    "        #-------------------#\n",
    "        y_hat_train = ffNN_CVer.predict(X_train)\n",
    "\n",
    "        eval_time_ffNN = time.time()\n",
    "        y_hat_test = ffNN_CVer.predict(X_test)\n",
    "        eval_time_ffNN = time.time() - eval_time_ffNN\n",
    "\n",
    "        # Counter number of parameters #\n",
    "        #------------------------------#\n",
    "        # Extract Best Model\n",
    "        best_model = ffNN_CVer.best_estimator_\n",
    "        # Count Number of Parameters\n",
    "        N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "\n",
    "\n",
    "        # Return Values #\n",
    "        #---------------#\n",
    "        return y_hat_train, y_hat_test, N_params_best_ffNN, eval_time_ffNN\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('Deep Feature Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dim > 1:\n",
    "    def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "        #----------------------------#\n",
    "        # Maximally Interacting Layer #\n",
    "        #-----------------------------#\n",
    "        # Initialize Inputs\n",
    "        input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "\n",
    "        #------------------#\n",
    "        #   Core Layers    #\n",
    "        #------------------#\n",
    "        core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "        # Activation\n",
    "        core_layers = tf.nn.swish(core_layers)\n",
    "        # Train additional Depth?\n",
    "        if depth>1:\n",
    "            # Add additional deep layer(s)\n",
    "            for depth_i in range(1,depth):\n",
    "                core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "                # Activation\n",
    "                core_layers = tf.nn.swish(core_layers)\n",
    "\n",
    "        #------------------#\n",
    "        #  Readout Layers  #\n",
    "        #------------------# \n",
    "        # Affine (Readout) Layer (Dense Fully Connected)\n",
    "        output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "        # Define Input/Output Relationship (Arch.)\n",
    "        trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "\n",
    "\n",
    "        #----------------------------------#\n",
    "        # Define Optimizer & Compile Archs.\n",
    "        #----------------------------------#\n",
    "        opt = Adam(lr=learning_rate)\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "        return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "    def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "        # Update Dictionary\n",
    "        param_grid_in_internal = param_grid_in\n",
    "        param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "        # Deep Feature Network\n",
    "        ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                                verbose=True)\n",
    "\n",
    "        # Randomized CV\n",
    "        ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                        n_jobs=n_jobs,\n",
    "                                        cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                        param_distributions=param_grid_in_internal,\n",
    "                                        n_iter=n_iter,\n",
    "                                        return_train_score=True,\n",
    "                                        random_state=2020,\n",
    "                                        verbose=10)\n",
    "\n",
    "        # Fit Model #\n",
    "        #-----------#\n",
    "        ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "        # Write Predictions #\n",
    "        #-------------------#\n",
    "        y_hat_train = ffNN_CVer.predict(X_train)\n",
    "\n",
    "        eval_time_ffNN = time.time()\n",
    "        y_hat_test = ffNN_CVer.predict(X_test)\n",
    "        eval_time_ffNN = time.time() - eval_time_ffNN\n",
    "\n",
    "        # Counter number of parameters #\n",
    "        #------------------------------#\n",
    "        # Extract Best Model\n",
    "        best_model = ffNN_CVer.best_estimator_\n",
    "        # Count Number of Parameters\n",
    "        N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "\n",
    "\n",
    "        # Return Values #\n",
    "        #---------------#\n",
    "        return y_hat_train, y_hat_test, N_params_best_ffNN, eval_time_ffNN\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('DNN Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.1286s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "GRP_time = time.time()\n",
    "GPR_means, GPR_vars, GPR_means_test, GPR_vars_test, GPR_trash, GPR_test_time_prediction = get_GPR(X_train,\n",
    "                                                                                                  X_test,\n",
    "                                                                                                  Y_train_mean_emp) \n",
    "GRP_time = time.time() - GRP_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Gaussian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer Parameters to train on *(for training-set)* for deep Gaussian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 3995.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering Parameters for Deep Gaussian Network to train on!\n",
      "Done Getting Parameters for Deep Gaussian Network!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializations #\n",
    "#-----------------#\n",
    "print(\"Infering Parameters for Deep Gaussian Network to train on!\")\n",
    "# Start timer:\n",
    "timeBuilding_Training_Set_DGN = time.time()\n",
    "# Set Gaussian Dimension\n",
    "dim_Gaussian_space = output_dim*(1+output_dim)\n",
    "\n",
    "\n",
    "# Get Optimized Parameters to train Deep Gaussian Network On\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    # Define Function Defining log-likelihood of Gaussian dist.\n",
    "    if problem_dim == 1:\n",
    "        ## Count Data-set (outputed-samples) size\n",
    "        n = Y_train.shape[1]\n",
    "        ## Dummy Initialized Parameters\n",
    "        initParams = [1, 1]\n",
    "        def gaussian_log_like(parameters_in):\n",
    "            mean = parameters_in[0]   \n",
    "            sigma = parameters_in[1]\n",
    "\n",
    "            # Calculate negative log likelihood\n",
    "            negative_log_likelihood = -np.sum(stats.norm.logpdf(Y_train[i,], loc=mean, scale=sigma))\n",
    "            return negative_log_likelihood\n",
    "        \n",
    "        # Search for MAE Gaussian Parameters\n",
    "        results_loop = ((minimize(gaussian_log_like, initParams, method='Nelder-Mead')).x).reshape(1,-1)\n",
    "    \n",
    "    else:\n",
    "        # Get Sample Means\n",
    "        mean_loop = np.mean(Y_train[i,],axis=0)\n",
    "        # Get (regularized Cholesky) Squareroot of Sample Covariance\n",
    "        cov_loop = np.tril(np.linalg.cholesky(np.cov(Y_train[i,].T)+(10**-6)*np.diag(np.ones(output_dim)))).reshape(-1,)\n",
    "        \n",
    "        # Coercion\n",
    "        results_loop = np.append(mean_loop,cov_loop).reshape(-1,dim_Gaussian_space)\n",
    "    # Update Targets #\n",
    "    #----------------#\n",
    "    if i == 0:\n",
    "        Y_train_var_emp = results_loop\n",
    "    else:\n",
    "        Y_train_var_emp = np.append(Y_train_var_emp,results_loop,axis=0)\n",
    "# Stop timer:\n",
    "time.Building_Training_Set_DGN = time.time() - timeBuilding_Training_Set_DGN\n",
    "print(\"Done Getting Parameters for Deep Gaussian Network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Network on Infered Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Training Deep Gaussian Network!\n",
      "===============================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6500 - mse: 0.7869 - mae: 0.6500 - mape: 447.3390\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6499 - mse: 0.7868 - mae: 0.6499 - mape: 446.8886\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6499 - mse: 0.7867 - mae: 0.6499 - mape: 446.3971\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6498 - mse: 0.7866 - mae: 0.6498 - mape: 446.0573\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6498 - mse: 0.7865 - mae: 0.6498 - mape: 445.3826\n",
      "3/3 [==============================] - 0s 818us/step\n",
      "2/2 [==============================] - 0s 805us/step\n",
      "====================================\n",
      "Training Deep Gaussian Network!: END\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "print(\"===============================\")\n",
    "print(\"Training Deep Gaussian Network!\")\n",
    "print(\"===============================\")\n",
    "# Train simple deep classifier\n",
    "timer_DGN = time.time()\n",
    "if output_dim == 1:\n",
    "    # Redefine (Dimension-related) Elements of Grid\n",
    "    param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "    param_grid_Deep_Classifier['output_dim'] = [output_dim]\n",
    "    Deep_Gaussian_train_parameters, Deep_Gaussian_test_parameters, N_params_deep_Gaussian, timer_output_Deep_Gaussian = build_ffNN_Gaussian(n_folds = CV_folds, \n",
    "                                                                                                                                            n_jobs = n_jobs, \n",
    "                                                                                                                                            n_iter = n_iter, \n",
    "                                                                                                                                            param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                                                            X_train = X_train, \n",
    "                                                                                                                                            y_train = Y_train_var_emp,\n",
    "                                                                                                                                            X_test = X_test)\n",
    "else:\n",
    "    # Redefine (Dimension-related) Elements of Grid\n",
    "    param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "    param_grid_Deep_Classifier['output_dim'] = [Y_train_var_emp.shape[1]]\n",
    "    Deep_Gaussian_train_parameters, Deep_Gaussian_test_parameters, N_params_deep_Gaussian, timer_output_Deep_Gaussian = build_ffNN(n_folds = CV_folds, \n",
    "                                                                                                                                   n_jobs = n_jobs, \n",
    "                                                                                                                                   n_iter = n_iter, \n",
    "                                                                                                                                   param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                                                   X_train = X_train, \n",
    "                                                                                                                                   y_train = Y_train_var_emp,\n",
    "                                                                                                                                   X_test = X_test)\n",
    "# Format as float\n",
    "Deep_Gaussian_train_parameters = np.array(Deep_Gaussian_train_parameters,dtype=float)\n",
    "Deep_Gaussian_test_parameters = np.array(Deep_Gaussian_test_parameters,dtype=float)\n",
    "timer_DGN = time.time() - timer_DGN\n",
    "print(\"====================================\")\n",
    "print(\"Training Deep Gaussian Network!: END\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Quality and Prediction Metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#---------------------------------------#\n",
      " Get Training Errors for: Gaussian Models\n",
      "#---------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 20/20 [00:00<00:00, 946.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_Bootstraps = N_Boostraps_BCA\n",
    "print(\"#---------------------------------------#\")\n",
    "print(\" Get Training Errors for: Gaussian Models\")\n",
    "print(\"#---------------------------------------#\")\n",
    "for i in tqdm(range((X_train.shape[0]))):        \n",
    "    if output_dim == 1:\n",
    "        # Get Samples\n",
    "        ## From: Deep Gaussian Network (DGN)\n",
    "        hat_mu_DGaussianNet = Deep_Gaussian_train_parameters[i][0]\n",
    "        hat_sd_DGaussianNet = np.sqrt(Deep_Gaussian_train_parameters[i][1])\n",
    "        sample_DGaussianNet = np.random.normal(hat_mu_DGaussianNet,\n",
    "                                               hat_sd_DGaussianNet,\n",
    "                                               N_Monte_Carlo_Samples)\n",
    "\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP = GPR_means[i]\n",
    "        hat_sd_GPR = np.sqrt(GPR_vars[i])\n",
    "        sample_GRP = np.random.normal(hat_mu_GRP,\n",
    "                                      hat_sd_GPR,\n",
    "                                      N_Monte_Carlo_Samples)\n",
    "\n",
    "        # Compute Error(s)\n",
    "        ## W1\n",
    "        ### DGN\n",
    "        W1_loop_DGN = ot.emd2_1d(sample_DGaussianNet,\n",
    "                                 np.array(Y_train[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ### GPR\n",
    "        W1_loop_GPR = ot.emd2_1d(sample_GRP,\n",
    "                                 np.array(Y_train[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ## M1\n",
    "        Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "        if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "            Mu = direct_facts[i,]\n",
    "        else:\n",
    "            Mu = Mu_MC\n",
    "        ### Error(s)\n",
    "        Mean_loop_DGN = np.sum(np.abs(hat_mu_DGaussianNet-Mu))\n",
    "        Mean_loop_GPR = np.sum(np.abs(hat_mu_GRP-Mu_MC))\n",
    "    else:\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP = GPR_means[i,]\n",
    "        hat_sd_GPR = np.sqrt(GPR_vars[i])\n",
    "        sample_GRP = np.random.multivariate_normal(hat_mu_GRP,\n",
    "                                                   hat_sd_GPR*np.diag(np.ones(output_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate Gaussian Process Regressor's Prediction\n",
    "        sample_GRP = np.random.multivariate_normal(GPR_means[i,],\n",
    "                                                   np.diag(np.repeat(GPR_vars[i],problem_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate deep Gaussian Network's prediction\n",
    "        # Extract Prediction(s)\n",
    "        ## Get Mean\n",
    "        mean_loop = Deep_Gaussian_train_parameters[0,:problem_dim]\n",
    "        ## Get Covariance for Predicted Cholesky Root\n",
    "        cov_sqrt_chol_loop = Deep_Gaussian_train_parameters[0,problem_dim:]\n",
    "        cov_sqrt_chol_loop = cov_sqrt_chol_loop.reshape(output_dim,output_dim)\n",
    "        cov_sqrt_chol_loop = (np.matmul(cov_sqrt_chol_loop,cov_sqrt_chol_loop.T))\n",
    "        ## Get Empirical Samples\n",
    "        sample_DGaussianNet = np.random.multivariate_normal(mean_loop,\n",
    "                                                            cov_sqrt_chol_loop,\n",
    "                                                            N_Monte_Carlo_Samples)\n",
    "        \n",
    "        ## W1\n",
    "        W1_loop_GPR = ot.sliced.sliced_wasserstein_distance(X_s = sample_GRP, \n",
    "                                                            X_t = Y_train[i,],\n",
    "                                                            seed = 2020)\n",
    "        W1_loop_DGN = ot.sliced.sliced_wasserstein_distance(X_s = sample_DGaussianNet, \n",
    "                                                            X_t = Y_train[i,],\n",
    "                                                            seed = 2020)\n",
    "        \n",
    "        ## M1\n",
    "        Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "        Mu = Mu_MC\n",
    "        Mean_loop_GPR = np.sum(np.abs(hat_mu_GRP-Mu_MC))\n",
    "        Mean_loop_DGN = np.sum(np.abs(cov_sqrt_chol_loop-Mu_MC))\n",
    "   \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_Errors_GPR = W1_loop_GPR\n",
    "        W1_Errors_DGN = W1_loop_DGN\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR =  Mean_loop_GPR\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN =  Mean_loop_DGN\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_Errors_GPR = np.append(W1_Errors_GPR,W1_loop_GPR)\n",
    "        W1_Errors_DGN = np.append(W1_Errors_DGN,W1_loop_DGN)\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR =  np.append(Mean_Errors_GPR,Mean_loop_GPR)\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN =  np.append(Mean_Errors_DGN,Mean_loop_DGN)\n",
    "        \n",
    "    \n",
    "# Compute Error Metrics with Bootstrapped Confidence Intervals\n",
    "W1_Errors_GPR = np.array(bootstrap(np.abs(W1_Errors_GPR),n=N_Bootstraps)(.95))\n",
    "W1_Errors_DGN = np.array(bootstrap(np.abs(W1_Errors_DGN),n=N_Bootstraps)(.95))\n",
    "M1_Errors_GPR = np.array(bootstrap(np.abs(Mean_Errors_GPR),n=N_Bootstraps)(.95))\n",
    "M1_Errors_DGN = np.array(bootstrap(np.abs(Mean_Errors_DGN),n=N_Bootstraps)(.95))\n",
    "\n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 868.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------#\n",
      " Get Testing Errors for: Gaussian Models\n",
      "#--------------------------------------#\n",
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "N_Bootstraps = N_Boostraps_BCA\n",
    "print(\"#--------------------------------------#\")\n",
    "print(\" Get Testing Errors for: Gaussian Models\")\n",
    "print(\"#--------------------------------------#\")\n",
    "for i in tqdm(range((X_test.shape[0]))):        \n",
    "    if output_dim == 1:\n",
    "        # Get Samples\n",
    "        ## From: Deep Gaussian Network (DGN)\n",
    "        hat_mu_DGaussianNet_test = Deep_Gaussian_test_parameters[i,][0]\n",
    "        hat_sd_DGaussianNet_test = np.sqrt(Deep_Gaussian_test_parameters[i,][1])\n",
    "        sample_DGaussianNet_test = np.random.normal(hat_mu_DGaussianNet_test,\n",
    "                                               hat_sd_DGaussianNet_test,\n",
    "                                               N_Monte_Carlo_Samples)\n",
    "\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP_test = GPR_means_test[i]\n",
    "        hat_sd_GPR_test = np.sqrt(GPR_vars_test[i])\n",
    "        sample_GRP_test = np.random.normal(hat_mu_GRP_test,\n",
    "                                      hat_sd_GPR_test,\n",
    "                                      N_Monte_Carlo_Samples)\n",
    "\n",
    "        # Compute Error(s)\n",
    "        ## W1\n",
    "        ### DGN\n",
    "        W1_loop_DGN_test = ot.emd2_1d(sample_DGaussianNet_test,\n",
    "                                 np.array(Y_test[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ### GPR\n",
    "        W1_loop_GPR_test = ot.emd2_1d(sample_GRP_test,\n",
    "                                 np.array(Y_test[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ## M1\n",
    "        Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "        if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "            Mu_test = direct_facts_test[i,]\n",
    "        else:\n",
    "            Mu_test = Mu_MC_test\n",
    "        ### Error(s)\n",
    "        Mean_loop_DGN_test = np.sum(np.abs(hat_mu_DGaussianNet_test-Mu_test))\n",
    "        Mean_loop_GPR_test = np.sum(np.abs(hat_mu_GRP_test-Mu_test))\n",
    "    else:\n",
    "        ## Get Multivariate Gaussian Process Regressor's Prediction\n",
    "        hat_mu_GRP_test = GPR_means_test[i,]\n",
    "        hat_sd_GPR_test = np.sqrt(GPR_vars_test[i])\n",
    "        sample_GRP_test = np.random.multivariate_normal(GPR_means_test[i,],\n",
    "                                                   np.diag(np.repeat(GPR_vars_test[i],problem_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate deep Gaussian Network's prediction\n",
    "        # Extract Prediction(s)\n",
    "        ## Get Mean\n",
    "        mean_loop = Deep_Gaussian_test_parameters[0,:problem_dim]\n",
    "        ## Get Covariance for Predicted Cholesky Root\n",
    "        cov_sqrt_chol_loop = Deep_Gaussian_test_parameters[0,problem_dim:]\n",
    "        cov_sqrt_chol_loop = cov_sqrt_chol_loop.reshape(output_dim,output_dim)\n",
    "        cov_sqrt_chol_loop = (np.matmul(cov_sqrt_chol_loop,cov_sqrt_chol_loop.T))\n",
    "        ## Get Empirical Samples\n",
    "        sample_DGaussianNet_test = np.random.multivariate_normal(mean_loop,\n",
    "                                                                 cov_sqrt_chol_loop,\n",
    "                                                                 N_Monte_Carlo_Samples)\n",
    "        \n",
    "        ## W1\n",
    "        W1_loop_GPR_test = ot.sliced.sliced_wasserstein_distance(X_s = sample_GRP_test, \n",
    "                                                                 X_t = Y_test[i,],\n",
    "                                                                 seed = 2020)\n",
    "        W1_loop_DGN_test = ot.sliced.sliced_wasserstein_distance(X_s = sample_DGaussianNet_test, \n",
    "                                                                 X_t = Y_test[i,],\n",
    "                                                                 seed = 2020)\n",
    "        \n",
    "        ## M1\n",
    "        Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "        Mu_test = Mu_MC_test\n",
    "        Mean_loop_GPR_test = np.sum(np.abs(hat_mu_GRP_test-Mu_MC_test))\n",
    "        Mean_loop_DGN_test = np.sum(np.abs(cov_sqrt_chol_loop-Mu_MC_test))\n",
    "   \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_Errors_GPR_test = W1_loop_GPR_test\n",
    "        W1_Errors_DGN_test = W1_loop_DGN_test\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR_test =  Mean_loop_GPR_test\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN_test =  Mean_loop_DGN_test\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_Errors_GPR_test = np.append(W1_Errors_GPR_test,\n",
    "                                       W1_loop_GPR_test)\n",
    "        W1_Errors_DGN_test = np.append(W1_Errors_DGN_test,\n",
    "                                       W1_loop_DGN_test)\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR_test =  np.append(Mean_Errors_GPR_test,Mean_loop_GPR_test)\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN_test =  np.append(Mean_Errors_DGN_test,Mean_loop_DGN_test)\n",
    "        \n",
    "    \n",
    "# Compute Error Metrics with Bootstrapped Confidence Intervals\n",
    "W1_Errors_GPR_test = np.array(bootstrap(np.abs(W1_Errors_GPR_test),n=N_Bootstraps)(.95))\n",
    "W1_Errors_DGN_test = np.array(bootstrap(np.abs(W1_Errors_DGN_test),n=N_Bootstraps)(.95))\n",
    "M1_Errors_GPR_test = np.array(bootstrap(np.abs(Mean_Errors_GPR_test),n=N_Bootstraps)(.95))\n",
    "M1_Errors_DGN_test = np.array(bootstrap(np.abs(Mean_Errors_DGN_test),n=N_Bootstraps)(.95))\n",
    "\n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Performance Metrics:\n",
    "NB, this means that this script *must* be run after the point-mass benchmarks script!\n",
    "\n",
    "## Update Prediction-Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Updating Performance Metrics Dataframe and Saved!\n",
      "-------------------------------------------------\n",
      "Training Results to date:\n",
      "                                      DNM  MC-Oracle       GPR        DGN\n",
      "W1-95L                           0.035683   0.000000  3.854650   1.193158\n",
      "W1                               0.070665   0.000000  4.414748   1.462055\n",
      "W1-95R                           0.112095   0.000000  4.961399   1.719889\n",
      "M-95L                            0.038380   0.032202  0.041873   0.041844\n",
      "M                                0.056162   0.056162  0.056537   0.069497\n",
      "M-95R                            0.079759   0.074166  0.075285   0.109987\n",
      "N_Par                          205.000000   0.000000  0.000000  51.000000\n",
      "Train_Time                       5.346497   3.844073  0.509723   1.554867\n",
      "Test_Time/MC-Oracle_Test_Time    0.043141   1.000000  0.000156   0.038196\n",
      "Test Results to date:\n",
      "                                      DNM  MC-Oracle       GPR        DGN\n",
      "W1-95L                           0.035683   0.000000  3.854650   1.193158\n",
      "W1                               0.070665   0.000000  4.414748   1.462055\n",
      "W1-95R                           0.112095   0.000000  4.961399   1.719889\n",
      "M-95L                            0.038380   0.032202  0.041873   0.041844\n",
      "M                                0.056162   0.056162  0.056537   0.069497\n",
      "M-95R                            0.079759   0.074166  0.075285   0.109987\n",
      "N_Par                          205.000000   0.000000  0.000000  51.000000\n",
      "Train_Time                       5.346497   3.844073  0.509723   1.554867\n",
      "Test_Time/MC-Oracle_Test_Time    0.043141   1.000000  0.000156   0.038196\n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------------------------------\")\n",
    "print(\"Updating Performance Metrics Dataframe and Saved!\")\n",
    "print(\"-------------------------------------------------\")\n",
    "# Append Gaussian Process Regressor Performance\n",
    "# Train\n",
    "Summary_pred_Qual_models_internal[\"GPR\"] = pd.Series(np.append(np.append(W1_Errors_GPR,\n",
    "                                                                M1_Errors_GPR),\n",
    "                                                         np.array([0,\n",
    "                                                                   GRP_time,\n",
    "                                                                   (GPR_test_time_prediction/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "## Test\n",
    "Summary_pred_Qual_models_test[\"GPR\"] = pd.Series(np.append(np.append(W1_Errors_GPR_test,\n",
    "                                                                M1_Errors_GPR_test),\n",
    "                                                         np.array([0,\n",
    "                                                                   GRP_time,\n",
    "                                                                   (GPR_test_time_prediction/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "# Append Deep Gaussian Network Performance\n",
    "Summary_pred_Qual_models_internal[\"DGN\"] = pd.Series(np.append(np.append(W1_Errors_DGN,\n",
    "                                                                M1_Errors_DGN),\n",
    "                                                      np.array([N_params_deep_Gaussian,\n",
    "                                                                timer_DGN,\n",
    "                                                                (timer_output_Deep_Gaussian/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "## Test\n",
    "Summary_pred_Qual_models_test[\"DGN\"] = pd.Series(np.append(np.append(W1_Errors_DGN_test,\n",
    "                                                                     M1_Errors_DGN_test),\n",
    "                                                           np.array([N_params_deep_Gaussian,\n",
    "                                                                     timer_DGN,\n",
    "                                                                     (timer_output_Deep_Gaussian/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "# Update Performance Metrics\n",
    "## Train\n",
    "## Get Worst-Case\n",
    "Summary_pred_Qual_models_train = Summary_pred_Qual_models_internal\n",
    "Summary_pred_Qual_models_internal = np.maximum(Summary_pred_Qual_models_internal,Summary_pred_Qual_models_test)\n",
    "## Write Performance Metrics\n",
    "Summary_pred_Qual_models_internal.to_latex((results_tables_path+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"))\n",
    "Summary_pred_Qual_models_train.to_latex((results_tables_path+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS_train.tex\"))\n",
    "Summary_pred_Qual_models_test.to_latex((results_tables_path+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS_test.tex\"))\n",
    "\n",
    "# Remove W1 estimates from x \\mapsto \\delta_{f(x)}\n",
    "Summary_pred_Qual_models = Summary_pred_Qual_models_internal.copy()\n",
    "Summary_pred_Qual_models.loc[['W1-95L','W1','W1-95R'],['ENET','KRidge','ENET','GBRF','DNN']] = \"-\"\n",
    "Summary_pred_Qual_models.to_latex((results_tables_path+\"Final_Results/Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"))\n",
    "\n",
    "# Update User\n",
    "print(Summary_pred_Qual_models)\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Updated Performance Metrics Dataframe and Saved!\")\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
