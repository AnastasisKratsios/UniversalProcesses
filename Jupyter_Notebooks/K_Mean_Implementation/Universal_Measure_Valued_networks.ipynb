{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Conditional Laws for Random-Fields - via:\n",
    "\n",
    "## Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model $\\mathcal{NN}_{1_{\\mathbb{R}^n},\\mathcal{D}}^{\\sigma:\\star}$.\n",
    "\n",
    "---\n",
    "\n",
    "By: [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---\n",
    "\n",
    "## What does this code do?\n",
    "1. Learn Heteroskedastic Non-Linear Regression Problem\n",
    "     - $Y\\sim f_{\\text{unkown}}(x) + \\epsilon$ where $f$ is an known function and $\\epsilon\\sim Laplace(0,\\|x\\|)$\n",
    "2. Learn Random Bayesian Network's Law:\n",
    "    - $Y = W_J Y^{J-1}, \\qquad Y^{j}\\triangleq \\sigma\\bullet A^{j}Y^{j-1} + b^{j}, \\qquad Y^0\\triangleq x$\n",
    "\n",
    "3. In the above example if $A_j = M_j\\odot \\tilde{A_j}$ where $\\tilde{A}_j$ is a deterministic matrix and $M_j$ is a \"mask\", that is, a random matrix with binary entries and $\\odot$ is the Hadamard product then we recover the dropout framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode:\n",
    "Software/Hardware Testing or Real-Deal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random DNN\n",
    "# f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# Random DNN internal noise\n",
    "# f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "Depth_Bayesian_DNN = 10\n",
    "width = 20\n",
    "\n",
    "# Random Dropout applied to trained DNN\n",
    "f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "Dropout_rate = 0.1\n",
    "\n",
    "# Rough SDE (time 1)\n",
    "f_unknown_mode = \"Rough_SDE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough SDE Meta-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDE with Rough Driver\n",
    "N_Euler_Steps = 10**2\n",
    "Hurst_Exponent = 0.001\n",
    "\n",
    "def alpha(t,x):\n",
    "    output_drift_update = t-x\n",
    "    return output_drift_update\n",
    "\n",
    "def beta(t,x):\n",
    "    output_vol_update = (t+0.001)*np.diag(np.cos(x))\n",
    "    return output_vol_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Algorithm:\n",
    "---\n",
    "- Random $\\delta$-bounded partition on input space,\n",
    "- Train deep classifier on infered classes.\n",
    "---\n",
    "---\n",
    "---\n",
    "## Notes - Why the procedure is so computationally efficient?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auxiliaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\"\n",
    "\n",
    "\n",
    "### Set Seed\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)\n",
    "- Ratio $\\frac{\\text{Testing Datasize}}{\\text{Training Datasize}}$.\n",
    "- Number of Training Points to Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = .2\n",
    "N_train_size = 10**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Monte_Carlo_Samples = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial radis of $\\delta$-bounded random partition of $\\mathcal{X}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.01\n",
    "Proportion_per_cluster = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Setting *N_Quantizers_to_parameterize* prevents any barycenters and sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate from: $Y=f(X,W)$ \n",
    "- Random DNN (internal noise): \n",
    "    - $f(X,W) = f_{\\text{unknown}}(X+U)$\n",
    "- Random DNN: \n",
    "    - $f(X,W) = f_{\\text{unknown}}(X)+W$\n",
    "    \n",
    "*Non-linear dependance on exhaugenous noise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedastic Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    # Define Simulator\n",
    "    def Simulator(x_in):\n",
    "        var = np.sqrt(np.sum(x_in**2))\n",
    "        # Pushforward\n",
    "        f_x = f_unknown(x_in)\n",
    "        # Apply Noise After\n",
    "        noise = np.random.laplace(0,var,N_Monte_Carlo_Samples)\n",
    "        f_x_noise = np.cos(f_x) + noise\n",
    "        return f_x_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Random_Weights\":\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,) \n",
    "        # Feature Map Layer\n",
    "        W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "    #     Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Bayesian_Dropout\":\n",
    "    # Initialize Drouput Parameters\n",
    "    N_Dropout = int(np.maximum(1,round(width*Dropout_rate)))\n",
    "    \n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            # Apply Random Dropout\n",
    "            random_mask_coordinates_i = np.random.choice(range(width),N_Dropout)\n",
    "            random_mask_coordinates_j = np.random.choice(range(width),N_Dropout)\n",
    "            W_internal[random_mask_coordinates_i,random_mask_coordinates_j] = 0\n",
    "            # Apply Dropped-out layer\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fSDEs\n",
    "Lean the conditional law of $I_{X_1 \\in Ball(0,1)}$ where $X_t$ solves an SDE with fBM driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    # Initialize Drouput Parameters\n",
    "    N_Dropout = int(np.maximum(1,round(width*Dropout_rate)))\n",
    "    \n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        # Get fBM path\n",
    "        for d in range(problem_dim):\n",
    "            fBM_gen_loop = (((FBM(n=N_Euler_Steps, hurst=Hurst_Exponent, length=1, method='daviesharte')).fbm())[1:]).reshape(-1,1)\n",
    "            if d == 0:\n",
    "                fBM_gen = fBM_gen_loop\n",
    "            else:\n",
    "                fBM_gen = np.append(fBM_gen,fBM_gen_loop,axis=-1)\n",
    "        # Perform Integral\n",
    "        for t in range(N_Euler_Steps):\n",
    "            drift_update = alpha(t/N_Euler_Steps,x_internal)/N_Euler_Steps\n",
    "            vol_update = beta(t/N_Euler_Steps,x_internal)\n",
    "            x_internal = x_internal + drift_update + np.matmul(vol_update,fBM_gen[t,])\n",
    "        # Sum at output\n",
    "        output_indicator = np.max(x_internal)\n",
    "        return output_indicator\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test_size = int(np.round(N_train_size*train_test_ratio,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try initial sampling-type implementation!  It worked nicely..i.e.: centers were given!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Training Set\n",
    "X_train = np.random.uniform(size=np.array([N_train_size,problem_dim]),low=.5,high=1.5)\n",
    "\n",
    "# Get Testing Set\n",
    "test_set_indices = np.random.choice(range(X_train.shape[0]),N_test_size)\n",
    "X_test = X_train[test_set_indices,]\n",
    "X_test = X_test + np.random.uniform(low=-(delta/np.sqrt(problem_dim)), \n",
    "                                    high = -(delta/np.sqrt(problem_dim)),\n",
    "                                    size = X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize k_means\n",
    "N_Quantizers_to_parameterize = int(np.maximum(2,round(Proportion_per_cluster*X_train.shape[0])))\n",
    "kmeans = KMeans(n_clusters=N_Quantizers_to_parameterize, random_state=0).fit(X_train)\n",
    "# Get Classes\n",
    "Train_classes = np.array(pd.get_dummies(kmeans.labels_))\n",
    "# Get Center Measures\n",
    "Barycenters_Array_x = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Barycenters\n",
    "*Here we make the assumption that we can directly resample $f(X=x,U)$ if necessary...or that it is available as part of the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:03<00:00,  6.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(Barycenters_Array_x.shape[0])):\n",
    "    # Put Datum\n",
    "    Bar_x_loop = Barycenters_Array_x[i,]\n",
    "    # Product Monte-Carlo Sample for Input\n",
    "    Bar_y_loop = (Simulator(Bar_x_loop)).reshape(1,-1)\n",
    "\n",
    "    # Update Dataset\n",
    "    if i == 0:\n",
    "        Barycenters_Array = Bar_y_loop\n",
    "    else:\n",
    "        Barycenters_Array = np.append(Barycenters_Array,Bar_y_loop,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:54<00:00,  5.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    # Put Datum\n",
    "    x_loop = X_train[i,]\n",
    "    # Product Monte-Carlo Sample for Input\n",
    "    y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "    # Update Dataset\n",
    "    if i == 0:\n",
    "        Y_train = y_loop\n",
    "        Y_train_mean_emp = np.mean(y_loop)\n",
    "    else:\n",
    "        Y_train = np.append(Y_train,y_loop,axis=0)\n",
    "        Y_train_mean_emp = np.append(Y_train_mean_emp,np.mean(y_loop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:42<00:00,  5.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# Start Timer\n",
    "Test_Set_PredictionTime_MC = time.time()\n",
    "\n",
    "# Generate Data\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    # Put Datum\n",
    "    x_loop = X_test[i,]\n",
    "    # Product Monte-Carlo Sample for Input\n",
    "    y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "    # Update Dataset\n",
    "    if i == 0:\n",
    "        Y_test = y_loop\n",
    "    else:\n",
    "        Y_test = np.append(Y_test,y_loop,axis=0)\n",
    "        \n",
    "# End Timer\n",
    "Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "Type_A_timer_Begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train a deep (feed-forward) classifier:\n",
    "$$\n",
    "\\hat{f}\\triangleq \\operatorname{Softmax}_N\\circ W_J\\circ \\sigma \\bullet \\dots \\sigma \\bullet W_1,\n",
    "$$\n",
    "to identify which barycenter we are closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Packages and CV Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Re-Load Classifier Function(s)\n",
    "exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Training Classifer Portion of Type-A Model\n",
      "==========================================\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:   24.3s remaining:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:   28.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.9084 - accuracy: 0.0200\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8823 - accuracy: 0.0700\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8631 - accuracy: 0.0500\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8455 - accuracy: 0.0700\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.8265 - accuracy: 0.0800\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.8093 - accuracy: 0.1000\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7887 - accuracy: 0.1000\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7661 - accuracy: 0.0900\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.7382 - accuracy: 0.0900\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7090 - accuracy: 0.1000\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.6777 - accuracy: 0.1000\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.6433 - accuracy: 0.1000\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.6020 - accuracy: 0.1000\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5614 - accuracy: 0.1000\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.5227 - accuracy: 0.1400\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.4774 - accuracy: 0.1600\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.4324 - accuracy: 0.1100\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.3863 - accuracy: 0.1100\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.3403 - accuracy: 0.1300\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.2980 - accuracy: 0.1400\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2539 - accuracy: 0.1400\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.2098 - accuracy: 0.1800\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 3.1659 - accuracy: 0.2100\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.1229 - accuracy: 0.2000\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0848 - accuracy: 0.1900\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0495 - accuracy: 0.1700\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.0127 - accuracy: 0.1400\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9808 - accuracy: 0.1500\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.9487 - accuracy: 0.1400\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9141 - accuracy: 0.1400\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.8834 - accuracy: 0.1800\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.8496 - accuracy: 0.2000\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 2.8125 - accuracy: 0.2200\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.7845 - accuracy: 0.2500\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7510 - accuracy: 0.2500\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.7174 - accuracy: 0.2700\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6854 - accuracy: 0.2700\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6596 - accuracy: 0.3100\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.6353 - accuracy: 0.3300\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.6060 - accuracy: 0.3600\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5761 - accuracy: 0.3700\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5522 - accuracy: 0.3400\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.5147 - accuracy: 0.3300\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4957 - accuracy: 0.3100\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4783 - accuracy: 0.2800\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.4482 - accuracy: 0.2800\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.4157 - accuracy: 0.2500\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3798 - accuracy: 0.2700\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.3503 - accuracy: 0.2900\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.3250 - accuracy: 0.3400\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2991 - accuracy: 0.3400\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2918 - accuracy: 0.3400\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.2789 - accuracy: 0.3000\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2468 - accuracy: 0.3200\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.2024 - accuracy: 0.3400\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1673 - accuracy: 0.3700\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 2.1411 - accuracy: 0.4000\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.1217 - accuracy: 0.4100\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0968 - accuracy: 0.4100\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0765 - accuracy: 0.4300\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.0524 - accuracy: 0.3900\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.0244 - accuracy: 0.4200\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9943 - accuracy: 0.4500\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9686 - accuracy: 0.4700\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9486 - accuracy: 0.4500\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9418 - accuracy: 0.4100\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9174 - accuracy: 0.3900\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.8912 - accuracy: 0.4800\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8719 - accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.8453 - accuracy: 0.4700\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.8155 - accuracy: 0.4800\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7894 - accuracy: 0.5100\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.7696 - accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.7389 - accuracy: 0.5200\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7129 - accuracy: 0.5400\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7086 - accuracy: 0.5200\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.6967 - accuracy: 0.5500\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.6675 - accuracy: 0.5700\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.6528 - accuracy: 0.5500\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6434 - accuracy: 0.4800\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.6224 - accuracy: 0.5400\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5973 - accuracy: 0.5800\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5699 - accuracy: 0.5700\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 1.5645 - accuracy: 0.5600\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.5421 - accuracy: 0.5700\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.5352 - accuracy: 0.6000\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.5188 - accuracy: 0.5800\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.4939 - accuracy: 0.6400\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.4783 - accuracy: 0.6100\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.4707 - accuracy: 0.6500\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.4581 - accuracy: 0.6500\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.4676 - accuracy: 0.6100\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.4342 - accuracy: 0.5800\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3968 - accuracy: 0.6400\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3927 - accuracy: 0.6400\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3824 - accuracy: 0.5700\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.3635 - accuracy: 0.6400\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3594 - accuracy: 0.6700\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3627 - accuracy: 0.5800\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3515 - accuracy: 0.6400\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3368 - accuracy: 0.6300\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.3272 - accuracy: 0.6300\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3021 - accuracy: 0.6800\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.2871 - accuracy: 0.6400\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.2737 - accuracy: 0.6400\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2567 - accuracy: 0.6600\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.2378 - accuracy: 0.6700\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2172 - accuracy: 0.7000\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.2005 - accuracy: 0.7400\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.2108 - accuracy: 0.7000\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.2064 - accuracy: 0.7100\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1942 - accuracy: 0.7300\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1868 - accuracy: 0.7400\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1863 - accuracy: 0.6500\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1658 - accuracy: 0.6600\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1508 - accuracy: 0.6900\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1600 - accuracy: 0.6500\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 1.1447 - accuracy: 0.7200\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1298 - accuracy: 0.6800\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.1120 - accuracy: 0.6900\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1047 - accuracy: 0.7300\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1028 - accuracy: 0.7100\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0993 - accuracy: 0.6400\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0985 - accuracy: 0.6300\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0701 - accuracy: 0.6800\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0470 - accuracy: 0.7200\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0441 - accuracy: 0.7200\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0578 - accuracy: 0.7000\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0462 - accuracy: 0.7100\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0253 - accuracy: 0.7300\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.0182 - accuracy: 0.7100\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9930 - accuracy: 0.7700\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9774 - accuracy: 0.7700\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9738 - accuracy: 0.7600\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9853 - accuracy: 0.7100\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9752 - accuracy: 0.7300\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9621 - accuracy: 0.7900\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9454 - accuracy: 0.8200\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9357 - accuracy: 0.7600\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9334 - accuracy: 0.7300\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9177 - accuracy: 0.7400\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9142 - accuracy: 0.7700\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9036 - accuracy: 0.7800\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8838 - accuracy: 0.8000\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8979 - accuracy: 0.7800\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8970 - accuracy: 0.7900\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8946 - accuracy: 0.7900\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8833 - accuracy: 0.8200\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8773 - accuracy: 0.7900\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8677 - accuracy: 0.7700\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8672 - accuracy: 0.7900\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8449 - accuracy: 0.7700\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8434 - accuracy: 0.7700\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8524 - accuracy: 0.7800\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8482 - accuracy: 0.7900\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8400 - accuracy: 0.8000\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8414 - accuracy: 0.7700\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8169 - accuracy: 0.8200\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8251 - accuracy: 0.8100\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8206 - accuracy: 0.7700\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8002 - accuracy: 0.8000\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7905 - accuracy: 0.8300\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7942 - accuracy: 0.8300\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7818 - accuracy: 0.8000\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7958 - accuracy: 0.7800\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8007 - accuracy: 0.7500\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7773 - accuracy: 0.7600\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7570 - accuracy: 0.7800\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7637 - accuracy: 0.8000\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7818 - accuracy: 0.8500\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7767 - accuracy: 0.7900\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7716 - accuracy: 0.7900\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7404 - accuracy: 0.8600\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.8100\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7348 - accuracy: 0.8400\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7450 - accuracy: 0.8100\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7268 - accuracy: 0.8200\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7061 - accuracy: 0.8500\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7082 - accuracy: 0.7900\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7444 - accuracy: 0.7700\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7642 - accuracy: 0.8200\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7267 - accuracy: 0.7900\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6908 - accuracy: 0.8100\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.8100\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6681 - accuracy: 0.8400\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.8400\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.8600\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.8200\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.8000\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6627 - accuracy: 0.8500\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6580 - accuracy: 0.8600\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6602 - accuracy: 0.8700\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.8700\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6175 - accuracy: 0.8800\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6458 - accuracy: 0.8800\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6750 - accuracy: 0.8000\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6433 - accuracy: 0.8500\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6073 - accuracy: 0.8700\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.8100\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6554 - accuracy: 0.8600\n",
      "4/4 [==============================] - 0s 921us/step\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "===============================================\n",
      "Training Classifer Portion of Type Model: Done!\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==========================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "param_grid_Deep_Classifier['output_dim'] = [N_Quantizers_to_parameterize]\n",
    "\n",
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier, timer_output = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter = n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = Train_classes,\n",
    "                                                                                                        X_test = X_test)\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"Training Classifer Portion of Type Model: Done!\")\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predicted Quantized Distributions\n",
    "- Each *row* of \"Predicted_Weights\" is the $\\beta\\in \\Delta_N$.\n",
    "- Each *Column* of \"Barycenters_Array\" denotes the $x_1,\\dots,x_N$ making up the points of the corresponding empirical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Empirical Weights\n",
    "empirical_weights = (np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples).reshape(-1,)\n",
    "\n",
    "for i in range(N_Quantizers_to_parameterize):\n",
    "    if i == 0:\n",
    "        points_of_mass = Barycenters_Array[i,]\n",
    "    else:\n",
    "        points_of_mass = np.append(points_of_mass,Barycenters_Array[i,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Noisless Mean\n",
    "direct_facts = np.apply_along_axis(f_unknown, 1, X_train)\n",
    "direct_facts_test = np.apply_along_axis(f_unknown, 1, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Error(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize \"Higher Moments\" Loss Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Higher_Moments_Loss(Mu_hat_input,Mu_hat_MC_input):\n",
    "    for k in range(10):\n",
    "        moment_hat_loop = np.sum((Mu_hat_input**k)*points_of_mass)/np.math.factorial(k)\n",
    "        moment_MC_hat_loop = np.mean(Mu_hat_MC_input**k)/np.math.factorial(k)\n",
    "        if k == 0:\n",
    "            moment_hat = moment_hat_loop\n",
    "            moment_MC_hat = moment_MC_hat_loop\n",
    "        else:\n",
    "            moment_hat = np.append(moment_hat,moment_hat_loop)\n",
    "            moment_MC_hat = np.append(moment_MC_hat,moment_MC_hat_loop)\n",
    "\n",
    "    return moment_hat,moment_MC_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute *Training* Error(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:00<00:01, 48.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------#\n",
      " Get Training Error(s)\n",
      "#--------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 49.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#--------------------#\")\n",
    "print(\" Get Training Error(s)\")\n",
    "print(\"#--------------------#\")\n",
    "for i in tqdm(range((X_train.shape[0]))):\n",
    "    for j in range(N_Quantizers_to_parameterize):\n",
    "        b_loop = np.repeat(predicted_classes_train[i,j],N_Monte_Carlo_Samples)\n",
    "        if j == 0:\n",
    "            b = b_loop\n",
    "        else:\n",
    "            b = np.append(b,b_loop)\n",
    "        b = b.reshape(-1,1)\n",
    "        b = b\n",
    "    b = np.array(b,dtype=float).reshape(-1,)\n",
    "    b = b/N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Compute Error(s)\n",
    "    ## W1\n",
    "    W1_loop = ot.emd2_1d(points_of_mass,\n",
    "                         np.array(Y_train[i,]).reshape(-1,),\n",
    "                         b,\n",
    "                         empirical_weights)\n",
    "    \n",
    "    ## M1\n",
    "    Mu_hat = np.sum(b*(points_of_mass))\n",
    "    Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Mu = direct_facts[i,]\n",
    "    else:\n",
    "        Mu = Mu_MC\n",
    "        \n",
    "    ### Error(s)\n",
    "    Mean_loop = (Mu_hat-Mu)\n",
    "    Mean_loop_MC = (Mu_hat-Mu_MC)\n",
    "    \n",
    "    ## M2\n",
    "    Var_hat = np.sum(((points_of_mass-Mu_hat)**2)*b)\n",
    "    Var_MC = np.mean(np.array(Y_train[i]-Mu_MC)**2)\n",
    "    if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "        Var = np.mean((direct_facts[i,]-Mu)**2)\n",
    "    else:\n",
    "        Var = Var_MC        \n",
    "    \n",
    "    ### Error(s)\n",
    "    Var_loop = np.abs(Var_hat-Var)\n",
    "    Var_loop_MC = np.abs(Var_MC-Var)\n",
    "    \n",
    "    # Get Higher Moments Loss\n",
    "    Higher_momentserrors_loop,Higher_MC_momentserrors_loop = Higher_Moments_Loss(b,np.array(Y_train[i,]))\n",
    "    Higher_Moments_Errors_loop = np.abs(Higher_momentserrors_loop-Higher_MC_momentserrors_loop)\n",
    "    \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_errors = W1_loop\n",
    "        Mean_errors =  Mean_loop\n",
    "        Var_errors = Var_loop\n",
    "        Mean_errors_MC =  Mean_loop_MC\n",
    "        Var_errors_MC = Var_loop_MC\n",
    "        Higher_Moments_Errors = Higher_Moments_Errors_loop\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_errors = np.append(W1_errors,W1_loop)\n",
    "        Mean_errors =  np.append(Mean_errors,Mean_loop)\n",
    "        Var_errors = np.append(Var_errors,Var_loop)\n",
    "        Mean_errors_MC =  np.append(Mean_errors_MC,Mean_loop_MC)\n",
    "        Var_errors_MC = np.append(Var_errors_MC,Var_loop_MC)\n",
    "        Higher_Moments_Errors = np.append(Higher_Moments_Errors,Higher_Moments_Errors_loop)\n",
    "        \n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute *Testing* Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:00<00:00, 46.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#----------------#\n",
      " Get Test Error(s)\n",
      "#----------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 41.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"#----------------#\")\n",
    "print(\" Get Test Error(s)\")\n",
    "print(\"#----------------#\")\n",
    "for i in tqdm(range((X_test.shape[0]))):\n",
    "    for j in range(N_Quantizers_to_parameterize):\n",
    "        b_loop_test = np.repeat(predicted_classes_test[i,j],N_Monte_Carlo_Samples)\n",
    "        if j == 0:\n",
    "            b_test = b_loop_test\n",
    "        else:\n",
    "            b_test = np.append(b,b_loop)\n",
    "        b_test = b_test.reshape(-1,1)\n",
    "    b_test = np.array(b,dtype=float).reshape(-1,)\n",
    "    b_test = b/N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Compute Error(s)\n",
    "    ## W1\n",
    "    W1_loop_test = ot.emd2_1d(points_of_mass,\n",
    "                         np.array(Y_test[i,]).reshape(-1,),\n",
    "                         b,\n",
    "                         empirical_weights)\n",
    "    \n",
    "    ## M1\n",
    "    Mu_hat_test = np.sum(b_test*(points_of_mass))\n",
    "    Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "    if f_unknown_mode == \"Rough_SDE\":\n",
    "        Mu_test = Mu_MC_test\n",
    "    else:\n",
    "        Mu_test = direct_facts_test[i,]\n",
    "    Mu_test = direct_facts_test[i,]\n",
    "    ### Error(s)\n",
    "    Mean_loop_test = (Mu_hat_test-Mu_test)\n",
    "    Mean_loop_MC_test = (Mu_hat_test-Mu_MC_test)\n",
    "    \n",
    "    ## M2\n",
    "    Var_hat_test = np.sum(((points_of_mass-Mu_hat_test)**2)*b)\n",
    "    Var_MC_test = np.mean(np.array(Y_test[i]-Mu_MC)**2)\n",
    "    if f_unknown_mode == \"Rough_SDE\":\n",
    "        Var_test = Var_MC_test\n",
    "    else:\n",
    "        Var_test = np.mean((direct_facts_test[i,]-Mu)**2)\n",
    "    \n",
    "    ### Error(s)\n",
    "    Var_loop_test = np.abs(Var_hat_test-Var_test)\n",
    "    Var_loop_MC_test = np.abs(Var_MC_test-Var_test)\n",
    "    \n",
    "    # Get Higher Moments Loss\n",
    "    Higher_momentserrors_test_loop,Higher_MC_momentserrors_test_loop = Higher_Moments_Loss(b,np.array(Y_test[i,]))\n",
    "    Higher_Moments_Errors_test_loop = np.abs(Higher_momentserrors_test_loop-Higher_MC_momentserrors_test_loop)\n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_errors_test = W1_loop_test\n",
    "        Mean_errors_test =  Mean_loop_test\n",
    "        Var_errors_test = Var_loop_test\n",
    "        Mean_errors_MC_test =  Mean_loop_MC_test\n",
    "        Var_errors_MC_test = Var_loop_MC_test\n",
    "        Higher_Moments_test_Errors = Higher_Moments_Errors_test_loop\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_errors_test = np.append(W1_errors_test,W1_loop_test)\n",
    "        Mean_errors_test =  np.append(Mean_errors_test,Mean_loop_test)\n",
    "        Var_errors_test = np.append(Var_errors_test,Var_loop_test)\n",
    "        Mean_errors_MC_test =  np.append(Mean_errors_MC_test,Mean_loop_MC_test)\n",
    "        Var_errors_MC_test = np.append(Var_errors_MC_test,Var_loop_MC_test)\n",
    "        Higher_Moments_test_Errors = np.append(Higher_Moments_test_Errors,Higher_Moments_Errors_test_loop)\n",
    "        \n",
    "        \n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Type_A_timer_end = time.time()\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Time_Lapse_Model_A = Type_A_timer_end - Type_A_timer_Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Benchmarks\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# %run Benchmarks_Model_Builder.ipynb\n",
    "exec(open('Benchmarks_Model_Builder.py').read())\n",
    "exec(open('CV_Grid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements the Elastic-Net Regression- only predicting mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Elastic Net Model\n",
    "Lin_reg.fit(X_train,Y_train_mean_emp)\n",
    "\n",
    "# Get Predictions\n",
    "ENET_predict = Lin_reg.predict(X_train)\n",
    "ENET_predict_test = Lin_reg.predict(X_test)\n",
    "\n",
    "# Get Prediction Errors\n",
    "## Train\n",
    "for i in range(X_train.shape[0]):\n",
    "    Mu = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-ENET_predict[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_ENET = error_loop\n",
    "    else:\n",
    "        Mean_errors_ENET = np.append(Mean_errors_ENET,error_loop)\n",
    "## Test\n",
    "for i in range(X_test.shape[0]):\n",
    "    Mu_test = direct_facts[i,]\n",
    "    error_loop_test = np.abs(Mu-ENET_predict_test[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_ENET_test = error_loop_test\n",
    "    else:\n",
    "        Mean_errors_ENET_test = np.append(Mean_errors_ENET_test,error_loop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0428s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0180s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "Xhat_Kridge, Xhat_Kridge_test , relic = get_Kernel_Ridge_Regressor(X_train,X_test,Y_train_mean_emp)\n",
    "\n",
    "\n",
    "# Get Prediction Errors\n",
    "## Train\n",
    "for i in range(X_train.shape[0]):\n",
    "    Mu = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-Xhat_Kridge[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_KRidge = error_loop\n",
    "    else:\n",
    "        Mean_errors_KRidge = np.append(Mean_errors_KRidge,error_loop)\n",
    "## Test\n",
    "for i in range(X_test.shape[0]):\n",
    "    Mu_test = direct_facts[i,]\n",
    "    error_loop_test = np.abs(Mu-Xhat_Kridge_test[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_KRidge_test = error_loop_test\n",
    "    else:\n",
    "        Mean_errors_KRidge_test = np.append(Mean_errors_KRidge_test,error_loop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Boosted Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.7s remaining:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "GBRF_y_hat_train, GBRF_y_hat_test, GBRF_model = get_GBRF(X_train,X_test,Y_train_mean_emp)\n",
    "\n",
    "# Get Prediction Errors\n",
    "## Train\n",
    "for i in range(X_train.shape[0]):\n",
    "    Mu = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-GBRF_y_hat_train[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_GBRF = error_loop\n",
    "    else:\n",
    "        Mean_errors_GBRF = np.append(Mean_errors_GBRF,error_loop)\n",
    "## Test\n",
    "for i in range(X_test.shape[0]):\n",
    "    Mu_test = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-GBRF_y_hat_test[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_GBRF_test = error_loop_test\n",
    "    else:\n",
    "        Mean_errors_GBRF_test = np.append(Mean_errors_GBRF_test,error_loop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward (Vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:   18.2s remaining:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:   22.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.9098 - mse: 0.8294 - mae: 0.9098 - mape: 101.2258\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9081 - mse: 0.8264 - mae: 0.9081 - mape: 101.0422\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9065 - mse: 0.8234 - mae: 0.9065 - mape: 100.8591\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9048 - mse: 0.8204 - mae: 0.9048 - mape: 100.6763\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.9032 - mse: 0.8175 - mae: 0.9032 - mape: 100.4930\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.9015 - mse: 0.8145 - mae: 0.9015 - mape: 100.3103\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8999 - mse: 0.8115 - mae: 0.8999 - mape: 100.1266\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8982 - mse: 0.8086 - mae: 0.8982 - mape: 99.9426\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8966 - mse: 0.8056 - mae: 0.8966 - mape: 99.7582\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8949 - mse: 0.8026 - mae: 0.8949 - mape: 99.5735\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8933 - mse: 0.7996 - mae: 0.8933 - mape: 99.3892\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8916 - mse: 0.7967 - mae: 0.8916 - mape: 99.2031\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8899 - mse: 0.7937 - mae: 0.8899 - mape: 99.0174\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8883 - mse: 0.7907 - mae: 0.8883 - mape: 98.8309\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8866 - mse: 0.7877 - mae: 0.8866 - mape: 98.6432\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8849 - mse: 0.7847 - mae: 0.8849 - mape: 98.4547\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8832 - mse: 0.7817 - mae: 0.8832 - mape: 98.2659\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8815 - mse: 0.7787 - mae: 0.8815 - mape: 98.0770\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8798 - mse: 0.7757 - mae: 0.8798 - mape: 97.8871\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8781 - mse: 0.7727 - mae: 0.8781 - mape: 97.6959\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8763 - mse: 0.7697 - mae: 0.8763 - mape: 97.5043\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8746 - mse: 0.7667 - mae: 0.8746 - mape: 97.3122\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.8729 - mse: 0.7636 - mae: 0.8729 - mape: 97.1183\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8711 - mse: 0.7606 - mae: 0.8711 - mape: 96.9232\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8694 - mse: 0.7575 - mae: 0.8694 - mape: 96.7263\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8676 - mse: 0.7544 - mae: 0.8676 - mape: 96.5285\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8658 - mse: 0.7513 - mae: 0.8658 - mape: 96.3299\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8640 - mse: 0.7482 - mae: 0.8640 - mape: 96.1299\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8622 - mse: 0.7451 - mae: 0.8622 - mape: 95.9279\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8604 - mse: 0.7419 - mae: 0.8604 - mape: 95.7264\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8586 - mse: 0.7388 - mae: 0.8586 - mape: 95.5228\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8567 - mse: 0.7356 - mae: 0.8567 - mape: 95.3178\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8549 - mse: 0.7325 - mae: 0.8549 - mape: 95.1107\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8530 - mse: 0.7293 - mae: 0.8530 - mape: 94.9031\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8511 - mse: 0.7260 - mae: 0.8511 - mape: 94.6935\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8492 - mse: 0.7228 - mae: 0.8492 - mape: 94.4826\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8473 - mse: 0.7196 - mae: 0.8473 - mape: 94.2689\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8454 - mse: 0.7163 - mae: 0.8454 - mape: 94.0530\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8434 - mse: 0.7130 - mae: 0.8434 - mape: 93.8353\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8414 - mse: 0.7097 - mae: 0.8414 - mape: 93.6165\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8395 - mse: 0.7063 - mae: 0.8395 - mape: 93.3945\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8374 - mse: 0.7029 - mae: 0.8374 - mape: 93.1718\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8354 - mse: 0.6995 - mae: 0.8354 - mape: 92.9455\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8334 - mse: 0.6961 - mae: 0.8334 - mape: 92.7164\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8313 - mse: 0.6927 - mae: 0.8313 - mape: 92.4850\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8292 - mse: 0.6892 - mae: 0.8292 - mape: 92.2521\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8271 - mse: 0.6857 - mae: 0.8271 - mape: 92.0190\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8250 - mse: 0.6822 - mae: 0.8250 - mape: 91.7822\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8228 - mse: 0.6787 - mae: 0.8228 - mape: 91.5449\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.8207 - mse: 0.6751 - mae: 0.8207 - mape: 91.3043\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8185 - mse: 0.6716 - mae: 0.8185 - mape: 91.0615\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8163 - mse: 0.6679 - mae: 0.8163 - mape: 90.8162\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8141 - mse: 0.6643 - mae: 0.8141 - mape: 90.5697\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8118 - mse: 0.6607 - mae: 0.8118 - mape: 90.3196\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8096 - mse: 0.6570 - mae: 0.8096 - mape: 90.0673\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8073 - mse: 0.6533 - mae: 0.8073 - mape: 89.8120\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.8050 - mse: 0.6495 - mae: 0.8050 - mape: 89.5531\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8026 - mse: 0.6458 - mae: 0.8026 - mape: 89.2917\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.8002 - mse: 0.6420 - mae: 0.8002 - mape: 89.0278\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7979 - mse: 0.6381 - mae: 0.7979 - mape: 88.7615\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7954 - mse: 0.6343 - mae: 0.7954 - mape: 88.4936\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7930 - mse: 0.6304 - mae: 0.7930 - mape: 88.2219\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7906 - mse: 0.6266 - mae: 0.7906 - mape: 87.9494\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7881 - mse: 0.6226 - mae: 0.7881 - mape: 87.6719\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7856 - mse: 0.6187 - mae: 0.7856 - mape: 87.3918\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7830 - mse: 0.6147 - mae: 0.7830 - mape: 87.1093\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7804 - mse: 0.6106 - mae: 0.7804 - mape: 86.8221\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7778 - mse: 0.6066 - mae: 0.7778 - mape: 86.5312\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7752 - mse: 0.6025 - mae: 0.7752 - mape: 86.2400\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7726 - mse: 0.5984 - mae: 0.7726 - mape: 85.9456\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7699 - mse: 0.5943 - mae: 0.7699 - mape: 85.6457\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7672 - mse: 0.5901 - mae: 0.7672 - mape: 85.3426\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7644 - mse: 0.5858 - mae: 0.7644 - mape: 85.0362\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7616 - mse: 0.5816 - mae: 0.7616 - mape: 84.7262\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7588 - mse: 0.5773 - mae: 0.7588 - mape: 84.4120\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7559 - mse: 0.5730 - mae: 0.7559 - mape: 84.0946\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7531 - mse: 0.5686 - mae: 0.7531 - mape: 83.7749\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7502 - mse: 0.5643 - mae: 0.7502 - mape: 83.4499\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7472 - mse: 0.5598 - mae: 0.7472 - mape: 83.1216\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7442 - mse: 0.5554 - mae: 0.7442 - mape: 82.7903\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7412 - mse: 0.5509 - mae: 0.7412 - mape: 82.4552\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.7382 - mse: 0.5464 - mae: 0.7382 - mape: 82.1156\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7351 - mse: 0.5418 - mae: 0.7351 - mape: 81.7713\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7320 - mse: 0.5373 - mae: 0.7320 - mape: 81.4240\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7288 - mse: 0.5326 - mae: 0.7288 - mape: 81.0688\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7256 - mse: 0.5279 - mae: 0.7256 - mape: 80.7124\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7223 - mse: 0.5232 - mae: 0.7223 - mape: 80.3496\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7190 - mse: 0.5185 - mae: 0.7190 - mape: 79.9818\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7157 - mse: 0.5137 - mae: 0.7157 - mape: 79.6097\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7123 - mse: 0.5088 - mae: 0.7123 - mape: 79.2319\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7088 - mse: 0.5040 - mae: 0.7088 - mape: 78.8494\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7054 - mse: 0.4991 - mae: 0.7054 - mape: 78.4657\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7019 - mse: 0.4941 - mae: 0.7019 - mape: 78.0757\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6984 - mse: 0.4892 - mae: 0.6984 - mape: 77.6829\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6948 - mse: 0.4842 - mae: 0.6948 - mape: 77.2846\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6912 - mse: 0.4792 - mae: 0.6912 - mape: 76.8829\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6875 - mse: 0.4741 - mae: 0.6875 - mape: 76.4735\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6838 - mse: 0.4690 - mae: 0.6838 - mape: 76.0575\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6800 - mse: 0.4638 - mae: 0.6800 - mape: 75.6358\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6761 - mse: 0.4586 - mae: 0.6761 - mape: 75.2051\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6722 - mse: 0.4534 - mae: 0.6722 - mape: 74.7739\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6683 - mse: 0.4481 - mae: 0.6683 - mape: 74.3364\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6643 - mse: 0.4428 - mae: 0.6643 - mape: 73.8935\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6603 - mse: 0.4375 - mae: 0.6603 - mape: 73.4460\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6562 - mse: 0.4321 - mae: 0.6562 - mape: 72.9901\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6520 - mse: 0.4267 - mae: 0.6520 - mape: 72.5264\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6478 - mse: 0.4212 - mae: 0.6478 - mape: 72.0565\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6435 - mse: 0.4157 - mae: 0.6435 - mape: 71.5805\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6392 - mse: 0.4101 - mae: 0.6392 - mape: 71.0987\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6348 - mse: 0.4045 - mae: 0.6348 - mape: 70.6104\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6304 - mse: 0.3989 - mae: 0.6304 - mape: 70.1158\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6259 - mse: 0.3933 - mae: 0.6259 - mape: 69.6158\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6213 - mse: 0.3876 - mae: 0.6213 - mape: 69.1068\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6167 - mse: 0.3819 - mae: 0.6167 - mape: 68.5968\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6121 - mse: 0.3762 - mae: 0.6121 - mape: 68.0760\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6074 - mse: 0.3705 - mae: 0.6074 - mape: 67.5530\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6026 - mse: 0.3647 - mae: 0.6026 - mape: 67.0220\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5977 - mse: 0.3589 - mae: 0.5977 - mape: 66.4813\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5928 - mse: 0.3531 - mae: 0.5928 - mape: 65.9373\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5879 - mse: 0.3472 - mae: 0.5879 - mape: 65.3846\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5829 - mse: 0.3414 - mae: 0.5829 - mape: 64.8295\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5778 - mse: 0.3355 - mae: 0.5778 - mape: 64.2661\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5727 - mse: 0.3296 - mae: 0.5727 - mape: 63.6927\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5675 - mse: 0.3237 - mae: 0.5675 - mape: 63.1145\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5622 - mse: 0.3177 - mae: 0.5622 - mape: 62.5261\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5569 - mse: 0.3118 - mae: 0.5569 - mape: 61.9372\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5515 - mse: 0.3058 - mae: 0.5515 - mape: 61.3353\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5460 - mse: 0.2999 - mae: 0.5460 - mape: 60.7257\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5405 - mse: 0.2939 - mae: 0.5405 - mape: 60.1134\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5350 - mse: 0.2879 - mae: 0.5350 - mape: 59.4934\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5293 - mse: 0.2819 - mae: 0.5293 - mape: 58.8654\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5236 - mse: 0.2759 - mae: 0.5236 - mape: 58.2281\n",
      "Epoch 133/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5178 - mse: 0.2699 - mae: 0.5178 - mape: 57.5832\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5119 - mse: 0.2639 - mae: 0.5119 - mape: 56.9277\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5059 - mse: 0.2578 - mae: 0.5059 - mape: 56.2619\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4999 - mse: 0.2517 - mae: 0.4999 - mape: 55.5892\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4937 - mse: 0.2456 - mae: 0.4937 - mape: 54.9031\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4875 - mse: 0.2395 - mae: 0.4875 - mape: 54.2091\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4812 - mse: 0.2335 - mae: 0.4812 - mape: 53.5074\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4748 - mse: 0.2274 - mae: 0.4748 - mape: 52.7938\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4683 - mse: 0.2213 - mae: 0.4683 - mape: 52.0752\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4618 - mse: 0.2153 - mae: 0.4618 - mape: 51.3449\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4551 - mse: 0.2091 - mae: 0.4551 - mape: 50.6049\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4484 - mse: 0.2031 - mae: 0.4484 - mape: 49.8531\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4416 - mse: 0.1970 - mae: 0.4416 - mape: 49.0990\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4346 - mse: 0.1910 - mae: 0.4346 - mape: 48.3254\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4276 - mse: 0.1850 - mae: 0.4276 - mape: 47.5445\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4206 - mse: 0.1791 - mae: 0.4206 - mape: 46.7618\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4134 - mse: 0.1732 - mae: 0.4134 - mape: 45.9647\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4062 - mse: 0.1673 - mae: 0.4062 - mape: 45.1650\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3989 - mse: 0.1615 - mae: 0.3989 - mape: 44.3533\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3916 - mse: 0.1557 - mae: 0.3916 - mape: 43.5330\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3841 - mse: 0.1499 - mae: 0.3841 - mape: 42.6984\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3765 - mse: 0.1442 - mae: 0.3765 - mape: 41.8557\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3688 - mse: 0.1385 - mae: 0.3688 - mape: 40.9985\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3610 - mse: 0.1329 - mae: 0.3610 - mape: 40.1311\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3531 - mse: 0.1273 - mae: 0.3531 - mape: 39.2505\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3451 - mse: 0.1218 - mae: 0.3451 - mape: 38.3633\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3370 - mse: 0.1163 - mae: 0.3370 - mape: 37.4621\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3288 - mse: 0.1110 - mae: 0.3288 - mape: 36.5499\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3206 - mse: 0.1057 - mae: 0.3206 - mape: 35.6356\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3123 - mse: 0.1004 - mae: 0.3123 - mape: 34.7085\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3039 - mse: 0.0953 - mae: 0.3039 - mape: 33.7739\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2954 - mse: 0.0903 - mae: 0.2954 - mape: 32.8261\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2867 - mse: 0.0853 - mae: 0.2867 - mape: 31.8621\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2779 - mse: 0.0805 - mae: 0.2779 - mape: 30.8821\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2690 - mse: 0.0757 - mae: 0.2690 - mape: 29.8922\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2600 - mse: 0.0710 - mae: 0.2600 - mape: 28.8866\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2509 - mse: 0.0664 - mae: 0.2509 - mape: 27.8698\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2416 - mse: 0.0619 - mae: 0.2416 - mape: 26.8366\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2322 - mse: 0.0575 - mae: 0.2322 - mape: 25.7990\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2227 - mse: 0.0533 - mae: 0.2227 - mape: 24.7372\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2131 - mse: 0.0493 - mae: 0.2131 - mape: 23.6694\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2034 - mse: 0.0453 - mae: 0.2034 - mape: 22.5878\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1935 - mse: 0.0415 - mae: 0.1935 - mape: 21.4855\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1835 - mse: 0.0378 - mae: 0.1835 - mape: 20.3707\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1734 - mse: 0.0343 - mae: 0.1734 - mape: 19.2493\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1631 - mse: 0.0310 - mae: 0.1631 - mape: 18.1066\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1528 - mse: 0.0278 - mae: 0.1528 - mape: 16.9576\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1425 - mse: 0.0248 - mae: 0.1425 - mape: 15.8079\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1329 - mse: 0.0221 - mae: 0.1329 - mape: 14.7523\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1239 - mse: 0.0196 - mae: 0.1239 - mape: 13.7532\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1151 - mse: 0.0173 - mae: 0.1151 - mape: 12.7751\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1071 - mse: 0.0154 - mae: 0.1071 - mape: 11.8987\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0998 - mse: 0.0136 - mae: 0.0998 - mape: 11.0886\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.0936 - mse: 0.0122 - mae: 0.0936 - mape: 10.4138\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0883 - mse: 0.0110 - mae: 0.0883 - mape: 9.8347\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0829 - mse: 0.0098 - mae: 0.0829 - mape: 9.2382\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0789 - mse: 0.0090 - mae: 0.0789 - mape: 8.7986\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0751 - mse: 0.0082 - mae: 0.0751 - mape: 8.3898\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0724 - mse: 0.0076 - mae: 0.0724 - mape: 8.0927\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0700 - mse: 0.0072 - mae: 0.0700 - mape: 7.8343\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0683 - mse: 0.0069 - mae: 0.0683 - mape: 7.6425\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0671 - mse: 0.0067 - mae: 0.0671 - mape: 7.5151\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0661 - mse: 0.0066 - mae: 0.0661 - mape: 7.4084\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0657 - mse: 0.0065 - mae: 0.0657 - mape: 7.3672\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0652 - mse: 0.0065 - mae: 0.0652 - mape: 7.3150\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0649 - mse: 0.0064 - mae: 0.0649 - mape: 7.2770\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0647 - mse: 0.0064 - mae: 0.0647 - mape: 7.2540\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0645 - mse: 0.0064 - mae: 0.0645 - mape: 7.2340\n",
      "4/4 [==============================] - 0s 915us/step\n",
      "1/1 [==============================] - 0s 864us/step\n"
     ]
    }
   ],
   "source": [
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "param_grid_Deep_Classifier['output_dim'] = [1]\n",
    "\n",
    "YHat_ffNN, YHat_ffNN_test = build_ffNN(n_folds = CV_folds,\n",
    "                                              n_jobs = n_jobs, \n",
    "                                              n_iter = n_iter, \n",
    "                                              param_grid_in = param_grid_Deep_Classifier,  \n",
    "                                              X_train = X_train, \n",
    "                                              y_train = Y_train_mean_emp,\n",
    "                                              X_test = X_test)\n",
    "\n",
    "\n",
    "# Get Prediction Errors\n",
    "## Train\n",
    "for i in range(X_train.shape[0]):\n",
    "    Mu = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-YHat_ffNN[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_ffNN = error_loop\n",
    "    else:\n",
    "        Mean_errors_ffNN = np.append(Mean_errors_ffNN,error_loop)\n",
    "## Test\n",
    "for i in range(X_test.shape[0]):\n",
    "    Mu_test = direct_facts[i,]\n",
    "    error_loop = np.abs(Mu-YHat_ffNN_test[i])\n",
    "    if i == 0:\n",
    "        Mean_errors_ffNN_test = error_loop_test\n",
    "    else:\n",
    "        Mean_errors_ffNN_test = np.append(Mean_errors_ffNN_test,error_loop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Mean-Centric Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             M1     M1_MC   M1_ENET  M1_KRidge  M1_GBRFR   M1_ffNN\n",
      "Train  0.034827  0.034827  1.153699   1.154720  1.153645  1.156902\n",
      "Test   1.547666  0.896309  0.667851   0.668013  0.748990  0.748990\n"
     ]
    }
   ],
   "source": [
    "Summary_mean_models = pd.DataFrame({\"M1\":np.array([np.mean(np.abs(Mean_errors)),np.mean(np.abs(Mean_errors_test))]),\n",
    "                                    \"M1_MC\":np.array([np.mean(np.abs(Mean_errors_MC)),np.mean(np.abs(Mean_errors_MC_test))]),\n",
    "                                    \"M1_ENET\":np.array([np.mean(np.abs(Mean_errors_ENET)),np.mean(np.abs(Mean_errors_ENET_test))]),\n",
    "                                    \"M1_KRidge\":np.array([np.mean(np.abs(Mean_errors_KRidge)),np.mean(np.abs(Mean_errors_KRidge_test))]),\n",
    "                                    \"M1_GBRFR\":np.array([np.mean(np.abs(Mean_errors_GBRF)),np.mean(np.abs(Mean_errors_GBRF_test))]),\n",
    "                                    \"M1_ffNN\":np.array([np.mean(np.abs(Mean_errors_ffNN)),np.mean(np.abs(Mean_errors_ffNN_test))])\n",
    "                                   },index=[\"Train\",\"Test\"])\n",
    "\n",
    "print(Summary_mean_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Moment Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          N_Centers   N_Q  N_Params  Training Time  \\\n",
      "Model_Complexity_metrics         50  1000     50850     3.6607E+01   \n",
      "\n",
      "                          T_Test/T_Test-MC  Time Test  Time EM-MC  \n",
      "Model_Complexity_metrics        5.1567E-04 5.3067E-02  1.0291E+02  \n",
      "              W1         M1      M1_MC        Var     Var_MC       High  \\\n",
      "Train 4.8253E-02 3.4827E-02 3.4827E-02 5.8315E-02 0.0000E+00 4.4557E+03   \n",
      "Test  6.6903E-02 1.5477E+00 8.9631E-01 7.6364E-01 0.0000E+00 4.4557E+03   \n",
      "\n",
      "       N_Centers   N_Q  N_Params  Training Time  T_Test/T_Test-MC  \\\n",
      "Train         50  1000     50850     3.6607E+01        5.1567E-04   \n",
      "Test          50  1000     50850     3.6607E+01        5.1567E-04   \n",
      "\n",
      "       Problem_Dimension  \n",
      "Train                  2  \n",
      "Test                   2  \n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------#\n",
    "W1_95 = bootstrap(W1_errors, n=1000, func=np.mean)(.95)\n",
    "W1_99 = bootstrap(W1_errors, n=1000, func=np.mean)(.99)\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "Model_Complexity = pd.DataFrame({\"N_Centers\":N_Quantizers_to_parameterize,\n",
    "                                 \"N_Q\":N_Monte_Carlo_Samples,\n",
    "                                 \"N_Params\":N_params_deep_classifier,\n",
    "                                 \"Training Time\":Time_Lapse_Model_A,\n",
    "                                 \"T_Test/T_Test-MC\": (timer_output/Test_Set_PredictionTime_MC),\n",
    "                                 \"Time Test\": timer_output,\n",
    "                                 \"Time EM-MC\": Test_Set_PredictionTime_MC},index=[\"Model_Complexity_metrics\"])\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "Summary = pd.DataFrame({\"W1\":np.array([np.mean(np.abs(W1_errors)),np.mean(np.abs(W1_errors_test))]),\n",
    "                        \"M1\":np.array([np.mean(np.abs(Mean_errors)),np.mean(np.abs(Mean_errors_test))]),\n",
    "                        \"M1_MC\":np.array([np.mean(np.abs(Mean_errors_MC)),np.mean(np.abs(Mean_errors_MC_test))]),\n",
    "                        \"Var\":np.array([np.mean(np.abs(Var_errors)),np.mean(np.abs(Var_errors_test))]),\n",
    "                        \"Var_MC\":np.array([np.mean(np.abs(Var_errors_MC)),np.mean(np.abs(Var_errors_MC_test))]),\n",
    "                        \"High\": np.array([np.mean(np.abs(Higher_Moments_Errors)),np.mean(np.abs(Higher_Moments_test_Errors))]),\n",
    "                        \"N_Centers\":np.array((N_Quantizers_to_parameterize,N_Quantizers_to_parameterize)),\n",
    "                        \"N_Q\":np.array((N_Monte_Carlo_Samples,N_Monte_Carlo_Samples)),\n",
    "                        \"N_Params\":np.array((N_params_deep_classifier,N_params_deep_classifier)),\n",
    "                        \"Training Time\":np.array((Time_Lapse_Model_A,Time_Lapse_Model_A)),\n",
    "                        \"T_Test/T_Test-MC\":np.array(((timer_output/Test_Set_PredictionTime_MC),(timer_output/Test_Set_PredictionTime_MC))),\n",
    "                        \"Problem_Dimension\":np.array((problem_dim,problem_dim))\n",
    "                       },index=[\"Train\",\"Test\"])\n",
    "\n",
    "\n",
    "# Write Performance Metrics to file #\n",
    "#-----------------------------------#\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "Model_Complexity.to_latex((results_tables_path+\"Latent_Width_NSDE\"+str(width)+\"Problemdimension\"+str(problem_dim)+\"__ModelComplexities.tex\"))\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "Summary.to_latex((results_tables_path+\"Latent_Width_NSDE\"+str(width)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Update User\n",
    "print(Model_Complexity)\n",
    "print(Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>W1</th>\n",
       "      <th>M1</th>\n",
       "      <th>M1_MC</th>\n",
       "      <th>Var</th>\n",
       "      <th>Var_MC</th>\n",
       "      <th>High</th>\n",
       "      <th>N_Centers</th>\n",
       "      <th>N_Q</th>\n",
       "      <th>N_Params</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>T_Test/T_Test-MC</th>\n",
       "      <th>Problem_Dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>4.8253E-02</td>\n",
       "      <td>3.4827E-02</td>\n",
       "      <td>3.4827E-02</td>\n",
       "      <td>5.8315E-02</td>\n",
       "      <td>0.0000E+00</td>\n",
       "      <td>4.4557E+03</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>50850</td>\n",
       "      <td>3.6607E+01</td>\n",
       "      <td>5.1567E-04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>6.6903E-02</td>\n",
       "      <td>1.5477E+00</td>\n",
       "      <td>8.9631E-01</td>\n",
       "      <td>7.6364E-01</td>\n",
       "      <td>0.0000E+00</td>\n",
       "      <td>4.4557E+03</td>\n",
       "      <td>50</td>\n",
       "      <td>1000</td>\n",
       "      <td>50850</td>\n",
       "      <td>3.6607E+01</td>\n",
       "      <td>5.1567E-04</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              W1         M1      M1_MC        Var     Var_MC       High  \\\n",
       "Train 4.8253E-02 3.4827E-02 3.4827E-02 5.8315E-02 0.0000E+00 4.4557E+03   \n",
       "Test  6.6903E-02 1.5477E+00 8.9631E-01 7.6364E-01 0.0000E+00 4.4557E+03   \n",
       "\n",
       "       N_Centers   N_Q  N_Params  Training Time  T_Test/T_Test-MC  \\\n",
       "Train         50  1000     50850     3.6607E+01        5.1567E-04   \n",
       "Test          50  1000     50850     3.6607E+01        5.1567E-04   \n",
       "\n",
       "       Problem_Dimension  \n",
       "Train                  2  \n",
       "Test                   2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance wrt Mean Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M1_MC</th>\n",
       "      <th>M1_ENET</th>\n",
       "      <th>M1_KRidge</th>\n",
       "      <th>M1_GBRFR</th>\n",
       "      <th>M1_ffNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>3.4827E-02</td>\n",
       "      <td>3.4827E-02</td>\n",
       "      <td>1.1537E+00</td>\n",
       "      <td>1.1547E+00</td>\n",
       "      <td>1.1536E+00</td>\n",
       "      <td>1.1569E+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>1.5477E+00</td>\n",
       "      <td>8.9631E-01</td>\n",
       "      <td>6.6785E-01</td>\n",
       "      <td>6.6801E-01</td>\n",
       "      <td>7.4899E-01</td>\n",
       "      <td>7.4899E-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              M1      M1_MC    M1_ENET  M1_KRidge   M1_GBRFR    M1_ffNN\n",
       "Train 3.4827E-02 3.4827E-02 1.1537E+00 1.1547E+00 1.1536E+00 1.1569E+00\n",
       "Test  1.5477E+00 8.9631E-01 6.6785E-01 6.6801E-01 7.4899E-01 7.4899E-01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary_mean_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
