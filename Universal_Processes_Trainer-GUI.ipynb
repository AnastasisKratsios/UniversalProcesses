{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model (Type A)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Algorithm:\n",
    "---\n",
    "## 1) Generate Data:\n",
    "Generates the empirical measure $\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$ of $X_T$ conditional on $X_0=x_0\\in \\mathbb{R}$ *($x_0$ and $T>0$ are user-provided)* by simulating from:\n",
    "$$ \n",
    "X_T = x + \\int_0^T \\alpha(s,x)ds + \\int_0^T\\beta(s,x)dW_s.\n",
    "$$\n",
    "\n",
    "## 2) Get \"Sample Barycenters\":\n",
    "Let $\\{\\mu_n\\}_{n=1}^N\\subset\\mathcal{P}_1(\\mathbb{R}^d)$.  Then, the *sample barycenter* is defined by:\n",
    "1. $\\mathcal{M}^{(0)}\\triangleq \\left\\{\\hat{\\mu}_n\\right\\}_{n=1}^N$,\n",
    "2. For $1\\leq n\\leq \\mbox{N sample barycenters}$: \n",
    "    - $\n",
    "\\mu^{\\star}\\in \\underset{\\tilde{\\mu}\\in \\mathcal{M}^{(n)}}{\\operatorname{argmin}}\\, \\sum_{n=1}^N \\mathcal{W}_1\\left(\\mu^{\\star},\\mu_n\\right),\n",
    "$\n",
    "    - $\\mathcal{M}^{(n)}\\triangleq \\mathcal{M}^{(n-1)} - \\{\\mu^{\\star}\\},$\n",
    "*i.e., the closest generated measure form the random sample to all other elements of the random sample.*\n",
    "\n",
    "---\n",
    "**Note:** *We simplify the computational burden of getting the correct classes by putting this right into this next loop.*\n",
    "\n",
    "## 3) Train Deep Classifier:\n",
    "$\\hat{f}\\in \\operatorname{argmin}_{f \\in \\mathcal{NN}_{d:N}^{\\star}} \n",
    "\\sum_{x \\in \\mathbb{X}}\n",
    "\\, \n",
    "\\mathbb{H}\n",
    "\\left(\n",
    "    \\operatorname{Softmax}_N\\circ f(x)_n| I\\left\\{W_1(\\hat{\\mu}_n,\\mu_x),\\inf_{m\\leq N} W_1(\\hat{\\mu}_m,\\mu_x)\\right\\}\n",
    "\\right);\n",
    "$\n",
    "where $\\mathbb{H}$ is the categorical cross-entropy.  \n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "## Notes - Why the procedure is so computationally efficient?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many random polulations to visualize:\n",
    "Visualization_Size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "*This hyperparameter describes the proportion of the data used as sample-barycenters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantization_Proportion = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Euler_Maruyama_Steps = 100\n",
    "N_Monte_Carlo_Samples = 1000\n",
    "N_Monte_Carlo_Samples_Test = 1000 # How many MC-samples to draw from test-set?\n",
    "T_end = 1\n",
    "Direct_Sampling = False #This hyperparameter determines if we use a Euler-Maryama scheme or if we use something else.  \n",
    "\n",
    "## Grid\n",
    "N_Grid_Finess = 200\n",
    "Max_Grid = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Setting *N_Quantizers_to_parameterize* prevents any barycenters and sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDE Simulation Hyper-Parameter(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(t,x):\n",
    "    return np.sin(math.pi*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(t,x):\n",
    "    return 1+ (x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Run Backend:\n",
    "*Runs the backend with these pre-specified hyperparameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "• Grid Instances:  200 and : 150  Testing instances.\n",
      "• 200  Centers will be produced; from a total datasize of:  200 !  (That's  1  percent).\n",
      "• Each Wasserstein-1 Ball should contain:  1 elements from the training set.\n",
      "Current Monte-Carlo Step:\n",
      "Using Monte-Carlo Sampling directly from measure at time-T of X_T.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Universal_Processes_Trainer_BACKEND.ipynb:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  \"cells\": [\n",
      "Universal_Processes_Trainer_BACKEND.ipynb:2: RuntimeWarning: invalid value encountered in sin\n",
      "  \"cells\": [\n",
      "  0%|          | 1/200 [00:04<16:25,  4.95s/it]"
     ]
    }
   ],
   "source": [
    "# Use is user prefers GUI\n",
    "%run Universal_Processes_Trainer_BACKEND.ipynb\n",
    "# Use for faster/shorter execution times (note: more amenable to terminal)\n",
    "# exec(open('Universal_Processes_Trainer_BACKEND.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
