trial_run = True


# #### Load
# %run Loader.ipynb
exec(open('Helper_Scripts_and_Loading/Loader.py').read())
# Load Packages/Modules
exec(open('Helper_Scripts_and_Loading/Init_Dump.py').read())
import time as time #<- Note sure why...but its always seems to need 'its own special loading...'


# ---

# ---

# ---

# ## Problem Dimension

# In[3]:


problem_dim = 100


# ---
# ### More Meta-Parameters for "Vanilla" fractional SDE

# #### Noise Parameters:
# - Hust exponent: $B_t^{H\boldsymbol{\leftarrow}}$ determines the roughness of the driving fBM.
# - Uniform noise level is: *depricated*

# In[4]:


Hurst_Exponent = 0.5
uniform_noise_level = 0


# #### Discritization/Grid:
# - T_end: Is the place to stop the process
# - N_Euler_Maruyama_Steps: Is the number of time discritization steps taken to generate $X_{\text{T_end}}$ starting at $X_{\text{T_begin}}$.
# - Grid_Finess: Is the number of initial states $x\in \mathbb{R}^d$ to generate the process $X_t^x$ as starting from.
# - Max_Grid: Is the size of the compact where the initial states (the $x$s can be generated on).

# In[5]:


# End times for Time-Grid
T_end = 1
T_end_test = 1.1


# In[6]:


N_Euler_Maruyama_Steps = 10**3


# In[7]:


## Grid
N_Grid_Finess = 1
Max_Grid = 0.5
x_0 = 1


# #### Centers

# In[8]:


# Number of Centers (\hat{\mu}_s)
N_Quantizers_to_parameterize = 1
N_Clusters = 2

# Hyper-parameters of Cover
delta = 0.1
N_measures_per_center = 10**1


# #### Dynamics of fSDE:

# -**Vanilla Drift**: Defines the dynamics of the "drift"; i.e.:
# $
# \alpha(t,x) \in C([0,\infty)\times \mathbb{R}^d, \mathbb{R}^d).
# $

# In[9]:


W_1 = np.random.uniform(low=-.5,high=.5,size=[problem_dim,problem_dim])
W_2 = np.random.uniform(low=-.5,high=.5,size=[problem_dim,problem_dim])

def alpha(t,x):
    # SDE Drift
#     x_internal = np.matmul(W_1,x)
#     x_internal = np.sin(x_internal)
#     x_internal = np.matmul(W_2,x_internal)
    # Vanilla
    x_internal = .1*np.ones(problem_dim)#(.1-.5*(.01**2))*t + np.cos(x)
    # Return Output
    return x_internal


# -**Vanilla Volatility**: Defines the dynamics of the "drift"; i.e.:
# $
# \beta(t,x)\in C([0,\infty)\times \mathbb{R}^d, \operatorname{Mat}_{d\times d}(\mathbb{R})).
# $

# In[10]:


def beta(t,x):
    return 0.01*np.ones(problem_dim)


# ---

# ## Simulation Meta-Parameter(s):

# ---

# ### Simulation Method:

# - **Learn Noise**:
# This is not used in the paper; nevertheless it works nicely.  It is used to learn the noise in a regression task' instead of just learning the mean.

# In[11]:


f_unknown_mode = "Heteroskedastic_NonLinear_Regression"


# - **Random DNN**:This is used to learn predictions from a random DNN with internal noise.
# 
# *Here, the noise is generated by an extreme learning machine with Ridge-regression readout.*
# 
# There are two available datasets; the first is an repliation (returns regression) task using SnP500 data and the second is a cryptocurrency regression task.

# In[12]:


# Random DNN internal noise
## Real-world data version

# f_unknown_mode = "Extreme_Learning_Machine"
### General Parameters
# activation_function == 'thresholding'
activation_function = 'sigmoid'

### Dataset Option 1
dataset_option = 'SnP'

### Dataset Option 2
# dataset_option = 'crypto'


# - **Random Dropout applied to trained DNN**: This learns to predict DNNs with dropout's behaviour *(a.k.a.: this distribution; for any $x$ in the input space).*

# In[13]:


# f_unknown_mode = "DNN_with_Bayesian_Dropout"
Dropout_rate = 0.75


# - **GD with Randomized Input**: This learns to predict the "random behaviour" *(a.k.a. the distribution)* of SGD with randomized input.  

# In[14]:


# f_unknown_mode = "GD_with_randomized_input"
# GD_epochs = 50


# - **SDE with fractional Driver**:
# This learns to predict $(t,x)\mapsto Law(X_t^x)$ where 
# $$
# X_t^x = x + \int_0^t \alpha(s,X_s^x)ds + \int_0^t \beta(s,X_s^x)dB_s^H;
# $$
# where $(B_t^H)_t$ is a [*frational Brownian motion*](https://arxiv.org/pdf/1406.1956.pdf) with [Hurst exponent](https://en.wikipedia.org/wiki/Hurst_exponent) in $[\frac1{2},1)$.

# In[15]:


# Depricated: f_unknown_mode = "Rough_SDE"
#f_unknown_mode = "Rough_SDE_Vanilla"
## Define Process' dynamics in (2) cell(s) below.


# - **Deep Bayesian DNN**: This next task predicts the predictions of a Bayesian DNN with random internal weights...everything is fully random. 
# 
# **Note:** *(This task is not as interesting as the rest but a good "sanity check" task..but it's great for debugging).

# In[16]:


Depth_Bayesian_DNN = 1
N_Random_Features = 10**2
## Simulated Data version
# f_unknown_mode = "DNN_with_Random_Weights"
width = 10**2


# ---

# #### Grid Hyperparameter(s)
# - Ratio $\frac{\text{Testing Datasize}}{\text{Training Datasize}}$.
# - Number of Training Points to Generate

# In[17]:


train_test_ratio = .1
N_train_size = 10**3


# Monte-Carlo Paramters

# In[18]:


## Monte-Carlo
N_Monte_Carlo_Samples = 10**4
N_Monte_Carlo_Samples_Test = 10**4 # How many MC-samples to draw from test-set?


# Initial radis of $\delta$-bounded random partition of $\mathcal{X}$!

# In[19]:


# Hyper-parameters of Cover
delta = 0.1
Proportion_per_cluster = .75


# ---

# ---

# ---

# # Execute Code:

# ---

# ---

# ---

# #### Get Internal Parameter(s) and Failsafe(s)
# These are mostly fail-safes and relabeled internal parameters used when merging code written on different days/moods.  

# In[20]:


if f_unknown_mode != 'Extreme_Learning_Machine':
    width = int(2*(problem_dim+1))
    
N_Euler_Steps = N_Euler_Maruyama_Steps
Hurst_Exponent = Hurst_Exponent


# # Simulate or Parse Data

# In[21]:


#-------------------------------------------------------#
print("Generating/Prasing Data and Training MC-Oracle")
#-------------------------------------------------------#
if (f_unknown_mode != 'Rough_SDE') and (f_unknown_mode != 'Rough_SDE_Vanilla'):
    # %run Data_Simulator_and_Parser.ipynb
    exec(open('./Helper_Scripts_and_Loading/Data_Simulator_and_Parser.py').read())
else:
    # Renaming Some internal Parameter(s)
    groud_truth = "rSDE"
    test_size_ratio = train_test_ratio
    Ratio_fBM_to_typical_vol = 1
    output_dim = problem_dim
    T_begin = 0
    T_end = 1
    # Run da code
#     %run Fractional_SDE/fractional_SDE_Simulator.ipynb
#     %run Fractional_SDE/Data_Simulator_and_Parser___fractional_SDE.ipynb
    exec(open('Fractional_SDE/fractional_SDE_Simulator.py').read())
    exec(open('Fractional_SDE/Data_Simulator_and_Parser___fractional_SDE.py').read())

# Verbosity is nice
print("Generated Data:")
print("Number of Training Datums:"+str(X_train.shape[0]))
print("Number of Testing Datums:"+str(X_test.shape[0]))


# ### Rescale

# In[22]:


# Rescale
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


# ## Run: Main (DNM) and Oracle Models

# In[ ]:


print("------------------------------")
print("Running script for main model!")
print("------------------------------")
# %run ./Models_Scripts/Universal_Measure_Valued_Networks_Backend.ipynb
exec(open('./Models_Scripts/Universal_Measure_Valued_Networks_Backend.py').read())
print("------------------------------------")
print("Done: Running script for main model!")
print("------------------------------------")


# ### Evaluate Main and Oracle Model's Performance

# In[ ]:


# %run ./Helper_Scripts_and_Loading/Universal_Measure_Valued_Networks_Backend_EVALUATOR.ipynb
exec(open('./Helper_Scripts_and_Loading/Universal_Measure_Valued_Networks_Backend_EVALUATOR.py').read())


# ## 1) *Pointmass Benchmark(s)*
# These benchmarks consist of subsets of $C(\mathbb{R}^d,\mathbb{R})$ which we lift to models in $C(\mathbb{R}^d,\cap_{1\leq q<\infty}\mathscr{P}_{q}(\mathbb{R}))$ via:
# $$
# \mathbb{R}^d \ni x \to f(x) \to \delta_{f(x)}\in \cap_{1\leq q<\infty}\mathcal{P}_{q}(\mathbb{R}).
# $$

# In[ ]:


exec(open('./Helper_Scripts_and_Loading/CV_Grid.py').read())
# Notebook Mode:
# %run Evaluation.ipynb
# %run ./Models_Scripts/Benchmarks_Model_Builder_Pointmass_Based.ipynb
# Terminal Mode (Default):
exec(open('./Helper_Scripts_and_Loading/Evaluation.py').read())
exec(open('./Models_Scripts/Benchmarks_Model_Builder_Pointmass_Based.py').read())


# # Summary of Point-Mass Regression Models

# #### Training Model Facts

# In[ ]:


print(Summary_pred_Qual_models)
Summary_pred_Qual_models


# #### Testing Model Facts

# In[ ]:


print(Summary_pred_Qual_models_test)
Summary_pred_Qual_models_test


# ## 2) *Gaussian Benchmarks*

# - Bencharm 1: [Gaussian Process Regressor](https://scikit-learn.org/stable/modules/gaussian_process.html)
# - Benchmark 2: Deep Gaussian Networks:
# These models train models which assume Gaussianity.  We may view these as models in $\mathcal{P}_2(\mathbb{R})$ via:
# $$
# \mathbb{R}^d \ni x \to (\hat{\mu}(x),\hat{\Sigma}(x)\hat{\Sigma}^{\top})\triangleq f(x) \in \mathbb{R}\times [0,\infty) \to 
# (2\pi)^{-\frac{d}{2}}\det(\hat{\Sigma}(x))^{-\frac{1}{2}} \, e^{ -\frac{1}{2}(\cdot - \hat{\mu}(x))^{{{\!\mathsf{T}}}} \hat{\Sigma}(x)^{-1}(\cdot - \hat{\mu}(x)) } \mu \in \mathcal{G}_d\subset \mathcal{P}_2(\mathbb{R});
# $$
# where $\mathcal{G}_1$ is the set of Gaussian measures on $\mathbb{R}$ equipped with the relative Wasserstein-1 topology.
# 
# Examples of this type of architecture are especially prevalent in uncertainty quantification; see ([Deep Ensembles](https://arxiv.org/abs/1612.01474)] or [NOMU: Neural Optimization-based Model Uncertainty](https://arxiv.org/abs/2102.13640).  Moreover, their universality in $C(\mathbb{R}^d,\mathcal{G}_2)$ is known, and has been shown in [Corollary 4.7](https://arxiv.org/abs/2101.05390).

# In[ ]:


# %run Jupyter_Notebooks/Jupyter_Notebooks_for_Final_Implementation/Benchmarks_Model_Builder_Mean_Var.ipynb
exec(open('./Models_Scripts/Benchmarks_Model_Builder_Mean_Var.py').read())


# In[ ]:


print("Prediction Quality (Updated): Test")
print(Summary_pred_Qual_models_test)
Summary_pred_Qual_models_test


# In[ ]:


print("Prediction Quality (Updated): Train")
print(Summary_pred_Qual_models)
Summary_pred_Qual_models


# # 3) The natural Universal Benchmark: [Bishop's Mixture Density Network](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)
# 
# This implementation is as follows:
# - For every $x$ in the trainingdata-set we fit a GMM $\hat{\nu}_x$, using the [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), with the same number of centers as the deep neural model in $\mathcal{NN}_{1_{\mathbb{R}^d},\mathcal{D}}^{\sigma:\star}$ which we are evaluating.  
# - A Mixture density network is then trained to predict the infered parameters; given any $x \in \mathbb{R}^d$.

# In[ ]:


if output_dim == 1:
    # %run Mixture_Density_Network.ipynb
    exec(open('./Models_Scripts/Mixture_Density_Network.py').read())


# ## Get Final Outputs
# Now we piece together all the numerical experiments and report a nice summary.

# ---
# # Final Results
# ---

# ## Prasing Quality Metric Results

# #### Finalizing Saving
# **Note:** *We do it in two steps since the grid sometimes does not want to write nicely...*

# In[ ]:


## Write Performance Metrics
### Incase caption breaks
Summary_pred_Qual_models.to_latex((results_tables_path+"/Final_Results/"+"Performance_metrics_Problem_Type_"+str(f_unknown_mode)+"Problemdimension"+str(problem_dim)+"__SUMMARY_METRICS.tex"),
                                 float_format="{:0.3g}".format)
text_file = open((results_tables_path+"/Final_Results/"+"ZZZ_CAPTION_Performance_metrics_Problem_Type_"+str(f_unknown_mode)+"Problemdimension"+str(problem_dim)+"__SUMMARY_METRICS___CAPTION.tex"), "w")
text_file.write("Quality Metrics; d:"+str(problem_dim)+", D:"+str(output_dim)+", Depth:"+str(Depth_Bayesian_DNN)+", Width:"+str(width)+", Dropout rate:"+str(Dropout_rate)+".")
text_file.close()


### Incase caption does not break
Summary_pred_Qual_models.to_latex((results_tables_path+"/Final_Results/"+"Performance_metrics_Problem_Type_"+str(f_unknown_mode)+"Problemdimension"+str(problem_dim)+"__SUMMARY_METRICS.tex"),
                                 caption=("Quality Metrics; d:"+str(problem_dim)+", D:"+str(output_dim)+", Depth:"+str(Depth_Bayesian_DNN)+", Width:"+str(width)+", Dropout rate:"+str(Dropout_rate)+"."),
                                 float_format="{:0.3g}".format)


# # For Terminal Runner(s):

# In[ ]:


# For Terminal Running
print("===================")
print("Predictive Quality:")
print("===================")
print(Summary_pred_Qual_models)
print("===================")
print(" ")
print(" ")
print(" ")
print("Kernel_Used_in_GPR: "+str(GPR_trash.kernel))
print("ðŸ™ƒðŸ™ƒ Have a wonderful day! ðŸ™ƒðŸ™ƒ")
Summary_pred_Qual_models


# ---
# # Fin
# ---

# ---
