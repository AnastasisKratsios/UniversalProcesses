{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Conditional Laws for Random-Fields - via:\n",
    "\n",
    "## Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model (Type A)\n",
    "\n",
    "---\n",
    "\n",
    "By: [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Algorithm:\n",
    "---\n",
    "## 1) Generate Data:\n",
    "Generates the empirical measure $\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$ of $X_T$ conditional on $X_0=x_0\\in \\mathbb{R}$ *($x_0$ and $T>0$ are user-provided)*.\n",
    "\n",
    "## 2) Get \"Sample Barycenters\":\n",
    "Let $\\{\\mu_n\\}_{n=1}^N\\subset\\mathcal{P}_1(\\mathbb{R}^d)$.  Then, the *sample barycenter* is defined by:\n",
    "1. $\\mathcal{M}^{(0)}\\triangleq \\left\\{\\hat{\\mu}_n\\right\\}_{n=1}^N$,\n",
    "2. For $1\\leq n\\leq \\mbox{N sample barycenters}$: \n",
    "    - $\n",
    "\\mu^{\\star}\\in \\underset{\\tilde{\\mu}\\in \\mathcal{M}^{(n)}}{\\operatorname{argmin}}\\, \\sum_{n=1}^N \\mathcal{W}_1\\left(\\mu^{\\star},\\mu_n\\right),\n",
    "$\n",
    "    - $\\mathcal{M}^{(n)}\\triangleq \\mathcal{M}^{(n-1)} - \\{\\mu^{\\star}\\},$\n",
    "*i.e., the closest generated measure form the random sample to all other elements of the random sample.*\n",
    "\n",
    "---\n",
    "**Note:** *We simplify the computational burden of getting the correct classes by putting this right into this next loop.*\n",
    "\n",
    "## 3) Train Deep Classifier:\n",
    "$\\hat{f}\\in \\operatorname{argmin}_{f \\in \\mathcal{NN}_{d:N}^{\\star}} \n",
    "\\sum_{x \\in \\mathbb{X}}\n",
    "\\, \n",
    "\\mathbb{H}\n",
    "\\left(\n",
    "    \\operatorname{Softmax}_N\\circ f(x)_n| I\\left\\{W_1(\\hat{\\mu}_n,\\mu_x),\\inf_{m\\leq N} W_1(\\hat{\\mu}_m,\\mu_x)\\right\\}\n",
    "\\right);\n",
    "$\n",
    "where $\\mathbb{H}$ is the categorical cross-entropy.  \n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "## Notes - Why the procedure is so computationally efficient?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many random polulations to visualize:\n",
    "Visualization_Size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground Truth:\n",
    "*The build-in Options:*\n",
    "- rSDE \n",
    "- pfBM\n",
    "- 2lnflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1:\n",
    "# groud_truth = \"rSDE\"\n",
    "# Option 2:\n",
    "groud_truth = \"2lnflow\"\n",
    "## Option 3:\n",
    "# groud_truth = \"pfBM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Euler_Maruyama_Steps = 10\n",
    "N_Monte_Carlo_Samples = 10**3\n",
    "N_Monte_Carlo_Samples_Test = 10**3 # How many MC-samples to draw from test-set?\n",
    "\n",
    "# End times for Time-Grid\n",
    "T_end = 1\n",
    "T_end_test = 1.1\n",
    "\n",
    "\n",
    "## Grid\n",
    "N_Grid_Finess = 10\n",
    "Max_Grid = 1\n",
    "\n",
    "# Number of Centers (\\hat{\\mu}_s)\n",
    "N_Quantizers_to_parameterize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.01\n",
    "N_measures_per_center = 10*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Setting *N_Quantizers_to_parameterize* prevents any barycenters and sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP:\n",
    "from operator import itemgetter \n",
    "from itertools import compress\n",
    "# Set Minibatch Size\n",
    "Random_Cover_Mini_Batch_Size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation from Rough SDE\n",
    "Simulate via Euler-M method from:\n",
    "$$ \n",
    "X_T = x + \\int_0^T \\alpha(s,x)ds + \\int_0^T((1-\\eta)\\beta(s,x)+\\eta\\sigma_s^H)dW_s.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(t,x):\n",
    "    return .1*(.1-.5*(.01**2))*t + np.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(t,x):\n",
    "    return 0.01+t*np.cos(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughness Meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rougness = 0.9 # Hurst Parameter\n",
    "Ratio_fBM_to_typical_vol = 0 # $\\eta$ in equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation from Measure-Valued $2$-Parameter Log-Gaussian Flow\n",
    "$$\n",
    "X_{t,x} \\sim \\log\\text{-}\\mathcal{N}\\left(\\alpha(t,x),\\beta(t,x)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *$\\alpha$ and $\\beta$ are specified below in the SDE Example*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed Fractional Brownian Motion\n",
    "Simulate from:\n",
    "$$\n",
    "X_t^x(\\omega) = f_1(x)f_2(t) + B_t^H(\\omega).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_dirction_x(x):\n",
    "    return x*np.cos(x)\n",
    "\n",
    "def finite_variation_t(t):\n",
    "    return t*(np.sin(math.pi*t) + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Import time separately\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Internal (Hyper)-Parameter(s)\n",
    "*Initialize the hyperparameters which are fully-specified by the user-provided hyperparameter(s).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of Auxiliary Internal-Variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize (Empirical) Weight(s)\n",
    "measure_weights = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples\n",
    "measure_weights_test = np.ones(N_Monte_Carlo_Samples_Test)/N_Monte_Carlo_Samples_Test\n",
    "\n",
    "# Get number of centers\n",
    "N_Centers_per_box = max(1,int(round(np.sqrt(N_Quantizers_to_parameterize))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Centers Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get x and t grid of \"centers\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Coordinates Grid\n",
    "\n",
    "*We separate the case of a $2$-parameter measure-valued flow from the SDE example as follows:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grid of Barycenters\n",
    "x_Grid_barycenters = np.arange(start=-Max_Grid,\n",
    "                               stop=Max_Grid,\n",
    "                               step = (2*Max_Grid/N_Centers_per_box))\n",
    "if groud_truth == \"2lnflow\":\n",
    "    t_Grid_barycenters = np.arange(start=0,\n",
    "                                   stop=T_end,\n",
    "                                   step = (T_end/N_Centers_per_box))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If we do not consider the 2-paramter (probability) measure-valued flow model; then we start all time points at the same place; so as to ensure that the \"test set\" consists exactly of future times corresponding to trained initial states!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(groud_truth == \"2lnflow\"):\n",
    "    t_Grid_barycenters = np.arange(start=0,\n",
    "                               stop=T_end,\n",
    "                               step = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Full-Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x_i in range(len(x_Grid_barycenters)):\n",
    "    for t_j in range(len(t_Grid_barycenters)):\n",
    "        new_grid_entry = np.array([t_Grid_barycenters[t_j],x_Grid_barycenters[x_i]]).reshape(1,-1)\n",
    "        if (x_i==0 and t_j ==0):\n",
    "            Grid_Barycenters = new_grid_entry\n",
    "        else:\n",
    "            Grid_Barycenters = np.append(Grid_Barycenters,new_grid_entry,axis=0)\n",
    "\n",
    "# Update Number of Quantizers Generated\n",
    "N_Quantizers_to_parameterize = Grid_Barycenters.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "This is $\\mathbb{X}$ and it represents the grid of initial states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD Simulator (Backend)\n",
    "# %run Simulator.ipynb\n",
    "exec(open('Simulator.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Timer (Model Type A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "Type_A_timer_Begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2$-Parameter $\\log$-Gaussian Flow\n",
    "Generate data by sampling from a random-field $(X_t^x)_{t,x}$ distributed according to:\n",
    "$$\n",
    "X_{t,x} \\sim \\log\\text{-}\\mathcal{N}\\left(\\alpha(t,x),\\beta(t,x)\\right).\n",
    "$$\n",
    "\n",
    "### Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 520.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Training Set - 2-logNormal Ground-Truth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if groud_truth == \"2lnflow\":\n",
    "    print(\"Building Training Set - 2-logNormal Ground-Truth\")\n",
    "    # Generate Training Data\n",
    "    for i in tqdm(range(Grid_Barycenters.shape[0])):\n",
    "        # Get output for center (mu-hat)\n",
    "        center_current, trash = twoparameter_flow_sampler((Grid_Barycenters[i]).reshape(1,2),N_Monte_Carlo_Samples)\n",
    "\n",
    "        # Get random sample in delta ball around ith center\n",
    "        sub_grid_loop = np.random.uniform(0,delta,(N_measures_per_center,2)) + Grid_Barycenters[i]\n",
    "\n",
    "        # Get Measures for this random sample\n",
    "        measures_locations_list_current, measures_weights_list_current = twoparameter_flow_sampler(sub_grid_loop,N_Monte_Carlo_Samples)\n",
    "        ##\n",
    "        measures_locations_list_current = measures_locations_list_current + center_current\n",
    "        measures_weights_list_current = measures_weights_list_current + trash\n",
    "        # Update Classes\n",
    "        Classifer_Wasserstein_Centers_loop = np.zeros([(N_measures_per_center+1),N_Quantizers_to_parameterize]) # The +1 is to account for the center which will be added to the random ball\n",
    "        Classifer_Wasserstein_Centers_loop[:, i] =  1\n",
    "        # Updates Classes\n",
    "        if i==0:\n",
    "            # INITIALIZE: Classifiers\n",
    "            Classifer_Wasserstein_Centers = Classifer_Wasserstein_Centers_loop\n",
    "            # INITIALIZE: Training Data\n",
    "            X_train = np.append((Grid_Barycenters[i]).reshape(1,2),sub_grid_loop,axis=0)\n",
    "            # INITIALIZE: Barycenters Array\n",
    "            Barycenters_Array = (center_current[0]).reshape(-1,1)\n",
    "            # INITIALIZE: Measures and locations\n",
    "            measures_locations_list = measures_locations_list_current\n",
    "            measures_weights_list = measures_weights_list_current\n",
    "        else:\n",
    "            # UPDATE: Classifer\n",
    "            Classifer_Wasserstein_Centers = np.append(Classifer_Wasserstein_Centers,Classifer_Wasserstein_Centers_loop,axis=0)\n",
    "            # UPDATE: Training Data\n",
    "            X_train = np.append(X_train,np.append((Grid_Barycenters[i]).reshape(1,2),sub_grid_loop,axis=0),axis=0)\n",
    "            # UPDATE: Populate Barycenters Array\n",
    "            Barycenters_Array = np.append(Barycenters_Array,((center_current[0]).reshape(-1,1)),axis=-1)\n",
    "            # UPDATE: Measures and locations\n",
    "            measures_locations_list = measures_locations_list + measures_locations_list_current\n",
    "            measures_weights_list = measures_locations_list + measures_weights_list_current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6028.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Test Set - 2-logNormal Ground-Truth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if groud_truth == \"2lnflow\":\n",
    "    print(\"Building Test Set - 2-logNormal Ground-Truth\")\n",
    "    # Generate Testing Dataset (Inputs)\n",
    "    x_tests = np.random.uniform(np.min(X_train[:,0]),np.max(X_train[:,0]),10)\n",
    "    t_tests = np.arange(start=0,\n",
    "                        stop=T_end,\n",
    "                        step = (T_end_test/N_Euler_Maruyama_Steps))\n",
    "    for x_i in tqdm(range(len(x_tests))):\n",
    "        for t_j in range(len(t_tests)):\n",
    "            test_set_entry = np.array([t_tests[t_j],x_tests[x_i]]).reshape(1,-1)\n",
    "            if (x_i==0 and t_j ==0):\n",
    "                X_test = test_set_entry\n",
    "            else:\n",
    "                X_test = np.append(X_test,test_set_entry,axis=0)\n",
    "\n",
    "    # Generate Testing Dataset (Outputs)\n",
    "    measures_locations_test_list, measures_weights_test_list = twoparameter_flow_sampler(X_test,N_Monte_Carlo_Samples_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough SDE:\n",
    "Simulation of the random-field:\n",
    "$$\n",
    "X_t^x = x + \\int_0^t \\alpha(s,X_t^x)ds + (1-\\eta)\\int_0^t \\beta(s,X_t^x)dW_t + \\int_0^t B_s^H dW_s;\n",
    "$$\n",
    "where: \n",
    " - $(B_t^H)_t$ is a [fractional Brownian Motion](https://arxiv.org/pdf/1406.1956.pdf) with [Hurst exponent](https://en.wikipedia.org/wiki/Hurst_exponent) $H\\in (0,1)$,\n",
    " - $(W_t)_t$ is a [Brownian Motion](https://en.wikipedia.org/wiki/Wiener_process),\n",
    " - $\\alpha$ and $\\beta$ are uniformly [Lipschitz-functions](https://en.wikipedia.org/wiki/Lipschitz_continuity) of appropriate input/output dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD Simulator (Backend)\n",
    "# %run Simulator.ipynb\n",
    "exec(open('Simulator.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW?\n",
    "if groud_truth == \"rSDE\":\n",
    "    print(\"Building Training + Testing Set - rough-SDE Ground-Truth\")\n",
    "    \n",
    "    # Initialize position Counter\n",
    "    position_counter = 0\n",
    "    # Iniitalize uniform weights vector\n",
    "    measures_weights_list_loop = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples\n",
    "\n",
    "    # For simplicity override:\n",
    "    N_Monte_Carlo_Samples_Test = N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Overrine Number of Centers\n",
    "    N_x = len(x_Grid_barycenters)\n",
    "    N_t = len(t_Grid_barycenters)\n",
    "    N_Quantizers_to_parameterize = N_x*N_t\n",
    "    \n",
    "    # Initialize number of training and testing to grab from each initial condition\n",
    "    N_train = int(N_Euler_Maruyama_Steps*(1-test_size_ratio))\n",
    "    N_test = N_Euler_Maruyama_Steps - N_train\n",
    "\n",
    "    for x_i in tqdm(range(N_x)):\n",
    "        for t_j in range(N_t):\n",
    "\n",
    "            # Get Current Locations\n",
    "            x_center = x_Grid_barycenters[x_i]\n",
    "            t_center = t_Grid_barycenters[t_j]\n",
    "\n",
    "            current_cover = Euler_Maruyama_Generator(x_0 = x_center,\n",
    "                                                     N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                                     N_Monte_Carlo_Samples = N_Monte_Carlo_Samples,\n",
    "                                                     T_begin = t_center,\n",
    "                                                     T_end = (t_center+delta),\n",
    "                                                     Hurst = Rougness,\n",
    "                                                     Ratio_fBM_to_typical_vol = Ratio_fBM_to_typical_vol)\n",
    "            # Get Barycenter\n",
    "            barycenter_at_current_location = current_cover[0,:]\n",
    "            \n",
    "            # Subset\n",
    "            ## Measure Location(s)\n",
    "            measures_locations_list_current_train = (current_cover[:N_train]).tolist()\n",
    "            measures_locations_list_current_test = (current_cover[:-N_train]).tolist()\n",
    "            ## Measure Weight(s)\n",
    "            measures_weights_list_current = list(itertools.repeat(measures_weights_list_loop,N_Monte_Carlo_Samples))\n",
    "\n",
    "            \n",
    "            # Get Current Training Data Positions\n",
    "            t_grid_current = (np.linspace(start=t_center,\n",
    "                                          stop=(t_center+delta),\n",
    "                                          num=N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "            x_grid_current = (x_center*np.ones(N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "\n",
    "            X_train_current = (np.append(x_grid_current,t_grid_current,axis=0)).T\n",
    "            ## Subset\n",
    "            X_train_updater = X_train_current[:N_train,:] # Get top of array (including center)\n",
    "            X_test_updater = X_train_current[-N_test:,:] # Get bottom of array (exclusing center)\n",
    "\n",
    "            # Get Current Classes\n",
    "            Classifer_Wasserstein_Centers_loop = np.zeros([N_train,N_Quantizers_to_parameterize])\n",
    "            Classifer_Wasserstein_Centers_loop[:, position_counter] =  1\n",
    "\n",
    "\n",
    "            # Updates Classes\n",
    "            if (x_i==0 and t_j==0):\n",
    "                # INITIALIZE: Classifiers\n",
    "                Classifer_Wasserstein_Centers = Classifer_Wasserstein_Centers_loop\n",
    "                # INITIALIZE: Training Data\n",
    "                X_train = X_train_updater\n",
    "                X_test = X_test_updater\n",
    "                # INITIALIZE: Barycenters Array\n",
    "                Barycenters_Array = barycenter_at_current_location.reshape(-1,1)\n",
    "                # INITIALIZE: Measures and locations\n",
    "                measures_locations_list = measures_locations_list_current_train\n",
    "                measures_locations_test_list = measures_locations_list_current_test\n",
    "                measures_weights_list = measures_weights_list_current\n",
    "                measures_weights_test_list = measures_weights_list_current\n",
    "            else:\n",
    "                # UPDATE: Classifer\n",
    "                Classifer_Wasserstein_Centers = np.append(Classifer_Wasserstein_Centers,Classifer_Wasserstein_Centers_loop,axis=0)\n",
    "                # UPDATE: Training Data\n",
    "                X_train = np.append(X_train,X_train_updater,axis=0)\n",
    "                X_test = np.append(X_test,X_test_updater,axis=0)\n",
    "                # UPDATE: Populate Barycenters Array\n",
    "                Barycenters_Array = np.append(Barycenters_Array,(barycenter_at_current_location.reshape(-1,1)),axis=-1)\n",
    "                # UPDATE: Measures and locations\n",
    "                ## Train\n",
    "                measures_locations_list = measures_locations_list + measures_locations_list_current_train\n",
    "                measures_weights_list = measures_locations_list + measures_weights_list_current\n",
    "                ## Test\n",
    "                measures_locations_test_list = measures_locations_test_list + measures_locations_list_current_test\n",
    "                measures_weights_test_list = measures_locations_test_list + measures_weights_list_current\n",
    "\n",
    "            # Update Position\n",
    "            position_counter = position_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train a deep (feed-forward) classifier:\n",
    "$$\n",
    "\\hat{f}\\triangleq \\operatorname{Softmax}_N\\circ W_J\\circ \\sigma \\bullet \\dots \\sigma \\bullet W_1,\n",
    "$$\n",
    "to identify which barycenter we are closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Re-Load Classifier Function(s)\n",
    "exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   35.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   35.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 4.4844 - accuracy: 0.0248\n",
      "Epoch 2/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 3.7986 - accuracy: 0.0576\n",
      "Epoch 3/400\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 3.0455 - accuracy: 0.1610\n",
      "Epoch 4/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 2.4744 - accuracy: 0.2971\n",
      "Epoch 5/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 2.0761 - accuracy: 0.3962\n",
      "Epoch 6/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 1.7643 - accuracy: 0.5048\n",
      "Epoch 7/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.5159 - accuracy: 0.5910\n",
      "Epoch 8/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.3258 - accuracy: 0.6619\n",
      "Epoch 9/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.1496 - accuracy: 0.7114\n",
      "Epoch 10/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.0066 - accuracy: 0.7800\n",
      "Epoch 11/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.8784 - accuracy: 0.8110\n",
      "Epoch 12/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.7547 - accuracy: 0.8681\n",
      "Epoch 13/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.8995\n",
      "Epoch 14/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.5602 - accuracy: 0.9324\n",
      "Epoch 15/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.9352\n",
      "Epoch 16/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.9667\n",
      "Epoch 17/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.9667\n",
      "Epoch 18/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.9657\n",
      "Epoch 19/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.9829\n",
      "Epoch 20/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9924\n",
      "Epoch 21/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.1820 - accuracy: 0.9929\n",
      "Epoch 22/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.1520 - accuracy: 0.9981\n",
      "Epoch 23/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.1368 - accuracy: 0.9967\n",
      "Epoch 24/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9990\n",
      "Epoch 25/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9995\n",
      "Epoch 26/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 1.0000\n",
      "Epoch 27/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0778 - accuracy: 1.0000\n",
      "Epoch 28/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 1.0000\n",
      "Epoch 29/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 1.0000\n",
      "Epoch 30/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 1.0000\n",
      "Epoch 31/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 1.0000\n",
      "Epoch 32/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 33/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 1.0000\n",
      "Epoch 34/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 35/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 36/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 37/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 38/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 39/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 40/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 41/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 42/400\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 43/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 44/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 45/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 46/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 47/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 48/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 49/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 50/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 51/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 52/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 53/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 54/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 55/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 56/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 57/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 58/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 59/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 60/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 61/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 62/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 63/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 64/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 65/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 66/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 67/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 68/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 69/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 70/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 71/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 72/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 73/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 74/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 75/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 76/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 77/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 78/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 79/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 80/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 81/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 82/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 83/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 84/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 85/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 86/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 87/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 88/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 89/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 90/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 91/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 92/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 93/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 94/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 9.5407e-04 - accuracy: 1.0000\n",
      "Epoch 95/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 9.0542e-04 - accuracy: 1.0000\n",
      "Epoch 96/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 8.6818e-04 - accuracy: 1.0000\n",
      "Epoch 97/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 8.2502e-04 - accuracy: 1.0000\n",
      "Epoch 98/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.9300e-04 - accuracy: 1.0000\n",
      "Epoch 99/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.5705e-04 - accuracy: 1.0000\n",
      "Epoch 100/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.2695e-04 - accuracy: 1.0000\n",
      "Epoch 101/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.9500e-04 - accuracy: 1.0000\n",
      "Epoch 102/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.6465e-04 - accuracy: 1.0000\n",
      "Epoch 103/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.3865e-04 - accuracy: 1.0000\n",
      "Epoch 104/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.1198e-04 - accuracy: 1.0000\n",
      "Epoch 105/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.8747e-04 - accuracy: 1.0000\n",
      "Epoch 106/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.6373e-04 - accuracy: 1.0000\n",
      "Epoch 107/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.3812e-04 - accuracy: 1.0000\n",
      "Epoch 108/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.2079e-04 - accuracy: 1.0000\n",
      "Epoch 109/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.9605e-04 - accuracy: 1.0000\n",
      "Epoch 110/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.7405e-04 - accuracy: 1.0000\n",
      "Epoch 111/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.5252e-04 - accuracy: 1.0000\n",
      "Epoch 112/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.3966e-04 - accuracy: 1.0000\n",
      "Epoch 113/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.2253e-04 - accuracy: 1.0000\n",
      "Epoch 114/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 4.0420e-04 - accuracy: 1.0000\n",
      "Epoch 115/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.8380e-04 - accuracy: 1.0000\n",
      "Epoch 116/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.7185e-04 - accuracy: 1.0000\n",
      "Epoch 117/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.5562e-04 - accuracy: 1.0000\n",
      "Epoch 118/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.3939e-04 - accuracy: 1.0000\n",
      "Epoch 119/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.2706e-04 - accuracy: 1.0000\n",
      "Epoch 120/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.1333e-04 - accuracy: 1.0000\n",
      "Epoch 121/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 3.0154e-04 - accuracy: 1.0000\n",
      "Epoch 122/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.8951e-04 - accuracy: 1.0000\n",
      "Epoch 123/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.7778e-04 - accuracy: 1.0000\n",
      "Epoch 124/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.6745e-04 - accuracy: 1.0000\n",
      "Epoch 125/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.5556e-04 - accuracy: 1.0000\n",
      "Epoch 126/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.4872e-04 - accuracy: 1.0000\n",
      "Epoch 127/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.3580e-04 - accuracy: 1.0000\n",
      "Epoch 128/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.2694e-04 - accuracy: 1.0000\n",
      "Epoch 129/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.1833e-04 - accuracy: 1.0000\n",
      "Epoch 130/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 2.1227e-04 - accuracy: 1.0000\n",
      "Epoch 131/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.9870e-04 - accuracy: 1.0000\n",
      "Epoch 132/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.9704e-04 - accuracy: 1.0000\n",
      "Epoch 133/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.8425e-04 - accuracy: 1.0000\n",
      "Epoch 134/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.7945e-04 - accuracy: 1.0000\n",
      "Epoch 135/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.7186e-04 - accuracy: 1.0000\n",
      "Epoch 136/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.6581e-04 - accuracy: 1.0000\n",
      "Epoch 137/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.5957e-04 - accuracy: 1.0000\n",
      "Epoch 138/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.5216e-04 - accuracy: 1.0000\n",
      "Epoch 139/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.4683e-04 - accuracy: 1.0000\n",
      "Epoch 140/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.4017e-04 - accuracy: 1.0000\n",
      "Epoch 141/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.3576e-04 - accuracy: 1.0000\n",
      "Epoch 142/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.2956e-04 - accuracy: 1.0000\n",
      "Epoch 143/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.2448e-04 - accuracy: 1.0000\n",
      "Epoch 144/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.2074e-04 - accuracy: 1.0000\n",
      "Epoch 145/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.1516e-04 - accuracy: 1.0000\n",
      "Epoch 146/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.1157e-04 - accuracy: 1.0000\n",
      "Epoch 147/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.0683e-04 - accuracy: 1.0000\n",
      "Epoch 148/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 1.0202e-04 - accuracy: 1.0000\n",
      "Epoch 149/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 9.8835e-05 - accuracy: 1.0000\n",
      "Epoch 150/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 9.5502e-05 - accuracy: 1.0000\n",
      "Epoch 151/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 9.1350e-05 - accuracy: 1.0000\n",
      "Epoch 152/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 8.8726e-05 - accuracy: 1.0000\n",
      "Epoch 153/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 8.3868e-05 - accuracy: 1.0000\n",
      "Epoch 154/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 8.1930e-05 - accuracy: 1.0000\n",
      "Epoch 155/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.8013e-05 - accuracy: 1.0000\n",
      "Epoch 156/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.4677e-05 - accuracy: 1.0000\n",
      "Epoch 157/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 7.2920e-05 - accuracy: 1.0000\n",
      "Epoch 158/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.9620e-05 - accuracy: 1.0000\n",
      "Epoch 159/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.6763e-05 - accuracy: 1.0000\n",
      "Epoch 160/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 6.3843e-05 - accuracy: 1.0000\n",
      "Epoch 161/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 2ms/step - loss: 6.1409e-05 - accuracy: 1.0000\n",
      "Epoch 162/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.9489e-05 - accuracy: 1.0000\n",
      "Epoch 163/400\n",
      "66/66 [==============================] - 0s 2ms/step - loss: 5.8590e-05 - accuracy: 1.0000\n",
      "Epoch 164/400\n",
      " 1/66 [..............................] - ETA: 0s - loss: 5.5807e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "print(\"==========================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [2]\n",
    "param_grid_Deep_Classifier['output_dim'] = [N_Quantizers_to_parameterize]\n",
    "\n",
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter = n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = Classifer_Wasserstein_Centers,\n",
    "                                                                                                        X_test = X_test)\n",
    "\n",
    "print(\"=================================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model: Done!\")\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predicted Quantized Distributions\n",
    "- Each *row* of \"Predicted_Weights\" is the $\\beta\\in \\Delta_N$.\n",
    "- Each *Column* of \"Barycenters_Array\" denotes the $x_1,\\dots,x_N$ making up the points of the corresponding empirical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Weights\n",
    "## Train\n",
    "print(\"#---------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): START\")\n",
    "print(\"#---------------------------------------#\")\n",
    "Predicted_Weights = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):    \n",
    "    b = np.repeat(np.array(predicted_classes_train[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b = b/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights = b\n",
    "    else:\n",
    "        Predicted_Weights = np.append(Predicted_Weights,b,axis=1)\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): END\")\n",
    "print(\"#-------------------------------------#\")\n",
    "\n",
    "## Test\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): START\")\n",
    "print(\"#-------------------------------------#\")\n",
    "Predicted_Weights_test = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):\n",
    "    b_test = np.repeat(np.array(predicted_classes_test[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b_test = b_test/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights_test = b_test\n",
    "    else:\n",
    "        Predicted_Weights_test = np.append(Predicted_Weights_test,b_test,axis=1)\n",
    "print(\"#-----------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): END\")\n",
    "print(\"#-----------------------------------#\")\n",
    "        \n",
    "# Format Points of Mass\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: START\")\n",
    "print(\"#-----------------------------#\")\n",
    "Barycenters_Array = Barycenters_Array.T.reshape(-1,)\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: END\")\n",
    "print(\"#-----------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Type_A_timer_end = time.time()\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Time_Lapse_Model_A = Type_A_timer_end - Type_A_timer_Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Moment Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Training Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors = np.array([])\n",
    "Mean_errors = np.array([])\n",
    "Var_errors = np.array([])\n",
    "Skewness_errors = np.array([])\n",
    "Kurtosis_errors = np.array([])\n",
    "predictions_mean = np.array([])\n",
    "true_mean = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_list)-1)):    \n",
    "    # Get Laws\n",
    "    W1_loop = ot.emd2_1d(Barycenters_Array,\n",
    "                         np.array(measures_locations_list[x_i]).reshape(-1,),\n",
    "                         Predicted_Weights[x_i,].reshape(-1,),\n",
    "                         (np.array(measures_weights_list[x_i])).reshape(-1,))\n",
    "    W1_errors = np.append(W1_errors,W1_loop)\n",
    "    # Get Means\n",
    "    Mu_hat = np.sum((Predicted_Weights[x_i])*(Barycenters_Array))\n",
    "    Mu = np.mean(np.array(measures_locations_list[x_i]))\n",
    "    Mean_errors =  np.append(Mean_errors,(Mu_hat-Mu))\n",
    "    ## Update Erros\n",
    "    predictions_mean = np.append(predictions_mean,Mu_hat)\n",
    "    true_mean = np.append(true_mean,Mu)\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat = np.sum((Barycenters_Array**2)*(Predicted_Weights[x_i]))\n",
    "    Var = np.mean(np.array(measures_locations_list[x_i])**2)\n",
    "    Var_errors = np.append(Var_errors,(Var_hat-Var)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat = np.sum((Barycenters_Array**3)*(Predicted_Weights[x_i]))\n",
    "    Skewness = np.mean(np.array(measures_locations_list[x_i])**3)\n",
    "    Skewness_errors = np.append(Skewness_errors,(abs(Skewness_hat-Skewness))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat = np.sum((Barycenters_Array**4)*(Predicted_Weights[x_i]))\n",
    "    Kurtosis = np.mean(np.array(measures_locations_list[x_i])**4)\n",
    "    Kurtosis_errors = np.append(Kurtosis_errors,(abs(Kurtosis_hat-Kurtosis))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance = np.array([np.min(np.abs(W1_errors)),np.mean(np.abs(W1_errors)),np.max(np.abs(W1_errors))])\n",
    "Mean_prediction_Performance = np.array([np.min(np.abs(Mean_errors)),np.mean(np.abs(Mean_errors)),np.max(np.abs(Mean_errors))])\n",
    "Var_prediction_Performance = np.array([np.min(np.abs(Var_errors)),np.mean(np.abs(Var_errors)),np.max(np.abs(Var_errors))])\n",
    "Skewness_prediction_Performance = np.array([np.min(np.abs(Skewness_errors)),np.mean(np.abs(Skewness_errors)),np.max(np.abs(Skewness_errors))])\n",
    "Kurtosis_prediction_Performance = np.array([np.min(np.abs(Kurtosis_errors)),np.mean(np.abs(Kurtosis_errors)),np.max(np.abs(Kurtosis_errors))])\n",
    "\n",
    "Type_A_Prediction = pd.DataFrame({\"W1\":W1_Performance,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance},index=[\"Min\",\"MAE\",\"Max\"])\n",
    "\n",
    "# Write Performance\n",
    "Type_A_Prediction.to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Train.tex\"))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Update User\n",
    "Type_A_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Test Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors_test = np.array([])\n",
    "Mean_errors_test = np.array([])\n",
    "Var_errors_test = np.array([])\n",
    "Skewness_errors_test = np.array([])\n",
    "Kurtosis_errors_test = np.array([])\n",
    "# Initialize Prediction Metrics\n",
    "predictions_mean_test = np.array([])\n",
    "true_mean_test = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_test_list))):    \n",
    "    # Get Laws\n",
    "    W1_loop_test = ot.emd2_1d(Barycenters_Array,\n",
    "                         np.array(measures_locations_test_list[x_i]).reshape(-1,),\n",
    "                         Predicted_Weights_test[x_i,].reshape(-1,),\n",
    "                         (np.array(measures_weights_test_list[x_i])).reshape(-1,))\n",
    "    W1_errors_test = np.append(W1_errors_test,W1_loop_test)\n",
    "    # Get Means\n",
    "    Mu_hat_test = np.sum((Predicted_Weights_test[x_i])*(Barycenters_Array))\n",
    "    Mu_test = np.mean(np.array(measures_locations_test_list[x_i]))\n",
    "    Mean_errors_test = np.append(Mean_errors_test,(Mu_hat_test-Mu_test))\n",
    "    ## Update Predictions\n",
    "    predictions_mean_test = np.append(predictions_mean_test,Mu_hat_test)\n",
    "    true_mean_test = np.append(true_mean_test,Mu_test)\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat_test = np.sum((Barycenters_Array**2)*(Predicted_Weights_test[x_i]))\n",
    "    Var_test = np.mean(np.array(measures_locations_test_list[x_i])**2)\n",
    "    Var_errors_test = np.append(Var_errors_test,(Var_hat_test-Var_test)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat_test = np.sum((Barycenters_Array**3)*(Predicted_Weights_test[x_i]))\n",
    "    Skewness_test = np.mean(np.array(measures_locations_test_list[x_i])**3)\n",
    "    Skewness_errors_test = np.append(Skewness_errors_test,(abs(Skewness_hat_test-Skewness_test))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat_test = np.sum((Barycenters_Array**4)*(Predicted_Weights_test[x_i]))\n",
    "    Kurtosis_test = np.mean(np.array(measures_locations_test_list[x_i])**4)\n",
    "    Kurtosis_errors_test = np.append(Kurtosis_errors_test,(abs(Kurtosis_hat_test-Kurtosis_test))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance_test = np.array([np.min(np.abs(W1_errors_test)),np.mean(np.abs(W1_errors_test)),np.mean(np.abs(W1_errors_test))])\n",
    "Mean_prediction_Performance_test = np.array([np.min(np.abs(Mean_errors_test)),np.mean(np.abs(Mean_errors_test)),np.mean(np.abs(Mean_errors_test))])\n",
    "Var_prediction_Performance_test = np.array([np.min(np.abs(Var_errors_test)),np.mean(np.abs(Var_errors_test)),np.mean(np.abs(Var_errors_test))])\n",
    "Skewness_prediction_Performance_test = np.array([np.min(np.abs(Skewness_errors_test)),np.mean(np.abs(Skewness_errors_test)),np.mean(np.abs(Skewness_errors_test))])\n",
    "Kurtosis_prediction_Performance_test = np.array([np.min(np.abs(Kurtosis_errors_test)),np.mean(np.abs(Kurtosis_errors_test)),np.mean(np.abs(Kurtosis_errors_test))])\n",
    "\n",
    "Type_A_Prediction_test = pd.DataFrame({\"W1\":W1_Performance_test,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance_test,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance_test,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance_test,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance_test},index=[\"Min\",\"MAE\",\"Max\"])\n",
    "\n",
    "# Write Performance\n",
    "Type_A_Prediction_test.to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Test.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Training-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predictions_mean,label=\"prediction\",color=\"purple\")\n",
    "plt.plot(true_mean,label=\"true\",color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Test-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions_mean_test,color=\"purple\")\n",
    "plt.plot(true_mean_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print for Terminal Legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#----------------------#\")\n",
    "print(\"Training-Set Performance\")\n",
    "print(\"#----------------------#\")\n",
    "print(Type_A_Prediction)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"#------------------#\")\n",
    "print(\"Test-Set Performance\")\n",
    "print(\"#------------------#\")\n",
    "print(Type_A_Prediction_test)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_A_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_A_Prediction_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
