{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Universal Regular Conditional Expectations:\n",
    "\n",
    "---\n",
    "This implements the universal deep neural model of $\\mathcal{NN}_{1_{\\mathbb{R}^n},\\mathcal{D}}^{\\sigma:\\star}$ [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---\n",
    "\n",
    "## What does this code do?\n",
    "1. Learn Heteroskedastic Non-Linear Regression Problem\n",
    "     - $Y\\sim f_{\\text{unkown}}(x) + \\epsilon$ where $f$ is an known function and $\\epsilon\\sim Laplace(0,\\|x\\|)$\n",
    "2. Learn Random Bayesian Network's Law:\n",
    "    - $Y = W_J Y^{J-1}, \\qquad Y^{j}\\triangleq \\sigma\\bullet A^{j}Y^{j-1} + b^{j}, \\qquad Y^0\\triangleq x$\n",
    "\n",
    "3. In the above example if $A_j = M_j\\odot \\tilde{A_j}$ where $\\tilde{A}_j$ is a deterministic matrix and $M_j$ is a \"mask\", that is, a random matrix with binary entries and $\\odot$ is the Hadamard product then we recover the dropout framework.\n",
    "4. Learn the probability distribution that the unique strong solution to the rough SDE with uniformly Lipschitz drivers driven by a factional Brownian motion with Hurst exponent $H \\in [\\frac1{2},1)$:\n",
    "$$\n",
    "X_t^x = x + \\int_0^t \\alpha(s,X_s^x)ds + \\int_0^t \\beta(s,X_s^x)dB_s^H\n",
    "$$\n",
    "belongs, at time $t=1$, to a ball about the initial point $x$ of random radius given by an independant exponential random-variable with shape parameter $\\lambda=2$\n",
    "5. Train a DNN to predict the returns of bitcoin with GD.  Since this has random initialization then each prediction of a given $x$ is stochastic...We learn the distribution of this conditional RV (conditioned on x in the input space).\n",
    "$$\n",
    "Y_x \\triangleq \\hat{f}_{\\theta_{T}}(x), \\qquad \\theta_{(t+1)}\\triangleq \\theta_{(t)} + \\lambda \\sum_{x \\in \\mathbb{X}} \\nabla_{\\theta}\\|\\hat{f}_{\\theta_t}(x) - f(x)\\|, \\qquad \\theta_0 \\sim N_d(0,1);\n",
    "$$\n",
    "$T\\in \\mathbb{N}$ is a fixed number of \"SGD\" iterations (typically identified by cross-validation on a single SGD trajectory for a single initialization) and where $\\theta \\in \\mathbb{R}^{(d_{J}+1)+\\sum_{j=0}^{J-1} (d_{j+1}d_j + 1)}$ and $d_j$ is the dimension of the \"bias\" vector $b_j$ defining each layer of the DNN with layer dimensions:\n",
    "$$\n",
    "\\hat{f}_{\\theta}(x)\\triangleq A^{(J)}x^{(J)} + b^{(J)},\\qquad x^{(j+1)}\\triangleq \\sigma\\bullet A^{j}x^{(j)} + b^{j},\\qquad x^{(0)}\\triangleq x\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode:\n",
    "Software/Hardware Testing or Real-Deal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random DNN\n",
    "# f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# Random DNN internal noise\n",
    "#f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "Depth_Bayesian_DNN = 2\n",
    "width = 10\n",
    "\n",
    "# Random Dropout applied to trained DNN\n",
    "f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "Dropout_rate = 0.1\n",
    "\n",
    "# GD with Randomized Input\n",
    "# f_unknown_mode = \"GD_with_randomized_input\"\n",
    "GD_epochs = 50\n",
    "\n",
    "# SDE with fractional Driver\n",
    "# f_unknown_mode = \"Rough_SDE\"\n",
    "N_Euler_Steps = 10**2\n",
    "Hurst_Exponent = 0.75\n",
    "\n",
    "# f_unknown_mode = \"Rough_SDE_Vanilla\"\n",
    "## Define Process' dynamics in (2) cell(s) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dim = 1\n",
    "width = int(2*(problem_dim+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla fractional SDE:\n",
    "If f_unknown_mode == \"Rough_SDE_Vanilla\" is selected, then we can specify the process's dynamics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------#\n",
    "# Define Process' Dynamics #\n",
    "#--------------------------#\n",
    "drift_constant = 0.1\n",
    "volatility_constant = 0.01\n",
    "\n",
    "# Define DNN Applier\n",
    "def f_unknown_drift_vanilla(x):\n",
    "    x_internal = x\n",
    "    x_internal = drift_constant*x_internal\n",
    "    return x_internal\n",
    "def f_unknown_vol_vanilla(x):\n",
    "    x_internal = volatility_constant*np.diag(np.ones(problem_dim))\n",
    "    return x_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: *Why the procedure is so computationally efficient*?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)\n",
    "- Ratio $\\frac{\\text{Testing Datasize}}{\\text{Training Datasize}}$.\n",
    "- Number of Training Points to Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = .2\n",
    "N_train_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Monte_Carlo_Samples = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial radis of $\\delta$-bounded random partition of $\\mathcal{X}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.1\n",
    "Proportion_per_cluster = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Auxiliary Script(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# %run Loader.ipynb\n",
    "exec(open('Loader.py').read())\n",
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "import time as time #<- Note sure why...but its always seems to need 'its own special loading...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate or Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Beginning Data-Parsing/Simulation Phase\n",
      "---------------------------------------\n",
      "Deciding on Which Simulator/Parser To Load\n",
      "Setting/Defining: Internal Parameters\n",
      "Deciding on Which Type of Data to Get/Simulate\n",
      "Simulating Output Data for given input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:56<00:00,  5.67s/it]\n",
      "100%|██████████| 2/2 [00:11<00:00,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Done Data-Parsing/Simulation Phase\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run Data_Simulator_and_Parser.ipynb\n",
    "exec(open('Data_Simulator_and_Parser.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Data\n",
    "This is especially important to avoid exploding gradient problems when training the ML-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Running script for main model!\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [00:00<00:00, 8672.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "==========================================\n",
      "Training Classifer Portion of Type-A Model\n",
      "==========================================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   38.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   38.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 6.6348 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.5394 - accuracy: 0.0040\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.1395 - accuracy: 0.0070\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.7843 - accuracy: 0.0110\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 5.4588 - accuracy: 0.0190\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.1810 - accuracy: 0.0270\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.9293 - accuracy: 0.0370\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.7306 - accuracy: 0.0370\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.5626 - accuracy: 0.0430\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.4398 - accuracy: 0.0420\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3219 - accuracy: 0.0460\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.2193 - accuracy: 0.0520\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.1429 - accuracy: 0.0490\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0607 - accuracy: 0.0550\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9893 - accuracy: 0.0590\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.9323 - accuracy: 0.0570\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8737 - accuracy: 0.0580\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.8222 - accuracy: 0.0730\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7708 - accuracy: 0.0710\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7231 - accuracy: 0.0700\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6939 - accuracy: 0.0710\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6492 - accuracy: 0.0620\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6034 - accuracy: 0.0850\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5712 - accuracy: 0.0830\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5335 - accuracy: 0.0740\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5028 - accuracy: 0.0830\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4616 - accuracy: 0.0840\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4495 - accuracy: 0.0710\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.4151 - accuracy: 0.0940\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3877 - accuracy: 0.0850\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3557 - accuracy: 0.0920\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3381 - accuracy: 0.0970\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.3222 - accuracy: 0.0910\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2986 - accuracy: 0.0830\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2657 - accuracy: 0.1100\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2400 - accuracy: 0.0940\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.2250 - accuracy: 0.1040\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1925 - accuracy: 0.1030\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1860 - accuracy: 0.1130\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1837 - accuracy: 0.1090\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1597 - accuracy: 0.1050\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1427 - accuracy: 0.1190\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1253 - accuracy: 0.1180\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0977 - accuracy: 0.1290\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0922 - accuracy: 0.1060\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0782 - accuracy: 0.1040\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0632 - accuracy: 0.1090\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0338 - accuracy: 0.1150\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0262 - accuracy: 0.1290\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0084 - accuracy: 0.1260\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9839 - accuracy: 0.1310\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9769 - accuracy: 0.1300\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9721 - accuracy: 0.1320\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9582 - accuracy: 0.1290\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9485 - accuracy: 0.1240\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9217 - accuracy: 0.1400\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9304 - accuracy: 0.1390\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9153 - accuracy: 0.1320\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8860 - accuracy: 0.1260\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8769 - accuracy: 0.1540\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8637 - accuracy: 0.1360\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8728 - accuracy: 0.1310\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8452 - accuracy: 0.1540\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8392 - accuracy: 0.1370\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8206 - accuracy: 0.1340\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8223 - accuracy: 0.1340\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8093 - accuracy: 0.1510\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.8062 - accuracy: 0.1530\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7842 - accuracy: 0.1410\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.7775 - accuracy: 0.1470\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7675 - accuracy: 0.1400\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7642 - accuracy: 0.1570\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7442 - accuracy: 0.1520\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.7495 - accuracy: 0.1480\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7149 - accuracy: 0.1690\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7080 - accuracy: 0.1590\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7113 - accuracy: 0.1610\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7137 - accuracy: 0.1510\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.6770 - accuracy: 0.1600\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6945 - accuracy: 0.1560\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6757 - accuracy: 0.1620\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.6672 - accuracy: 0.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6479 - accuracy: 0.1750\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6416 - accuracy: 0.1720\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6441 - accuracy: 0.1720\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6246 - accuracy: 0.1760\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6206 - accuracy: 0.1710\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6132 - accuracy: 0.1570\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5951 - accuracy: 0.1650\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.6010 - accuracy: 0.1720\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5871 - accuracy: 0.1550\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5828 - accuracy: 0.1860\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5490 - accuracy: 0.1980\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5651 - accuracy: 0.1830\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5657 - accuracy: 0.1740\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5561 - accuracy: 0.1650\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5572 - accuracy: 0.1760\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.5448 - accuracy: 0.1700\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5166 - accuracy: 0.1790\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5112 - accuracy: 0.1830\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5101 - accuracy: 0.1870\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5137 - accuracy: 0.1840\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5066 - accuracy: 0.1780\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5092 - accuracy: 0.1810\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4970 - accuracy: 0.1870\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4924 - accuracy: 0.1920\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4998 - accuracy: 0.1690\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4725 - accuracy: 0.1910\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4561 - accuracy: 0.1840\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4561 - accuracy: 0.1880\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4487 - accuracy: 0.1960\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4389 - accuracy: 0.2050\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4358 - accuracy: 0.2150\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4408 - accuracy: 0.2000\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4354 - accuracy: 0.1780\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4178 - accuracy: 0.1960\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4148 - accuracy: 0.2000\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4062 - accuracy: 0.1910\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4053 - accuracy: 0.1990\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4022 - accuracy: 0.1860\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3998 - accuracy: 0.2230\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4006 - accuracy: 0.1820\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3755 - accuracy: 0.2060\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3714 - accuracy: 0.2060\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3731 - accuracy: 0.2160\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3824 - accuracy: 0.2100\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3633 - accuracy: 0.2050\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3595 - accuracy: 0.2060\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3587 - accuracy: 0.2040\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3362 - accuracy: 0.2230\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3350 - accuracy: 0.2170\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3423 - accuracy: 0.2070\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3302 - accuracy: 0.2110\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3197 - accuracy: 0.2130\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3188 - accuracy: 0.2220\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3096 - accuracy: 0.2130\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3181 - accuracy: 0.2190\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3090 - accuracy: 0.2160\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3108 - accuracy: 0.2010\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2925 - accuracy: 0.2270\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3047 - accuracy: 0.2150\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2689 - accuracy: 0.2220\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2773 - accuracy: 0.2260\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2796 - accuracy: 0.2230\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2685 - accuracy: 0.2330\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2668 - accuracy: 0.2110\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2679 - accuracy: 0.2210\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2515 - accuracy: 0.2340\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2532 - accuracy: 0.2370\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2402 - accuracy: 0.2290\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2396 - accuracy: 0.2400\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2453 - accuracy: 0.2330\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2346 - accuracy: 0.2360\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2490 - accuracy: 0.2240\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2375 - accuracy: 0.2400\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2259 - accuracy: 0.2410\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2249 - accuracy: 0.2400\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2100 - accuracy: 0.2370\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2170 - accuracy: 0.2290\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2219 - accuracy: 0.2370\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2048 - accuracy: 0.2400\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2134 - accuracy: 0.2470\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1996 - accuracy: 0.2610\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1965 - accuracy: 0.2520\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1763 - accuracy: 0.2460\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1987 - accuracy: 0.2220\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1961 - accuracy: 0.2290\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1734 - accuracy: 0.2510\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1704 - accuracy: 0.2330\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1629 - accuracy: 0.2640\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1635 - accuracy: 0.2370\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1609 - accuracy: 0.2500\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1520 - accuracy: 0.2520\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1760 - accuracy: 0.2380\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1628 - accuracy: 0.2270\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1381 - accuracy: 0.2530\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1286 - accuracy: 0.2540\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1376 - accuracy: 0.2550\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1379 - accuracy: 0.2530\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1352 - accuracy: 0.2330\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1374 - accuracy: 0.2640\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1390 - accuracy: 0.2640\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1299 - accuracy: 0.2650\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1370 - accuracy: 0.2410\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1266 - accuracy: 0.2510\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1031 - accuracy: 0.2570\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1213 - accuracy: 0.2440\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1126 - accuracy: 0.2610\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1071 - accuracy: 0.2570\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1197 - accuracy: 0.2540\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0832 - accuracy: 0.2660\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1024 - accuracy: 0.2550\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0718 - accuracy: 0.2640\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0983 - accuracy: 0.2470\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0782 - accuracy: 0.2740\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0749 - accuracy: 0.2620\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0700 - accuracy: 0.2800\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0805 - accuracy: 0.2510\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0680 - accuracy: 0.2580\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0757 - accuracy: 0.2430\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 972us/step\n",
      "===============================================\n",
      "Training Classifer Portion of Type Model: Done!\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------#\n",
      " Get Training Error(s)\n",
      "#--------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:39<00:00,  3.57it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "#----------------#\n",
      " Get Test Error(s)\n",
      "#----------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:53<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------------#\n",
      " Get Testing Error(s): END\n",
      "#------------------------#\n",
      "                                         DNM  MC-Oracle\n",
      "W1-95L                              0.016596   0.000000\n",
      "W1                                  0.017881   0.000000\n",
      "W1-95R                              0.019078   0.000000\n",
      "M-95L                               0.123094   0.120491\n",
      "M                                   0.126680   0.126680\n",
      "M-95R                               0.132815   0.131784\n",
      "N_Par                          191550.000000   0.000000\n",
      "Train_Time                        463.745874  68.616328\n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000\n",
      "------------------------------------\n",
      "Done: Running script for main model!\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------\")\n",
    "print(\"Running script for main model!\")\n",
    "print(\"------------------------------\")\n",
    "# %run Universal_Measure_Valued_Networks_Backend.ipynb\n",
    "exec(open('Universal_Measure_Valued_Networks_Backend.py').read())\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Done: Running script for main model!\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Run: All Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) *Pointmass Benchmark(s)*\n",
    "These benchmarks consist of subsets of $C(\\mathbb{R}^d,\\mathbb{R})$ which we lift to models in $C(\\mathbb{R}^d,\\cap_{1\\leq q<\\infty}\\mathscr{P}_{q}(\\mathbb{R}))$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to f(x) \\to \\delta_{f(x)}\\in \\cap_{1\\leq q<\\infty}\\mathcal{P}_{q}(\\mathbb{R}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "--------------\n",
      "Training: ENET\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 383/1000 [00:00<00:00, 3821.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Training: ENET - Done\n",
      "---------------------\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3713.29it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3141.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET KRidge  \\\n",
      "W1-95L                              0.016596   0.000000            -      -   \n",
      "W1                                  0.017881   0.000000            -      -   \n",
      "W1-95R                              0.019078   0.000000            -      -   \n",
      "M-95L                               0.123094   0.120491      1130.45    NaN   \n",
      "M                                   0.126680   0.126680      1143.13    NaN   \n",
      "M-95R                               0.132815   0.131784      1157.04    NaN   \n",
      "N_Par                          191550.000000   0.000000         2000    NaN   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09    NaN   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06    NaN   \n",
      "\n",
      "                              GBRF  DNN  \n",
      "W1-95L                           -    -  \n",
      "W1                               -    -  \n",
      "W1-95R                           -    -  \n",
      "M-95L                          NaN  NaN  \n",
      "M                              NaN  NaN  \n",
      "M-95R                          NaN  NaN  \n",
      "N_Par                          NaN  NaN  \n",
      "Train_Time                     NaN  NaN  \n",
      "Test_Time/MC-Oracle_Test_Time  NaN  NaN  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "-----------------\n",
      "Training: K-Ridge\n",
      "-----------------\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.9s remaining:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.9s finished\n",
      " 18%|█▊        | 179/1000 [00:00<00:00, 1786.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3052.19it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3597.94it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge GBRF  DNN  \n",
      "W1-95L                                  -    -    -  \n",
      "W1                                      -    -    -  \n",
      "W1-95R                                  -    -    -  \n",
      "M-95L                             1125.56  NaN  NaN  \n",
      "M                                 1140.79  NaN  NaN  \n",
      "M-95R                             1156.01  NaN  NaN  \n",
      "N_Par                                   0  NaN  NaN  \n",
      "Train_Time                        1.60862  NaN  NaN  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  NaN  NaN  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "--------------\n",
      "Training: GBRF\n",
      "--------------\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.8s finished\n",
      " 38%|███▊      | 377/1000 [00:00<00:00, 3765.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3619.13it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3527.13it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge         GBRF  DNN  \n",
      "W1-95L                                  -            -    -  \n",
      "W1                                      -            -    -  \n",
      "W1-95R                                  -            -    -  \n",
      "M-95L                             1125.56      1185.77  NaN  \n",
      "M                                 1140.79      1206.84  NaN  \n",
      "M-95R                             1156.01      1227.92  NaN  \n",
      "N_Par                                   0       410324  NaN  \n",
      "Train_Time                        1.60862      2.15968  NaN  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  NaN  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "-------------\n",
      "Training: DNN\n",
      "-------------\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   16.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.5616 - mse: 1227907.8750 - mae: 1055.5616 - mape: 99.9971\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.5457 - mse: 1227875.0000 - mae: 1055.5457 - mape: 99.9954\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.5297 - mse: 1227841.3750 - mae: 1055.5297 - mape: 99.9936\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.5129 - mse: 1227806.7500 - mae: 1055.5129 - mape: 99.9918\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.4961 - mse: 1227771.0000 - mae: 1055.4961 - mape: 99.9899\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.4784 - mse: 1227734.0000 - mae: 1055.4784 - mape: 99.9880\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.4597 - mse: 1227695.2500 - mae: 1055.4597 - mape: 99.9860\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.4403 - mse: 1227654.7500 - mae: 1055.4403 - mape: 99.9838\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.4198 - mse: 1227611.7500 - mae: 1055.4198 - mape: 99.9816\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.3979 - mse: 1227566.8750 - mae: 1055.3979 - mape: 99.9792\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.3752 - mse: 1227518.8750 - mae: 1055.3752 - mape: 99.9767\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.3507 - mse: 1227468.0000 - mae: 1055.3507 - mape: 99.9740\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.3250 - mse: 1227414.5000 - mae: 1055.3250 - mape: 99.9712\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.2976 - mse: 1227357.5000 - mae: 1055.2976 - mape: 99.9682\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.2687 - mse: 1227296.8750 - mae: 1055.2687 - mape: 99.9651\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.2380 - mse: 1227232.8750 - mae: 1055.2380 - mape: 99.9617\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.2053 - mse: 1227165.0000 - mae: 1055.2053 - mape: 99.9582\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1055.1710 - mse: 1227092.6250 - mae: 1055.1710 - mape: 99.9544\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.1348 - mse: 1227017.0000 - mae: 1055.1348 - mape: 99.9505\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.0962 - mse: 1226937.0000 - mae: 1055.0962 - mape: 99.9462\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.0558 - mse: 1226852.3750 - mae: 1055.0558 - mape: 99.9418\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1055.0129 - mse: 1226763.2500 - mae: 1055.0129 - mape: 99.9371\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.9678 - mse: 1226670.0000 - mae: 1054.9678 - mape: 99.9322\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.9208 - mse: 1226570.7500 - mae: 1054.9208 - mape: 99.9271\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.8710 - mse: 1226467.2500 - mae: 1054.8710 - mape: 99.9216\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.8191 - mse: 1226358.5000 - mae: 1054.8191 - mape: 99.9159\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.7644 - mse: 1226244.7500 - mae: 1054.7644 - mape: 99.9100\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.7073 - mse: 1226125.5000 - mae: 1054.7073 - mape: 99.9037\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.6475 - mse: 1226001.0000 - mae: 1054.6475 - mape: 99.8972\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.5848 - mse: 1225870.3750 - mae: 1054.5848 - mape: 99.8904\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.5199 - mse: 1225734.5000 - mae: 1054.5199 - mape: 99.8832\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.4519 - mse: 1225592.7500 - mae: 1054.4519 - mape: 99.8758\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.3809 - mse: 1225445.1250 - mae: 1054.3809 - mape: 99.8681\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.3074 - mse: 1225291.2500 - mae: 1054.3074 - mape: 99.8600\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.2306 - mse: 1225131.6250 - mae: 1054.2306 - mape: 99.8517\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.1514 - mse: 1224965.7500 - mae: 1054.1514 - mape: 99.8430\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.0690 - mse: 1224793.6250 - mae: 1054.0690 - mape: 99.8340\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.9833 - mse: 1224615.5000 - mae: 1053.9833 - mape: 99.8247\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.8948 - mse: 1224430.8750 - mae: 1053.8948 - mape: 99.8150\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.8030 - mse: 1224240.0000 - mae: 1053.8030 - mape: 99.8050\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.7085 - mse: 1224042.3750 - mae: 1053.7085 - mape: 99.7946\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.6106 - mse: 1223838.5000 - mae: 1053.6106 - mape: 99.7839\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.5095 - mse: 1223627.8750 - mae: 1053.5095 - mape: 99.7729\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.4054 - mse: 1223410.8750 - mae: 1053.4054 - mape: 99.7615\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.2979 - mse: 1223186.3750 - mae: 1053.2979 - mape: 99.7498\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.1869 - mse: 1222955.6250 - mae: 1053.1869 - mape: 99.7376\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.0732 - mse: 1222718.1250 - mae: 1053.0732 - mape: 99.7252\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.9556 - mse: 1222473.7500 - mae: 1052.9556 - mape: 99.7124\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.8350 - mse: 1222222.6250 - mae: 1052.8350 - mape: 99.6992\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.7106 - mse: 1221963.8750 - mae: 1052.7106 - mape: 99.6856\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.5833 - mse: 1221698.3750 - mae: 1052.5833 - mape: 99.6717\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.4524 - mse: 1221425.3750 - mae: 1052.4524 - mape: 99.6574\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.3177 - mse: 1221145.8750 - mae: 1052.3177 - mape: 99.6427\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.1799 - mse: 1220859.1250 - mae: 1052.1799 - mape: 99.6276\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.0382 - mse: 1220564.3750 - mae: 1052.0382 - mape: 99.6121\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1051.8931 - mse: 1220262.3750 - mae: 1051.8931 - mape: 99.5962\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1051.7445 - mse: 1219953.5000 - mae: 1051.7445 - mape: 99.5800\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1051.5924 - mse: 1219636.8750 - mae: 1051.5924 - mape: 99.5634\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1051.4362 - mse: 1219312.8750 - mae: 1051.4362 - mape: 99.5463\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1051.2770 - mse: 1218981.1250 - mae: 1051.2770 - mape: 99.5289\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1051.1135 - mse: 1218640.7500 - mae: 1051.1135 - mape: 99.5110\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.9463 - mse: 1218294.0000 - mae: 1050.9463 - mape: 99.4928\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.7754 - mse: 1217938.8750 - mae: 1050.7754 - mape: 99.4740\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.6006 - mse: 1217575.5000 - mae: 1050.6006 - mape: 99.4550\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.4221 - mse: 1217205.1250 - mae: 1050.4221 - mape: 99.4354\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.2396 - mse: 1216826.0000 - mae: 1050.2396 - mape: 99.4155\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1050.0532 - mse: 1216439.3750 - mae: 1050.0532 - mape: 99.3951\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1049.8627 - mse: 1216044.5000 - mae: 1049.8627 - mape: 99.3742\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1049.6685 - mse: 1215641.3750 - mae: 1049.6685 - mape: 99.3529\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1049.4703 - mse: 1215229.8750 - mae: 1049.4703 - mape: 99.3313\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1049.2677 - mse: 1214809.6250 - mae: 1049.2677 - mape: 99.3092\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1049.0613 - mse: 1214381.6250 - mae: 1049.0613 - mape: 99.2866\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1048.8508 - mse: 1213945.1250 - mae: 1048.8508 - mape: 99.2636\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1048.6361 - mse: 1213499.3750 - mae: 1048.6361 - mape: 99.2401\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1048.4174 - mse: 1213045.0000 - mae: 1048.4174 - mape: 99.2162\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1048.1940 - mse: 1212582.5000 - mae: 1048.1940 - mape: 99.1919\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1047.9667 - mse: 1212111.0000 - mae: 1047.9667 - mape: 99.1670\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1047.7355 - mse: 1211630.7500 - mae: 1047.7355 - mape: 99.1418\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1047.4990 - mse: 1211141.7500 - mae: 1047.4990 - mape: 99.1159\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1047.2590 - mse: 1210644.6250 - mae: 1047.2590 - mape: 99.0897\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1047.0144 - mse: 1210138.5000 - mae: 1047.0144 - mape: 99.0629\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1046.7655 - mse: 1209622.3750 - mae: 1046.7655 - mape: 99.0358\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1046.5122 - mse: 1209098.0000 - mae: 1046.5122 - mape: 99.0081\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1046.2542 - mse: 1208564.7500 - mae: 1046.2542 - mape: 98.9799\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1045.9921 - mse: 1208022.2500 - mae: 1045.9921 - mape: 98.9512\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1045.7247 - mse: 1207470.7500 - mae: 1045.7247 - mape: 98.9220\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1045.4534 - mse: 1206909.6250 - mae: 1045.4534 - mape: 98.8922\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1045.1772 - mse: 1206339.0000 - mae: 1045.1772 - mape: 98.8620\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1044.8966 - mse: 1205758.1250 - mae: 1044.8966 - mape: 98.8314\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1044.6112 - mse: 1205169.0000 - mae: 1044.6112 - mape: 98.8002\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1044.3209 - mse: 1204570.1250 - mae: 1044.3209 - mape: 98.7685\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1044.0261 - mse: 1203961.3750 - mae: 1044.0261 - mape: 98.7361\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1043.7264 - mse: 1203343.0000 - mae: 1043.7264 - mape: 98.7034\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1043.4220 - mse: 1202716.5000 - mae: 1043.4220 - mape: 98.6700\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1043.1130 - mse: 1202079.3750 - mae: 1043.1130 - mape: 98.6362\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1042.7987 - mse: 1201432.1250 - mae: 1042.7987 - mape: 98.6018\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1042.4799 - mse: 1200775.6250 - mae: 1042.4799 - mape: 98.5670\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1042.1562 - mse: 1200108.7500 - mae: 1042.1562 - mape: 98.5316\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1041.8276 - mse: 1199432.6250 - mae: 1041.8276 - mape: 98.4956\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1041.4937 - mse: 1198746.7500 - mae: 1041.4937 - mape: 98.4590\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1041.1555 - mse: 1198051.6250 - mae: 1041.1555 - mape: 98.4219\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1040.8119 - mse: 1197344.7500 - mae: 1040.8119 - mape: 98.3844\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1040.4631 - mse: 1196628.8750 - mae: 1040.4631 - mape: 98.3461\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1040.1097 - mse: 1195901.5000 - mae: 1040.1097 - mape: 98.3075\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1039.7513 - mse: 1195165.1250 - mae: 1039.7513 - mape: 98.2684\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1039.3872 - mse: 1194418.2500 - mae: 1039.3872 - mape: 98.2285\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1039.0187 - mse: 1193661.1250 - mae: 1039.0187 - mape: 98.1881\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1038.6449 - mse: 1192893.5000 - mae: 1038.6449 - mape: 98.1472\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1038.2660 - mse: 1192115.0000 - mae: 1038.2660 - mape: 98.1058\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1037.8817 - mse: 1191326.5000 - mae: 1037.8817 - mape: 98.0639\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1037.4924 - mse: 1190526.5000 - mae: 1037.4924 - mape: 98.0214\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1037.0979 - mse: 1189717.8750 - mae: 1037.0979 - mape: 97.9782\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1036.6984 - mse: 1188898.8750 - mae: 1036.6984 - mape: 97.9345\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1036.2931 - mse: 1188067.1250 - mae: 1036.2931 - mape: 97.8902\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1035.8826 - mse: 1187227.3750 - mae: 1035.8826 - mape: 97.8453\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1035.4672 - mse: 1186375.3750 - mae: 1035.4672 - mape: 97.8000\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1035.0460 - mse: 1185513.8750 - mae: 1035.0460 - mape: 97.7539\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1034.6195 - mse: 1184641.1250 - mae: 1034.6195 - mape: 97.7073\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1034.1879 - mse: 1183760.5000 - mae: 1034.1879 - mape: 97.6600\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1033.7505 - mse: 1182866.7500 - mae: 1033.7505 - mape: 97.6121\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1033.3075 - mse: 1181963.1250 - mae: 1033.3075 - mape: 97.5637\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1032.8593 - mse: 1181046.8750 - mae: 1032.8593 - mape: 97.5147\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1032.4055 - mse: 1180119.8750 - mae: 1032.4055 - mape: 97.4652\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1031.9463 - mse: 1179181.6250 - mae: 1031.9463 - mape: 97.4150\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1031.4821 - mse: 1178232.5000 - mae: 1031.4821 - mape: 97.3644\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1031.0111 - mse: 1177274.8750 - mae: 1031.0111 - mape: 97.3128\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1030.5355 - mse: 1176304.8750 - mae: 1030.5355 - mape: 97.2609\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1030.0538 - mse: 1175323.8750 - mae: 1030.0538 - mape: 97.2082\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1029.5670 - mse: 1174333.5000 - mae: 1029.5670 - mape: 97.1549\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1029.0743 - mse: 1173329.6250 - mae: 1029.0743 - mape: 97.1012\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1028.5754 - mse: 1172314.6250 - mae: 1028.5754 - mape: 97.0467\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1028.0717 - mse: 1171289.3750 - mae: 1028.0717 - mape: 96.9916\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1027.5619 - mse: 1170252.0000 - mae: 1027.5619 - mape: 96.9359\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1027.0464 - mse: 1169204.5000 - mae: 1027.0464 - mape: 96.8798\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1026.5251 - mse: 1168145.7500 - mae: 1026.5251 - mape: 96.8228\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1025.9984 - mse: 1167076.2500 - mae: 1025.9984 - mape: 96.7652\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1025.4657 - mse: 1165997.7500 - mae: 1025.4657 - mape: 96.7068\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1024.9270 - mse: 1164906.8750 - mae: 1024.9270 - mape: 96.6481\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1024.3828 - mse: 1163803.7500 - mae: 1024.3828 - mape: 96.5884\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1023.8329 - mse: 1162690.0000 - mae: 1023.8329 - mape: 96.5283\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1023.2769 - mse: 1161566.1250 - mae: 1023.2769 - mape: 96.4677\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1022.7151 - mse: 1160430.1250 - mae: 1022.7151 - mape: 96.4062\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1022.1479 - mse: 1159282.0000 - mae: 1022.1479 - mape: 96.3442\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1021.5746 - mse: 1158124.0000 - mae: 1021.5746 - mape: 96.2816\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1020.9957 - mse: 1156952.2500 - mae: 1020.9957 - mape: 96.2185\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1020.4108 - mse: 1155769.1250 - mae: 1020.4108 - mape: 96.1546\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1019.8198 - mse: 1154576.3750 - mae: 1019.8198 - mape: 96.0902\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1019.2236 - mse: 1153372.3750 - mae: 1019.2236 - mape: 96.0253\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1018.6219 - mse: 1152158.3750 - mae: 1018.6219 - mape: 95.9595\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1018.0140 - mse: 1150934.3750 - mae: 1018.0140 - mape: 95.8930\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1017.3992 - mse: 1149697.7500 - mae: 1017.3992 - mape: 95.8258\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1016.7798 - mse: 1148449.1250 - mae: 1016.7798 - mape: 95.7583\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1016.1545 - mse: 1147189.3750 - mae: 1016.1545 - mape: 95.6901\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1015.5231 - mse: 1145919.6250 - mae: 1015.5231 - mape: 95.6214\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1014.8866 - mse: 1144640.6250 - mae: 1014.8866 - mape: 95.5518\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1014.2435 - mse: 1143348.5000 - mae: 1014.2435 - mape: 95.4816\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1013.5950 - mse: 1142043.8750 - mae: 1013.5950 - mape: 95.4112\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1012.9406 - mse: 1140731.1250 - mae: 1012.9406 - mape: 95.3396\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1012.2805 - mse: 1139409.0000 - mae: 1012.2805 - mape: 95.2677\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1011.6149 - mse: 1138076.1250 - mae: 1011.6149 - mape: 95.1949\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1010.9435 - mse: 1136731.2500 - mae: 1010.9435 - mape: 95.1217\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1010.2666 - mse: 1135374.5000 - mae: 1010.2666 - mape: 95.0480\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1009.5839 - mse: 1134010.7500 - mae: 1009.5839 - mape: 94.9735\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1008.8959 - mse: 1132632.1250 - mae: 1008.8959 - mape: 94.8986\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1008.2018 - mse: 1131244.5000 - mae: 1008.2018 - mape: 94.8230\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1007.5023 - mse: 1129848.3750 - mae: 1007.5023 - mape: 94.7468\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1006.7967 - mse: 1128438.6250 - mae: 1006.7967 - mape: 94.6699\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1006.0853 - mse: 1127020.7500 - mae: 1006.0853 - mape: 94.5924\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1005.3691 - mse: 1125591.1250 - mae: 1005.3691 - mape: 94.5142\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1004.6464 - mse: 1124152.2500 - mae: 1004.6464 - mape: 94.4355\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1003.9183 - mse: 1122701.8750 - mae: 1003.9183 - mape: 94.3562\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1003.1845 - mse: 1121239.6250 - mae: 1003.1845 - mape: 94.2765\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1002.4454 - mse: 1119770.5000 - mae: 1002.4454 - mape: 94.1960\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1001.7001 - mse: 1118288.7500 - mae: 1001.7001 - mape: 94.1148\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1000.9498 - mse: 1116796.8750 - mae: 1000.9498 - mape: 94.0333\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1000.1937 - mse: 1115298.8750 - mae: 1000.1937 - mape: 93.9507\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 999.4321 - mse: 1113789.8750 - mae: 999.4321 - mape: 93.8678\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 998.6644 - mse: 1112272.3750 - mae: 998.6644 - mape: 93.7839\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 997.8913 - mse: 1110742.6250 - mae: 997.8913 - mape: 93.6996\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 997.1127 - mse: 1109201.1250 - mae: 997.1127 - mape: 93.6151\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 996.3277 - mse: 1107649.1250 - mae: 996.3277 - mape: 93.5295\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 995.5375 - mse: 1106090.7500 - mae: 995.5375 - mape: 93.4434\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 994.7426 - mse: 1104521.5000 - mae: 994.7426 - mape: 93.3570\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 993.9409 - mse: 1102938.3750 - mae: 993.9409 - mape: 93.2697\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 993.1344 - mse: 1101344.6250 - mae: 993.1344 - mape: 93.1824\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 992.3218 - mse: 1099744.7500 - mae: 992.3218 - mape: 93.0937\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 991.5034 - mse: 1098131.6250 - mae: 991.5034 - mape: 93.0048\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 990.6797 - mse: 1096512.5000 - mae: 990.6797 - mape: 92.9152\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 989.8508 - mse: 1094882.7500 - mae: 989.8508 - mape: 92.8248\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 989.0151 - mse: 1093243.8750 - mae: 989.0151 - mape: 92.7339\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 988.1748 - mse: 1091595.5000 - mae: 988.1748 - mape: 92.6426\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 987.3278 - mse: 1089937.1250 - mae: 987.3278 - mape: 92.5503\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 986.4756 - mse: 1088266.7500 - mae: 986.4756 - mape: 92.4575\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 985.6177 - mse: 1086587.1250 - mae: 985.6177 - mape: 92.3644\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 984.7538 - mse: 1084897.8750 - mae: 984.7538 - mape: 92.2705\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 983.8840 - mse: 1083199.5000 - mae: 983.8840 - mape: 92.1758\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 983.0088 - mse: 1081491.5000 - mae: 983.0088 - mape: 92.0805\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 982.1277 - mse: 1079776.3750 - mae: 982.1277 - mape: 91.9845\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 981.2414 - mse: 1078051.5000 - mae: 981.2414 - mape: 91.8879\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 980.3486 - mse: 1076313.5000 - mae: 980.3486 - mape: 91.7908\n",
      "63/63 [==============================] - 0s 687us/step\n",
      "13/13 [==============================] - 0s 790us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2828.72it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 3185.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge         GBRF         DNN  \n",
      "W1-95L                                  -            -           -  \n",
      "W1                                      -            -           -  \n",
      "W1-95R                                  -            -           -  \n",
      "M-95L                             1125.56      1185.77     74.3589  \n",
      "M                                 1140.79      1206.84     74.6431  \n",
      "M-95R                             1156.01      1227.92     74.7857  \n",
      "N_Par                                   0       410324       41001  \n",
      "Train_Time                        1.60862      2.15968     36.7239  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exec(open('CV_Grid.py').read())\n",
    "# Notebook Mode:\n",
    "# %run Evaluation.ipynb\n",
    "# %run Benchmarks_Model_Builder_Pointmass_Based.ipynb\n",
    "# Terminal Mode (Default):\n",
    "exec(open('Evaluation.py').read())\n",
    "exec(open('Benchmarks_Model_Builder_Pointmass_Based.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Point-Mass Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge         GBRF         DNN  \n",
      "W1-95L                                  -            -           -  \n",
      "W1                                      -            -           -  \n",
      "W1-95R                                  -            -           -  \n",
      "M-95L                             1125.56      1185.77     74.3589  \n",
      "M                                 1140.79      1206.84     74.6431  \n",
      "M-95R                             1156.01      1227.92     74.7857  \n",
      "N_Par                                   0       410324       41001  \n",
      "Train_Time                        1.60862      2.15968     36.7239  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>1130.45</td>\n",
       "      <td>1125.56</td>\n",
       "      <td>1185.77</td>\n",
       "      <td>74.3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.126680</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>1143.13</td>\n",
       "      <td>1140.79</td>\n",
       "      <td>1206.84</td>\n",
       "      <td>74.6431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.132815</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>1157.04</td>\n",
       "      <td>1156.01</td>\n",
       "      <td>1227.92</td>\n",
       "      <td>74.7857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>191550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>410324</td>\n",
       "      <td>41001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>463.745874</td>\n",
       "      <td>68.616328</td>\n",
       "      <td>1.62012e+09</td>\n",
       "      <td>1.60862</td>\n",
       "      <td>2.15968</td>\n",
       "      <td>36.7239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.004381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.84636e-06</td>\n",
       "      <td>0.00121333</td>\n",
       "      <td>0.000433868</td>\n",
       "      <td>0.00442503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle         ENET  \\\n",
       "W1-95L                              0.016596   0.000000            -   \n",
       "W1                                  0.017881   0.000000            -   \n",
       "W1-95R                              0.019078   0.000000            -   \n",
       "M-95L                               0.123094   0.120491      1130.45   \n",
       "M                                   0.126680   0.126680      1143.13   \n",
       "M-95R                               0.132815   0.131784      1157.04   \n",
       "N_Par                          191550.000000   0.000000         2000   \n",
       "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
       "\n",
       "                                   KRidge         GBRF         DNN  \n",
       "W1-95L                                  -            -           -  \n",
       "W1                                      -            -           -  \n",
       "W1-95R                                  -            -           -  \n",
       "M-95L                             1125.56      1185.77     74.3589  \n",
       "M                                 1140.79      1206.84     74.6431  \n",
       "M-95R                             1156.01      1227.92     74.7857  \n",
       "N_Par                                   0       410324       41001  \n",
       "Train_Time                        1.60862      2.15968     36.7239  \n",
       "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.016596   0.000000  1.293743e+06   \n",
      "W1                                  0.017881   0.000000  1.322752e+06   \n",
      "W1-95R                              0.019078   0.000000  1.357523e+06   \n",
      "M-95L                               0.123094   0.120491  1.130449e+03   \n",
      "M                                   0.126680   0.126680  1.143126e+03   \n",
      "M-95R                               0.132815   0.131784  1.157037e+03   \n",
      "N_Par                          191550.000000   0.000000  2.000000e+03   \n",
      "Train_Time                        463.745874  68.616328  1.620119e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.846358e-06   \n",
      "\n",
      "                                     KRidge          GBRF           DNN  \n",
      "W1-95L                         1.283065e+06  1.426154e+06   5365.460536  \n",
      "W1                             1.317500e+06  1.473880e+06   5413.228493  \n",
      "W1-95R                         1.369838e+06  1.518482e+06   5472.563684  \n",
      "M-95L                          1.125560e+03  1.185770e+03     73.012146  \n",
      "M                              1.140791e+03  1.206844e+03     73.519282  \n",
      "M-95R                          1.156006e+03  1.227915e+03     73.947685  \n",
      "N_Par                          0.000000e+00  4.103240e+05  41001.000000  \n",
      "Train_Time                     1.608621e+00  2.159679e+00     36.723889  \n",
      "Test_Time/MC-Oracle_Test_Time  1.213329e-03  4.338676e-04      0.004425  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.293743e+06</td>\n",
       "      <td>1.283065e+06</td>\n",
       "      <td>1.426154e+06</td>\n",
       "      <td>5365.460536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.322752e+06</td>\n",
       "      <td>1.317500e+06</td>\n",
       "      <td>1.473880e+06</td>\n",
       "      <td>5413.228493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357523e+06</td>\n",
       "      <td>1.369838e+06</td>\n",
       "      <td>1.518482e+06</td>\n",
       "      <td>5472.563684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>1.130449e+03</td>\n",
       "      <td>1.125560e+03</td>\n",
       "      <td>1.185770e+03</td>\n",
       "      <td>73.012146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.126680</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>1.143126e+03</td>\n",
       "      <td>1.140791e+03</td>\n",
       "      <td>1.206844e+03</td>\n",
       "      <td>73.519282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.132815</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>1.157037e+03</td>\n",
       "      <td>1.156006e+03</td>\n",
       "      <td>1.227915e+03</td>\n",
       "      <td>73.947685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>191550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.103240e+05</td>\n",
       "      <td>41001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>463.745874</td>\n",
       "      <td>68.616328</td>\n",
       "      <td>1.620119e+09</td>\n",
       "      <td>1.608621e+00</td>\n",
       "      <td>2.159679e+00</td>\n",
       "      <td>36.723889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.004381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.846358e-06</td>\n",
       "      <td>1.213329e-03</td>\n",
       "      <td>4.338676e-04</td>\n",
       "      <td>0.004425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                              0.016596   0.000000  1.293743e+06   \n",
       "W1                                  0.017881   0.000000  1.322752e+06   \n",
       "W1-95R                              0.019078   0.000000  1.357523e+06   \n",
       "M-95L                               0.123094   0.120491  1.130449e+03   \n",
       "M                                   0.126680   0.126680  1.143126e+03   \n",
       "M-95R                               0.132815   0.131784  1.157037e+03   \n",
       "N_Par                          191550.000000   0.000000  2.000000e+03   \n",
       "Train_Time                        463.745874  68.616328  1.620119e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.846358e-06   \n",
       "\n",
       "                                     KRidge          GBRF           DNN  \n",
       "W1-95L                         1.283065e+06  1.426154e+06   5365.460536  \n",
       "W1                             1.317500e+06  1.473880e+06   5413.228493  \n",
       "W1-95R                         1.369838e+06  1.518482e+06   5472.563684  \n",
       "M-95L                          1.125560e+03  1.185770e+03     73.012146  \n",
       "M                              1.140791e+03  1.206844e+03     73.519282  \n",
       "M-95R                          1.156006e+03  1.227915e+03     73.947685  \n",
       "N_Par                          0.000000e+00  4.103240e+05  41001.000000  \n",
       "Train_Time                     1.608621e+00  2.159679e+00     36.723889  \n",
       "Test_Time/MC-Oracle_Test_Time  1.213329e-03  4.338676e-04      0.004425  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) *Gaussian Benchmarks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bencharm 1: [Gaussian Process Regressor](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "- Benchmark 2: Deep Gaussian Networks:\n",
    "These models train models which assume Gaussianity.  We may view these as models in $\\mathcal{P}_2(\\mathbb{R})$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to (\\hat{\\mu}(x),\\hat{\\Sigma}(x)\\hat{\\Sigma}^{\\top})\\triangleq f(x) \\in \\mathbb{R}\\times [0,\\infty) \\to \n",
    "(2\\pi)^{-\\frac{d}{2}}\\det(\\hat{\\Sigma}(x))^{-\\frac{1}{2}} \\, e^{ -\\frac{1}{2}(\\cdot - \\hat{\\mu}(x))^{{{\\!\\mathsf{T}}}} \\hat{\\Sigma}(x)^{-1}(\\cdot - \\hat{\\mu}(x)) } \\mu \\in \\mathcal{G}_d\\subset \\mathcal{P}_2(\\mathbb{R});\n",
    "$$\n",
    "where $\\mathcal{G}_1$ is the set of Gaussian measures on $\\mathbb{R}$ equipped with the relative Wasserstein-1 topology.\n",
    "\n",
    "Examples of this type of architecture are especially prevalent in uncertainty quantification; see ([Deep Ensembles](https://arxiv.org/abs/1612.01474)] or [NOMU: Neural Optimization-based Model Uncertainty](https://arxiv.org/abs/2102.13640).  Moreover, their universality in $C(\\mathbb{R}^d,\\mathcal{G}_2)$ is known, and has been shown in [Corollary 4.7](https://arxiv.org/abs/2101.05390)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.6s remaining:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   31.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   31.4s finished\n",
      "  0%|          | 1/1000 [00:00<02:20,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering Parameters for Deep Gaussian Network to train on!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:34<00:00, 28.78it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Getting Parameters for Deep Gaussian Network!\n",
      "===============================\n",
      "Training Deep Gaussian Network!\n",
      "===============================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   23.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0281 - mse: 1.1188 - mae: 1.0281 - mape: 23334634.0000\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0279 - mse: 1.1168 - mae: 1.0279 - mape: 23616722.0000\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1155 - mae: 1.0278 - mape: 23833136.0000\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1148 - mae: 1.0278 - mape: 23980334.0000\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1144 - mae: 1.0278 - mape: 24066398.0000\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1141 - mae: 1.0278 - mape: 24107526.0000\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24125076.0000\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24132356.0000\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24134722.0000\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135426.0000\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135528.0000\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135532.0000\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135522.0000\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135268.0000\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135964.0000\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135452.0000\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135862.0000\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135666.0000\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135778.0000\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135342.0000\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24134952.0000\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135834.0000\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135232.0000\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136100.0000\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135360.0000\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135356.0000\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135462.0000\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135630.0000\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135554.0000\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135218.0000\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135500.0000\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135558.0000\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135974.0000\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135018.0000\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135684.0000\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135516.0000\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135432.0000\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135472.0000\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135528.0000\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135454.0000\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135242.0000\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135450.0000\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135516.0000\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135986.0000\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24134888.0000\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136224.0000\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135186.0000\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135258.0000\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135662.0000\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135454.0000\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135686.0000\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135484.0000\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135252.0000\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135592.0000\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135192.0000\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135698.0000\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135822.0000\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135038.0000\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135756.0000\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135618.0000\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135540.0000\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135580.0000\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135268.0000\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135708.0000\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135346.0000\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135850.0000\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135184.0000\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135532.0000\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135926.0000\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135714.0000\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135562.0000\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135698.0000\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135646.0000\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135926.0000\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135798.0000\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1140 - mae: 1.0278 - mape: 24135932.0000\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135944.0000\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135686.0000\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135748.0000\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136098.0000\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135924.0000\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135942.0000\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136000.0000\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135998.0000\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136136.0000\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135800.0000\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136110.0000\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136346.0000\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24135996.0000\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136458.0000\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136128.0000\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136504.0000\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136228.0000\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136518.0000\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136160.0000\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136306.0000\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136464.0000\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136330.0000\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136472.0000\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136654.0000\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136610.0000\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136738.0000\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136976.0000\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136670.0000\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136694.0000\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136920.0000\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136766.0000\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136952.0000\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136748.0000\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136972.0000\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24136854.0000\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137020.0000\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137186.0000\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137062.0000\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137350.0000\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137066.0000\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137358.0000\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137242.0000\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137406.0000\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137248.0000\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137318.0000\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137540.0000\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137486.0000\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137502.0000\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137562.0000\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137518.0000\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137574.0000\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137664.0000\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137580.0000\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137724.0000\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137656.0000\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137678.0000\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137700.0000\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137876.0000\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137768.0000\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137922.0000\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137830.0000\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137906.0000\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137940.0000\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137958.0000\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24137912.0000\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138048.0000\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138012.0000\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138034.0000\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138104.0000\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138024.0000\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138108.0000\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138088.0000\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138180.0000\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138114.0000\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138122.0000\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138188.0000\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138214.0000\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138160.0000\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138212.0000\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138216.0000\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138216.0000\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138320.0000\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138236.0000\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138340.0000\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138232.0000\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138268.0000\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138298.0000\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138254.0000\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138298.0000\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138288.0000\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138342.0000\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138304.0000\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138346.0000\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138298.0000\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138366.0000\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138296.0000\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138346.0000\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138346.0000\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138348.0000\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138348.0000\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138360.0000\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138362.0000\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138392.0000\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138366.0000\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138388.0000\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138358.0000\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138370.0000\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138382.0000\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138380.0000\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138384.0000\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138390.0000\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138416.0000\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138390.0000\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138358.0000\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138420.0000\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138342.0000\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138372.0000\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138402.0000\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138380.0000\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138402.0000\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138422.0000\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138412.0000\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138412.0000\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.0278 - mse: 1.1139 - mae: 1.0278 - mape: 24138350.0000\n",
      "63/63 [==============================] - 0s 520us/step\n",
      "13/13 [==============================] - 0s 659us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Training Deep Gaussian Network!: END\n",
      "====================================\n",
      "#---------------------------------------#\n",
      " Get Training Errors for: Gaussian Models\n",
      "#---------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1277.92it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1352.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "#--------------------------------------#\n",
      " Get Testing Errors for: Gaussian Models\n",
      "#--------------------------------------#\n",
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "-------------------------------------------------\n",
      "Updating Performance Metrics Dataframe and Saved!\n",
      "-------------------------------------------------\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge         GBRF         DNN  \\\n",
      "W1-95L                                  -            -           -   \n",
      "W1                                      -            -           -   \n",
      "W1-95R                                  -            -           -   \n",
      "M-95L                             1125.56      1185.77     74.3589   \n",
      "M                                 1140.79      1206.84     74.6431   \n",
      "M-95R                             1156.01      1227.92     74.7857   \n",
      "N_Par                                   0       410324       41001   \n",
      "Train_Time                        1.60862      2.15968     36.7239   \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         1.285100e+06      2.292542  \n",
      "W1                             1.322710e+06      2.328779  \n",
      "W1-95R                         1.371959e+06      2.359401  \n",
      "M-95L                          1.129208e+03      1.129047  \n",
      "M                              1.143108e+03      1.144267  \n",
      "M-95R                          1.159595e+03      1.158204  \n",
      "N_Par                          0.000000e+00  41001.000000  \n",
      "Train_Time                     1.822400e+02     42.855985  \n",
      "Test_Time/MC-Oracle_Test_Time  6.712238e-03      0.003794  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run Benchmarks_Model_Builder_Mean_Var.ipynb\n",
    "exec(open('Benchmarks_Model_Builder_Mean_Var.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Quality (Updated): Test\n",
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                              0.016596   0.000000  1.293743e+06   \n",
      "W1                                  0.017881   0.000000  1.322752e+06   \n",
      "W1-95R                              0.019078   0.000000  1.357523e+06   \n",
      "M-95L                               0.123094   0.120491  1.130449e+03   \n",
      "M                                   0.126680   0.126680  1.143126e+03   \n",
      "M-95R                               0.132815   0.131784  1.157037e+03   \n",
      "N_Par                          191550.000000   0.000000  2.000000e+03   \n",
      "Train_Time                        463.745874  68.616328  1.620119e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.846358e-06   \n",
      "\n",
      "                                     KRidge          GBRF           DNN  \\\n",
      "W1-95L                         1.283065e+06  1.426154e+06   5365.460536   \n",
      "W1                             1.317500e+06  1.473880e+06   5413.228493   \n",
      "W1-95R                         1.369838e+06  1.518482e+06   5472.563684   \n",
      "M-95L                          1.125560e+03  1.185770e+03     73.012146   \n",
      "M                              1.140791e+03  1.206844e+03     73.519282   \n",
      "M-95R                          1.156006e+03  1.227915e+03     73.947685   \n",
      "N_Par                          0.000000e+00  4.103240e+05  41001.000000   \n",
      "Train_Time                     1.608621e+00  2.159679e+00     36.723889   \n",
      "Test_Time/MC-Oracle_Test_Time  1.213329e-03  4.338676e-04      0.004425   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         1.285100e+06      2.292542  \n",
      "W1                             1.322710e+06      2.328779  \n",
      "W1-95R                         1.371959e+06      2.359401  \n",
      "M-95L                          1.129208e+03      1.129047  \n",
      "M                              1.143108e+03      1.144267  \n",
      "M-95R                          1.159595e+03      1.158204  \n",
      "N_Par                          0.000000e+00  41001.000000  \n",
      "Train_Time                     1.822400e+02     42.855985  \n",
      "Test_Time/MC-Oracle_Test_Time  6.712238e-03      0.003794  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "      <th>GPR</th>\n",
       "      <th>DGN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.293743e+06</td>\n",
       "      <td>1.283065e+06</td>\n",
       "      <td>1.426154e+06</td>\n",
       "      <td>5365.460536</td>\n",
       "      <td>1.285100e+06</td>\n",
       "      <td>2.292542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.322752e+06</td>\n",
       "      <td>1.317500e+06</td>\n",
       "      <td>1.473880e+06</td>\n",
       "      <td>5413.228493</td>\n",
       "      <td>1.322710e+06</td>\n",
       "      <td>2.328779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.357523e+06</td>\n",
       "      <td>1.369838e+06</td>\n",
       "      <td>1.518482e+06</td>\n",
       "      <td>5472.563684</td>\n",
       "      <td>1.371959e+06</td>\n",
       "      <td>2.359401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>1.130449e+03</td>\n",
       "      <td>1.125560e+03</td>\n",
       "      <td>1.185770e+03</td>\n",
       "      <td>73.012146</td>\n",
       "      <td>1.129208e+03</td>\n",
       "      <td>1.129047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.126680</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>1.143126e+03</td>\n",
       "      <td>1.140791e+03</td>\n",
       "      <td>1.206844e+03</td>\n",
       "      <td>73.519282</td>\n",
       "      <td>1.143108e+03</td>\n",
       "      <td>1.144267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.132815</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>1.157037e+03</td>\n",
       "      <td>1.156006e+03</td>\n",
       "      <td>1.227915e+03</td>\n",
       "      <td>73.947685</td>\n",
       "      <td>1.159595e+03</td>\n",
       "      <td>1.158204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>191550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.103240e+05</td>\n",
       "      <td>41001.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>41001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>463.745874</td>\n",
       "      <td>68.616328</td>\n",
       "      <td>1.620119e+09</td>\n",
       "      <td>1.608621e+00</td>\n",
       "      <td>2.159679e+00</td>\n",
       "      <td>36.723889</td>\n",
       "      <td>1.822400e+02</td>\n",
       "      <td>42.855985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.004381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.846358e-06</td>\n",
       "      <td>1.213329e-03</td>\n",
       "      <td>4.338676e-04</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>6.712238e-03</td>\n",
       "      <td>0.003794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                              0.016596   0.000000  1.293743e+06   \n",
       "W1                                  0.017881   0.000000  1.322752e+06   \n",
       "W1-95R                              0.019078   0.000000  1.357523e+06   \n",
       "M-95L                               0.123094   0.120491  1.130449e+03   \n",
       "M                                   0.126680   0.126680  1.143126e+03   \n",
       "M-95R                               0.132815   0.131784  1.157037e+03   \n",
       "N_Par                          191550.000000   0.000000  2.000000e+03   \n",
       "Train_Time                        463.745874  68.616328  1.620119e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.846358e-06   \n",
       "\n",
       "                                     KRidge          GBRF           DNN  \\\n",
       "W1-95L                         1.283065e+06  1.426154e+06   5365.460536   \n",
       "W1                             1.317500e+06  1.473880e+06   5413.228493   \n",
       "W1-95R                         1.369838e+06  1.518482e+06   5472.563684   \n",
       "M-95L                          1.125560e+03  1.185770e+03     73.012146   \n",
       "M                              1.140791e+03  1.206844e+03     73.519282   \n",
       "M-95R                          1.156006e+03  1.227915e+03     73.947685   \n",
       "N_Par                          0.000000e+00  4.103240e+05  41001.000000   \n",
       "Train_Time                     1.608621e+00  2.159679e+00     36.723889   \n",
       "Test_Time/MC-Oracle_Test_Time  1.213329e-03  4.338676e-04      0.004425   \n",
       "\n",
       "                                        GPR           DGN  \n",
       "W1-95L                         1.285100e+06      2.292542  \n",
       "W1                             1.322710e+06      2.328779  \n",
       "W1-95R                         1.371959e+06      2.359401  \n",
       "M-95L                          1.129208e+03      1.129047  \n",
       "M                              1.143108e+03      1.144267  \n",
       "M-95R                          1.159595e+03      1.158204  \n",
       "N_Par                          0.000000e+00  41001.000000  \n",
       "Train_Time                     1.822400e+02     42.855985  \n",
       "Test_Time/MC-Oracle_Test_Time  6.712238e-03      0.003794  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prediction Quality (Updated): Test\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Quality (Updated): Train\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                              0.016596   0.000000            -   \n",
      "W1                                  0.017881   0.000000            -   \n",
      "W1-95R                              0.019078   0.000000            -   \n",
      "M-95L                               0.123094   0.120491      1130.45   \n",
      "M                                   0.126680   0.126680      1143.13   \n",
      "M-95R                               0.132815   0.131784      1157.04   \n",
      "N_Par                          191550.000000   0.000000         2000   \n",
      "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
      "\n",
      "                                   KRidge         GBRF         DNN  \\\n",
      "W1-95L                                  -            -           -   \n",
      "W1                                      -            -           -   \n",
      "W1-95R                                  -            -           -   \n",
      "M-95L                             1125.56      1185.77     74.3589   \n",
      "M                                 1140.79      1206.84     74.6431   \n",
      "M-95R                             1156.01      1227.92     74.7857   \n",
      "N_Par                                   0       410324       41001   \n",
      "Train_Time                        1.60862      2.15968     36.7239   \n",
      "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         1.285100e+06      2.292542  \n",
      "W1                             1.322710e+06      2.328779  \n",
      "W1-95R                         1.371959e+06      2.359401  \n",
      "M-95L                          1.129208e+03      1.129047  \n",
      "M                              1.143108e+03      1.144267  \n",
      "M-95R                          1.159595e+03      1.158204  \n",
      "N_Par                          0.000000e+00  41001.000000  \n",
      "Train_Time                     1.822400e+02     42.855985  \n",
      "Test_Time/MC-Oracle_Test_Time  6.712238e-03      0.003794  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "      <th>GPR</th>\n",
       "      <th>DGN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.285100e+06</td>\n",
       "      <td>2.292542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.322710e+06</td>\n",
       "      <td>2.328779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.371959e+06</td>\n",
       "      <td>2.359401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.120491</td>\n",
       "      <td>1130.45</td>\n",
       "      <td>1125.56</td>\n",
       "      <td>1185.77</td>\n",
       "      <td>74.3589</td>\n",
       "      <td>1.129208e+03</td>\n",
       "      <td>1.129047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.126680</td>\n",
       "      <td>0.126680</td>\n",
       "      <td>1143.13</td>\n",
       "      <td>1140.79</td>\n",
       "      <td>1206.84</td>\n",
       "      <td>74.6431</td>\n",
       "      <td>1.143108e+03</td>\n",
       "      <td>1.144267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.132815</td>\n",
       "      <td>0.131784</td>\n",
       "      <td>1157.04</td>\n",
       "      <td>1156.01</td>\n",
       "      <td>1227.92</td>\n",
       "      <td>74.7857</td>\n",
       "      <td>1.159595e+03</td>\n",
       "      <td>1.158204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>191550.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>410324</td>\n",
       "      <td>41001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>41001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>463.745874</td>\n",
       "      <td>68.616328</td>\n",
       "      <td>1.62012e+09</td>\n",
       "      <td>1.60862</td>\n",
       "      <td>2.15968</td>\n",
       "      <td>36.7239</td>\n",
       "      <td>1.822400e+02</td>\n",
       "      <td>42.855985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.004381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.84636e-06</td>\n",
       "      <td>0.00121333</td>\n",
       "      <td>0.000433868</td>\n",
       "      <td>0.00442503</td>\n",
       "      <td>6.712238e-03</td>\n",
       "      <td>0.003794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle         ENET  \\\n",
       "W1-95L                              0.016596   0.000000            -   \n",
       "W1                                  0.017881   0.000000            -   \n",
       "W1-95R                              0.019078   0.000000            -   \n",
       "M-95L                               0.123094   0.120491      1130.45   \n",
       "M                                   0.126680   0.126680      1143.13   \n",
       "M-95R                               0.132815   0.131784      1157.04   \n",
       "N_Par                          191550.000000   0.000000         2000   \n",
       "Train_Time                        463.745874  68.616328  1.62012e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.004381   1.000000  5.84636e-06   \n",
       "\n",
       "                                   KRidge         GBRF         DNN  \\\n",
       "W1-95L                                  -            -           -   \n",
       "W1                                      -            -           -   \n",
       "W1-95R                                  -            -           -   \n",
       "M-95L                             1125.56      1185.77     74.3589   \n",
       "M                                 1140.79      1206.84     74.6431   \n",
       "M-95R                             1156.01      1227.92     74.7857   \n",
       "N_Par                                   0       410324       41001   \n",
       "Train_Time                        1.60862      2.15968     36.7239   \n",
       "Test_Time/MC-Oracle_Test_Time  0.00121333  0.000433868  0.00442503   \n",
       "\n",
       "                                        GPR           DGN  \n",
       "W1-95L                         1.285100e+06      2.292542  \n",
       "W1                             1.322710e+06      2.328779  \n",
       "W1-95R                         1.371959e+06      2.359401  \n",
       "M-95L                          1.129208e+03      1.129047  \n",
       "M                              1.143108e+03      1.144267  \n",
       "M-95R                          1.159595e+03      1.158204  \n",
       "N_Par                          0.000000e+00  41001.000000  \n",
       "Train_Time                     1.822400e+02     42.855985  \n",
       "Test_Time/MC-Oracle_Test_Time  6.712238e-03      0.003794  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prediction Quality (Updated): Train\")\n",
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) The natural Universal Benchmark: [Bishop's Mixture Density Network](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)\n",
    "\n",
    "This implementation is as follows:\n",
    "- For every $x$ in the trainingdata-set we fit a GMM $\\hat{\\nu}_x$, using the [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), with the same number of centers as the deep neural model in $\\mathcal{NN}_{1_{\\mathbb{R}^d},\\mathcal{D}}^{\\sigma:\\star}$ which we are evaluating.  \n",
    "- A Mixture density network is then trained to predict the infered parameters; given any $x \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Preparing Training Outputs for MDNs using EM-Algorithm\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 246/1000 [03:24<13:17,  1.06s/it]"
     ]
    }
   ],
   "source": [
    "if output_dim == 1:\n",
    "    # %run Mixture_Density_Network.ipynb\n",
    "    exec(open('Mixture_Density_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Outputs\n",
    "Now we piece together all the numerical experiments and report a nice summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prasing Quality Metric Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Performance Metrics\n",
    "Summary_pred_Qual_models.to_latex((results_tables_path+\"/Final_Results/\"+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"),\n",
    "                                 caption=(\"Quality Metrics; d:\"+str(problem_dim)+\", D:\"+str(output_dim)+\", Depth:\"+str(Depth_Bayesian_DNN)+\", Width:\"+str(width)+\", Dropout rate:\"+str(Dropout_rate)+\".\"),\n",
    "                                 float_format=\"{:0.3g}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Terminal Runner(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Terminal Running\n",
    "print(\"===================\")\n",
    "print(\"Predictive Quality:\")\n",
    "print(\"===================\")\n",
    "print(Summary_pred_Qual_models)\n",
    "print(\"===================\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Kernel_Used_in_GPR: \"+str(GPR_trash.kernel))\n",
    "print(\"🙃🙃 Have a wonderful day! 🙃🙃\")\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
