{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Conditional Laws for Random-Fields - via:\n",
    "\n",
    "## Universal $\\mathcal{P}_1(\\mathbb{R})$-Deep Neural Model (Type A)\n",
    "\n",
    "---\n",
    "\n",
    "By: [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Algorithm:\n",
    "---\n",
    "## 1) Generate Data:\n",
    "Generates the empirical measure $\\sum_{n=1}^N \\delta_{X_T(\\omega_n)}$ of $X_T$ conditional on $X_0=x_0\\in \\mathbb{R}$ *($x_0$ and $T>0$ are user-provided)*.\n",
    "\n",
    "## 2) Get \"Sample Barycenters\":\n",
    "Let $\\{\\mu_n\\}_{n=1}^N\\subset\\mathcal{P}_1(\\mathbb{R}^d)$.  Then, the *sample barycenter* is defined by:\n",
    "1. $\\mathcal{M}^{(0)}\\triangleq \\left\\{\\hat{\\mu}_n\\right\\}_{n=1}^N$,\n",
    "2. For $1\\leq n\\leq \\mbox{N sample barycenters}$: \n",
    "    - $\n",
    "\\mu^{\\star}\\in \\underset{\\tilde{\\mu}\\in \\mathcal{M}^{(n)}}{\\operatorname{argmin}}\\, \\sum_{n=1}^N \\mathcal{W}_1\\left(\\mu^{\\star},\\mu_n\\right),\n",
    "$\n",
    "    - $\\mathcal{M}^{(n)}\\triangleq \\mathcal{M}^{(n-1)} - \\{\\mu^{\\star}\\},$\n",
    "*i.e., the closest generated measure form the random sample to all other elements of the random sample.*\n",
    "\n",
    "---\n",
    "**Note:** *We simplify the computational burden of getting the correct classes by putting this right into this next loop.*\n",
    "\n",
    "## 3) Train Deep Classifier:\n",
    "$\\hat{f}\\in \\operatorname{argmin}_{f \\in \\mathcal{NN}_{d:N}^{\\star}} \n",
    "\\sum_{x \\in \\mathbb{X}}\n",
    "\\, \n",
    "\\mathbb{H}\n",
    "\\left(\n",
    "    \\operatorname{Softmax}_N\\circ f(x)_n| I\\left\\{W_1(\\hat{\\mu}_n,\\mu_x),\\inf_{m\\leq N} W_1(\\hat{\\mu}_m,\\mu_x)\\right\\}\n",
    "\\right);\n",
    "$\n",
    "where $\\mathbb{H}$ is the categorical cross-entropy.  \n",
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "## Notes - Why the procedure is so computationally efficient?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auxiliaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n"
     ]
    }
   ],
   "source": [
    "trial_run = True\n",
    "groud_truth = \"rSDE\"\n",
    "\n",
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "\n",
    "### Set Seed\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Euler_Maruyama_Steps = 100\n",
    "N_Monte_Carlo_Samples = 10**2\n",
    "N_Monte_Carlo_Samples_Test = 10**3 # How many MC-samples to draw from test-set?\n",
    "\n",
    "# End times for Time-Grid\n",
    "T_end = 1\n",
    "T_end_test = 1.1\n",
    "\n",
    "\n",
    "## Grid\n",
    "N_Grid_Finess = 100\n",
    "Max_Grid = 1\n",
    "\n",
    "# Number of Centers (\\hat{\\mu}_s)\n",
    "N_Quantizers_to_parameterize = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This option sets $\\delta$ in $B_{\\mathbb{R}\\times [0,\\infty)}(\\hat{x}_n,\\delta)$; where $\\hat{x}_n\\in \\nu_{\\cdot}^{-1}[\\hat{\\mu}]$.  N_measures_per_center sets the number of samples to draw in this ball...by construction the training set is $\\delta$-bounded and $\\nu_{(x,t)}$, for any such $x$ is $\\omega_{\\nu_{\\cdot}}(\\delta)$-bounded in $\\mathcal{P}_1(\\mathbb{R})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.1\n",
    "N_measures_per_center = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Setting *N_Quantizers_to_parameterize* prevents any barycenters and sub-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters\n",
    "Ratio $\\frac{\\text{Testing Datasize}}{\\text{Training Datasize}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size_ratio = .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation from Rough Neural-SDE\n",
    "Simulate via Euler-M method from:\n",
    "$$ \n",
    "X_T^x = x + \\int_0^T \\alpha(s,X_s^x)ds + \\int_0^T\\beta(s,X_s^s)dW_s.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dim = 2\n",
    "width = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2a = np.random.uniform(size=np.array([problem_dim,width]),low=-.5,high=.5)\n",
    "W_1a = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "def alpha(t,x):\n",
    "    x_internal = x.reshape(-1,)\n",
    "    x_internal = np.matmul(W_1a,x_internal)\n",
    "    x_internal = np.matmul(W_2a,np.cos(x_internal))\n",
    "    x_internal = x_internal\n",
    "    return x_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2b = np.random.uniform(size=np.array([(problem_dim**2),width]),low=-.5,high=.5)\n",
    "W_1b = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "def beta(t,x):\n",
    "    x_internal = x.reshape(-1,)\n",
    "    x_internal = np.matmul(W_1b,x_internal)\n",
    "    x_internal = np.matmul(W_2b,np.maximum(0,np.array(x_internal)))\n",
    "    x_internal = x_internal.reshape([problem_dim,problem_dim])\n",
    "    return x_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughness Meta-parameters\n",
    " - Roughness is $H$,\n",
    " - Ratio_fBM_to_typical_vol is $\\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rougness = 0.01 # Hurst Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate using Euler-Maruyama + Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Define Simulator Data Generator #\n",
    "###################################\n",
    "def Euler_Maruyama_Generator(x_0,\n",
    "                             N_Euler_Maruyama_Steps = 10,\n",
    "                             N_Monte_Carlo_Samples = 100,\n",
    "                             T_begin = 0,\n",
    "                             T_end = 1,\n",
    "                             Hurst = 0.1): \n",
    "    #----------------------------#    \n",
    "    # DEFINE INTERNAL PARAMETERS #\n",
    "    #----------------------------#\n",
    "    # Internal Initialization(s)\n",
    "    ## Initialize current state\n",
    "    n_sample = 0\n",
    "    ## Initialize Incriments\n",
    "    dt = (T_end-T_begin)/N_Euler_Maruyama_Steps\n",
    "    sqrt_dt = np.sqrt(dt)\n",
    "    # Get Dimension\n",
    "    dim_x_0 = len(x_0)\n",
    "\n",
    "    #-----------------------------#    \n",
    "    # Generate Monte-Carlo Sample #\n",
    "    #-----------------------------#\n",
    "    for n_sample in range(N_Monte_Carlo_Samples):\n",
    "        # Initialize Current State \n",
    "        X_current = x_0\n",
    "        # Generate roughness\n",
    "        for fBM_path_i in range(dim_x_0):\n",
    "            sigma_rough_loop = FBM(n=N_Euler_Maruyama_Steps, hurst=Hurst, length=1, method='daviesharte').fbm().reshape(1,-1)\n",
    "            if fBM_path_i == 0:\n",
    "                sigma_rough = sigma_rough_loop\n",
    "            else:\n",
    "                sigma_rough = np.append(sigma_rough,sigma_rough_loop,axis=0)\n",
    "\n",
    "    #     Perform Euler-Maruyama Simulation\n",
    "        for t in range((sigma_rough.shape[1]-1)):\n",
    "            # Update Internal Parameters\n",
    "            ## Get Current Time\n",
    "            t_current = t*((T_end - T_begin)/N_Euler_Maruyama_Steps)\n",
    "\n",
    "            # Update Generated Path\n",
    "            ## Generate Current State-Update Components\n",
    "            drift_t = alpha(t_current,X_current.reshape(-1,))*dt\n",
    "            vol_t = beta(t_current,X_current)\n",
    "            BH_t = (sigma_rough[:,(1+t)])\n",
    "            ## Compute Update\n",
    "            X_current = drift_t + (np.matmul(vol_t,BH_t))       \n",
    "\n",
    "        # Update Empirical Measure\n",
    "        X_current = X_current.reshape(-1,1)\n",
    "        if n_sample ==0:\n",
    "            X_T_Empirical = X_current\n",
    "        else:\n",
    "            X_T_Empirical = np.append(X_T_Empirical,X_current,axis=-1)\n",
    "\n",
    "    # Reshape\n",
    "    X_T_Empirical = X_T_Empirical.T\n",
    "    \n",
    "    # Add Stationary Uniform Noise\n",
    "#     if uniform_noise>0:\n",
    "#         X_T_Empirical = X_T_Empirical #+ np.random.uniform(low=-uniform_noise,high=uniform_noise,size=X_T_Empirical.shape())\n",
    "    return X_T_Empirical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "# Define Output Data Generator #\n",
    "################################\n",
    "def Euler_Maruyama_simulator(Grid_in,\n",
    "                             N_Monte_Carlo_Samples=N_Monte_Carlo_Samples,\n",
    "                             Rougness=Rougness,\n",
    "                             N_Euler_Maruyama_Steps =N_Euler_Maruyama_Steps):\n",
    "    #----------------------------#\n",
    "    ## Generate Data Using Grid ##\n",
    "    #----------------------------#\n",
    "    # Internal Parameters\n",
    "    N_Grid_Instances = Grid_in.shape[0]\n",
    "    # Initializations\n",
    "    measure_weights = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples\n",
    "    measures_locations_list_internal = []\n",
    "    measures_weights_list_internal = []\n",
    "\n",
    "\n",
    "    # Perform Euler-Maruyama distritization + Monte-Carlo Sampling.\n",
    "    #----------------------------------------------------------------------------------------------#\n",
    "    # Perform Monte-Carlo Data Generation\n",
    "    for i in tqdm(range(N_Grid_Instances)):\n",
    "        x_loop = Grid_in[i,]\n",
    "        # Simulate Paths\n",
    "        paths_loop = Euler_Maruyama_Generator(x_0=x_loop,\n",
    "                                              N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                              N_Monte_Carlo_Samples = N_Monte_Carlo_Samples,\n",
    "                                              T_begin = 0,\n",
    "                                              T_end = 1,\n",
    "                                              Hurst = Rougness)\n",
    "\n",
    "        # Map numpy to list\n",
    "        measures_locations_loop = paths_loop.tolist()\n",
    "\n",
    "        # Append to List\n",
    "        measures_locations_list_internal.append(measures_locations_loop)\n",
    "        measures_weights_list_internal.append(measure_weights)\n",
    "\n",
    "\n",
    "    return measures_locations_list_internal, measures_weights_list_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\omega^{-1}(\\epsilon)$-Bounded Random Partitioner\n",
    "Generates a bounded random partition, in the sense of: [Geometry, Flows, and Graph-Partitioning Algorithms](https://cacm.acm.org/magazines/2008/10/515-geometry-flows-and-graph-partitioning-algorithms/fulltext?mobile=false), [A. Naor et al.](https://link.springer.com/article/10.1007/s00222-004-0400-5), as implemented in [Learning Sub-Patterns in Piece-Wise Continuous Functions](https://arxiv.org/abs/2010.15571)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Subsetting Matrix\n",
    "X_training_remaining = np.copy(X_train)\n",
    "# Initialize Random Radius\n",
    "delta=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False  True False False False  True]\n",
      "[False False False False False False False  True False False]\n",
      "[False False  True False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False  True False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False  True False False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False False False False  True False False False]\n",
      "[False False False False  True False False False False False]\n",
      "[ True False False False False False False False False False]\n",
      "[False False False False False False False False False False]\n",
      "[False False False False False False False False  True False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/kratsioa/.local/lib/python3.7/site-packages/scipy/spatial/kdtree.py:55: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.sum(np.abs(y-x)**p, axis=-1)\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
      "  app.launch_new_instance()\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "/usr/lib64/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/lib64/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "while np.min(X_training_remaining)<math.inf:    \n",
    "    # Randomize Radius\n",
    "    delta_loop = (delta*np.random.uniform(low=0,high=0.5,size=1))[0]\n",
    "    ##Tweak Delta\n",
    "    ### Note: To avoid \"too small\" of a delta\n",
    "    delta_loop = np.maximum(delta_loop,np.mean(dist_mat_loop[(distes_mat_loop<math.inf).reshape(-1,)]))\n",
    "    \n",
    "    # Get Random Center\n",
    "    random_center_loop = np.random.choice(range(X_training_remaining.shape[0]))\n",
    "    random_center_loop = (X_training_remaining[random_center_loop]).reshape([1,-1])\n",
    "\n",
    "    # Get Distances To Current Center\n",
    "    distes_mat_loop = distance_matrix(random_center_loop,X_training_remaining)\n",
    "\n",
    "    ## DataSet\n",
    "    good_indices = (distes_mat_loop<delta).reshape(-1,)\n",
    "    ## Get good cluster for current center\n",
    "    Classes_loop = (good_indices + 0).reshape(-1,1)\n",
    "\n",
    "    # Update(s)\n",
    "    ## Outputs\n",
    "    if np.max(X_training_remaining)<math.inf:\n",
    "        ## Initialize Centers\n",
    "        X_centers = random_center_loop\n",
    "        ## Initialize Classes\n",
    "        Train_classes = Classes_loop\n",
    "    else:\n",
    "        ## Update Centers\n",
    "        X_centers = np.append(X_current_cluster,X_current_cluster,axis=0)\n",
    "        ## Update Classes\n",
    "        Train_classes = np.append(Train_classes,Classes_loop,axis=-1)\n",
    "\n",
    "    ## Remove Clusterd Centers from Current Dataset\n",
    "    X_training_remaining[good_indices] = math.inf\n",
    "    print(good_indices)\n",
    "\n",
    "## Coerce Output\n",
    "Train_classes = Train_classes.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(Min_data_size_percentage,\n",
    "                               Delta_in, \n",
    "                               X_train_in,\n",
    "                               y_train_in, \n",
    "                               CV_folds_failsafe, \n",
    "                               min_size):\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    # 1) Sample radius from unifom distribution #\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    # 2) Apply Random Bijection (Shuffle) #\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    X_train_in_shuffled = X_train_in#.sample(frac=1)\n",
    "    y_train_in_shuffled = y_train_in#.sample(frac=1)\n",
    "\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # X) Initializations #\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # Initialize Random Radius\n",
    "    rand_radius = Delta_in*alpha\n",
    "\n",
    "    # Initialize Data_sizes & ratios\n",
    "    N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "    N_radios = np.array([])\n",
    "    N_pool_train_loop = N_tot\n",
    "    # Initialize List of Dataframes\n",
    "    X_internal_train_list = list()\n",
    "    y_internal_train_list = list()\n",
    "\n",
    "    # Initialize Partioned Data-pool\n",
    "    X_internal_train_pool = X_train_in_shuffled\n",
    "    y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "    # Initialize counter \n",
    "    part_current_loop = 0\n",
    "\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "    # 3) Iteratively Build Parts #\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "\n",
    "    while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "        # Extract Current Center\n",
    "        center_loop = X_internal_train_pool.iloc[0]\n",
    "        # Compute Distances\n",
    "        ## Training\n",
    "        distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "        distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "        # Evaluate which Distances are less than the given random radius\n",
    "        Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "        Part_train_loop_y = list(itertools.compress(y_internal_train_pool, (distances_pool_loop_train<rand_radius)))\n",
    "\n",
    "        # Remove all data-points which are \"too small\"\n",
    "        if X_internal_train_pool.shape[0] > max(CV_folds,4):\n",
    "            # Append Current part to list\n",
    "            X_internal_train_list.append(Part_train_loop)\n",
    "            y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "        # Remove current part from pool \n",
    "        X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "        y_internal_train_pool = list(itertools.compress(y_internal_train_pool, ((np.logical_not(distances_pool_loop_train<rand_radius)))))\n",
    "    \n",
    "\n",
    "        # Update Current size of pool of training data\n",
    "        N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "        N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "        # Update Counter\n",
    "        part_current_loop = part_current_loop +1\n",
    "        \n",
    "        # Update User\n",
    "        print((N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "    # Post processing #\n",
    "    #-----------------#\n",
    "    # Remove Empty Partitions\n",
    "    N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------#\n",
    "    # Combine parts which are too small to perform CV without an error\n",
    "    #-----------------------------------------------------------------#\n",
    "    # Initialize lists (partitions) with \"enough\" datums per part\n",
    "    X_internal_train_list_good = list()\n",
    "    y_internal_train_list_good = list()\n",
    "    X_small_parts = list()\n",
    "    y_small_parts = list()\n",
    "    # Initialize first list item test\n",
    "    is_first = True\n",
    "    # Initialize counter\n",
    "    goods_counter = 0\n",
    "    for search_i in range(len(X_internal_train_list)):\n",
    "        number_of_instances_in_part = len(X_internal_train_list[search_i]) \n",
    "        if number_of_instances_in_part < max(CV_folds_failsafe,min_size):\n",
    "            # Check if first \n",
    "            if is_first:\n",
    "                # Initialize set of small X_parts\n",
    "                X_small_parts = X_internal_train_list[search_i]\n",
    "                # Initialize set of small y_parts\n",
    "                y_small_parts = y_internal_train_list[search_i]\n",
    "\n",
    "                # Set is_first to false\n",
    "                is_first = False\n",
    "            else:\n",
    "                X_small_parts = X_small_parts.append(X_internal_train_list[search_i])\n",
    "                y_small_parts = np.append(y_small_parts,y_internal_train_list[search_i])\n",
    "#                 y_small_parts = y_small_parts.append(y_internal_train_list[search_i])\n",
    "        else:\n",
    "            # Append to current list\n",
    "            X_internal_train_list_good.append(X_internal_train_list[search_i])\n",
    "            y_internal_train_list_good.append(y_internal_train_list[search_i])\n",
    "            # Update goods counter \n",
    "            goods_counter = goods_counter +1\n",
    "\n",
    "    # Append final one to good list\n",
    "    X_internal_train_list_good.append(X_small_parts)\n",
    "    y_internal_train_list_good.append(y_small_parts)\n",
    "\n",
    "    # reset is_first to false (inscase we want to re-run this particular block)\n",
    "    is_first = True\n",
    "\n",
    "    # Set good lists to regular lists\n",
    "    X_internal_train_list = X_internal_train_list_good\n",
    "    y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return Value #\n",
    "    #--------------#\n",
    "    return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = .4\n",
    "N_train_size = 10\n",
    "N_test_size = int(np.round(N_train_size*train_test_ratio,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.uniform(size=np.array([N_train_size,problem_dim]),low=-.5,high=.5)\n",
    "X_test = np.random.uniform(size=np.array([N_test_size,problem_dim]),low=-.5,high=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training Data (Outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 78.47it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 79.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get Training Data\n",
    "## Timer\n",
    "train_DATA_MC = time.time()\n",
    "## Do: Simulation\n",
    "Y_train_locations,Y_train_weights = Euler_Maruyama_simulator(X_train)\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "## END: TIMER\n",
    "train_DATA_MC = time.time() - train_DATA_MC\n",
    "\n",
    "# Get Testing Data\n",
    "## Timer\n",
    "test_DATA_MC = time.time()\n",
    "## Do: Simulation\n",
    "Y_test_locations,Y_test_weights = Euler_Maruyama_simulator(X_test)\n",
    "# X_test = pd.DataFrame(X_test)\n",
    "## END: TIMER\n",
    "test_DATA_MC = time.time() - test_DATA_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "#### Start Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Timer\n",
    "Type_A_timer_Begin = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.8\n",
      "0.7\n",
      "0.6\n",
      "0.5\n",
      "0.4\n",
      "0.3\n",
      "0.2\n",
      "0.1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=0.01,\n",
    "                                                                   Delta_in=0.001, \n",
    "                                                                   X_train_in = X_train,\n",
    "                                                                   y_train_in = Y_test_locations, \n",
    "                                                                   CV_folds_failsafe = 1, \n",
    "                                                                   min_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          0         1\n",
       " 0  0.423216  0.077262\n",
       " 1 -0.257886  0.078035\n",
       " 2 -0.353631  0.344702\n",
       " 3 -0.480096  0.039046\n",
       " 4 -0.397106  0.114384\n",
       " 5 -0.039658  0.123780]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_parts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Number of Parts currently generated\n",
    "N_parts_generated = 0\n",
    "\n",
    "# Generate Partition (with option to regernerate if only 1 part is randomly produced)\n",
    "while N_parts_generated < 2:\n",
    "    # Generate Parts\n",
    "    X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=Min_data_size_percentage_auto, \n",
    "                                                                      q_in=q_in_auto, \n",
    "                                                                      X_train_in=X_train, \n",
    "                                                                      y_train_in=data_y, \n",
    "                                                                      CV_folds_failsafe=CV_folds,\n",
    "                                                                      min_size = min_size_part)\n",
    "    \n",
    "    # Update Number of Parts\n",
    "    N_parts_generated = len(X_parts_list)\n",
    "    # Shuffle hyperparameters\n",
    "    Min_data_size_percentage_auto = (Min_data_size_percentage_auto + random.uniform(0,.3)) % 1\n",
    "    q_in_auto = (q_in_auto + random.uniform(0,.3)) % 1\n",
    "    \n",
    "    # Update User\n",
    "    print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')\n",
    "    \n",
    "# Trash removal (removes empty parts)\n",
    "X_parts_list = list(filter(([]).__ne__, X_parts_list))\n",
    "y_parts_list = list(filter(([]).__ne__, y_parts_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if groud_truth == \"2lnflow\":\n",
    "    print(\"Building Test Set - 2-logNormal Ground-Truth\")\n",
    "    # Generate Testing Dataset (Inputs)\n",
    "    x_tests = np.random.uniform(np.min(X_train[:,0]),np.max(X_train[:,0]),10)\n",
    "    t_tests = np.arange(start=0,\n",
    "                        stop=T_end,\n",
    "                        step = (T_end_test/N_Euler_Maruyama_Steps))\n",
    "    for x_i in tqdm(range(len(x_tests))):\n",
    "        for t_j in range(len(t_tests)):\n",
    "            test_set_entry = np.array([t_tests[t_j],x_tests[x_i]]).reshape(1,-1)\n",
    "            if (x_i==0 and t_j ==0):\n",
    "                X_test = test_set_entry\n",
    "            else:\n",
    "                X_test = np.append(X_test,test_set_entry,axis=0)\n",
    "\n",
    "    # Generate Testing Dataset (Outputs)\n",
    "    measures_locations_test_list, measures_weights_test_list = twoparameter_flow_sampler(X_test,N_Monte_Carlo_Samples_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough SDE with Uniform Noise:\n",
    "Simulation of the random-field:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde{X}_t^x &= x + \\int_0^t \\alpha(s,X_t^x)ds + (1-\\eta)\\int_0^t \\beta(s,X_t^x)dW_t + dB_t^H,\\\\\n",
    "X_t^x & = \\tilde{X}_t^x + U;\n",
    "\\end{aligned}\n",
    "$$\n",
    "where: \n",
    " - $(B_t^H)_t$ is a [fractional Brownian Motion](https://arxiv.org/pdf/1406.1956.pdf) with [Hurst exponent](https://en.wikipedia.org/wiki/Hurst_exponent) $H\\in (0,1)$,\n",
    " - $(W_t)_t$ is a [Brownian Motion](https://en.wikipedia.org/wiki/Wiener_process),\n",
    " - $\\alpha$ and $\\beta$ are uniformly [Lipschitz-functions](https://en.wikipedia.org/wiki/Lipschitz_continuity) of appropriate input/output dimension,\n",
    " - $U_t\\sim \\text{Uniform}(-.5,.5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD Simulator (Backend)\n",
    "# %run Simulator.ipynb\n",
    "exec(open('Simulator.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW?\n",
    "if groud_truth == \"rSDE\":\n",
    "    print(\"Building Training + Testing Set - rough-SDE Ground-Truth\")\n",
    "    \n",
    "    # Initialize position Counter\n",
    "    position_counter = 0\n",
    "    # Iniitalize uniform weights vector\n",
    "    measures_weights_list_loop = np.ones(N_Monte_Carlo_Samples)/N_Monte_Carlo_Samples\n",
    "\n",
    "    # For simplicity override:\n",
    "    N_Monte_Carlo_Samples_Test = N_Monte_Carlo_Samples\n",
    "    \n",
    "    # Overrine Number of Centers\n",
    "    N_x = int(np.round(len(x_Grid_barycenters)*(1-test_size_ratio)))\n",
    "    N_x_test = int(np.round(len(x_Grid_barycenters)*(test_size_ratio)))\n",
    "    N_t = len(t_Grid_barycenters)\n",
    "    N_Quantizers_to_parameterize = N_x*N_t\n",
    "    \n",
    "    # Initialize number of training and testing to grab from each initial condition\n",
    "    N_train = int(N_Euler_Maruyama_Steps)\n",
    "    N_test = N_train\n",
    "\n",
    "    for x_i in tqdm(range(N_x)):\n",
    "        for t_j in range(N_t):\n",
    "\n",
    "            # Get Current Locations\n",
    "            x_center = x_Grid_barycenters[x_i]\n",
    "            t_center = t_Grid_barycenters[t_j]\n",
    "\n",
    "            current_cover = Euler_Maruyama_Generator(x_0 = x_center,\n",
    "                                                     N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                                     N_Monte_Carlo_Samples = N_Monte_Carlo_Samples,\n",
    "                                                     T_begin = t_center,\n",
    "                                                     T_end = (t_center+delta),\n",
    "                                                     Hurst = Rougness,\n",
    "                                                     Ratio_fBM_to_typical_vol = Ratio_fBM_to_typical_vol)\n",
    "            # Get Barycenter\n",
    "            barycenter_at_current_location = current_cover[0,:]\n",
    "            \n",
    "            # Subset\n",
    "            ## Measure Location(s)\n",
    "            measures_locations_list_current_train = (current_cover[:N_train]).tolist()\n",
    "            ## Measure Weight(s)\n",
    "            measures_weights_list_current = list(itertools.repeat(measures_weights_list_loop,N_Monte_Carlo_Samples))\n",
    "\n",
    "            \n",
    "            # Get Current Training Data Positions\n",
    "            t_grid_current = (np.linspace(start=t_center,\n",
    "                                          stop=(t_center+delta),\n",
    "                                          num=N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "            x_grid_current = (x_center*np.ones(N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "\n",
    "            X_train_current = (np.append(x_grid_current,t_grid_current,axis=0)).T\n",
    "            ## Subset\n",
    "            X_train_updater = X_train_current#[:N_train,:] # Get top of array (including center)\n",
    "\n",
    "            # Get Current Classes\n",
    "            Classifer_Wasserstein_Centers_loop = np.zeros([N_train,N_Quantizers_to_parameterize])\n",
    "            Classifer_Wasserstein_Centers_loop[:, position_counter] =  1\n",
    "\n",
    "\n",
    "            # Updates Classes\n",
    "            if (x_i==0 and t_j==0):\n",
    "                # INITIALIZE: Classifiers\n",
    "                Classifer_Wasserstein_Centers = Classifer_Wasserstein_Centers_loop\n",
    "                # INITIALIZE: Training Data\n",
    "                X_train = X_train_updater\n",
    "                # INITIALIZE: Barycenters Array\n",
    "                Barycenters_Array = barycenter_at_current_location.reshape(-1,1)\n",
    "                # INITIALIZE: Measures and locations\n",
    "                measures_locations_list = measures_locations_list_current_train\n",
    "                measures_weights_list = measures_weights_list_current\n",
    "            else:\n",
    "                # UPDATE: Classifer\n",
    "                Classifer_Wasserstein_Centers = np.append(Classifer_Wasserstein_Centers,Classifer_Wasserstein_Centers_loop,axis=0)\n",
    "                # UPDATE: Training Data\n",
    "                X_train = np.append(X_train,X_train_updater,axis=0)\n",
    "                # UPDATE: Populate Barycenters Array\n",
    "                Barycenters_Array = np.append(Barycenters_Array,(barycenter_at_current_location.reshape(-1,1)),axis=-1)\n",
    "                # UPDATE: Measures and locations\n",
    "                ## Train\n",
    "                measures_locations_list = measures_locations_list + measures_locations_list_current_train\n",
    "                measures_weights_list = measures_locations_list + measures_weights_list_current\n",
    "\n",
    "            # Update Position\n",
    "            position_counter = position_counter + 1\n",
    "            \n",
    "    for x_i in tqdm(range(N_x_test)):\n",
    "        for t_j in range(N_t):\n",
    "\n",
    "            # Get Current Locations\n",
    "            x_center = np.random.uniform(low=(-Max_Grid+x_0),high=(-Max_Grid+x_0),size=1)\n",
    "            t_center = t_Grid_barycenters[t_j]\n",
    "\n",
    "            current_cover = Euler_Maruyama_Generator(x_0 = x_center,\n",
    "                                                     N_Euler_Maruyama_Steps = N_Euler_Maruyama_Steps,\n",
    "                                                     N_Monte_Carlo_Samples = N_Monte_Carlo_Samples,\n",
    "                                                     T_begin = t_center,\n",
    "                                                     T_end = (t_center+delta),\n",
    "                                                     Hurst = Rougness,\n",
    "                                                     Ratio_fBM_to_typical_vol = Ratio_fBM_to_typical_vol)\n",
    "            # Get Barycenter\n",
    "            barycenter_at_current_location = current_cover[0,:]\n",
    "            \n",
    "            # Subset\n",
    "            ## Measure Location(s)\n",
    "            measures_locations_list_current_test = (current_cover[:-N_train]).tolist()\n",
    "            ## Measure Weight(s)\n",
    "            measures_weights_list_current = list(itertools.repeat(measures_weights_list_loop,N_Monte_Carlo_Samples))\n",
    "\n",
    "            \n",
    "            # Get Current Training Data Positions\n",
    "            t_grid_current = (np.linspace(start=t_center,\n",
    "                                          stop=(t_center+delta),\n",
    "                                          num=N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "            x_grid_current = (x_center*np.ones(N_Euler_Maruyama_Steps)).reshape(1,-1)\n",
    "\n",
    "            X_train_current = (np.append(x_grid_current,t_grid_current,axis=0)).T\n",
    "            ## Subset\n",
    "            X_test_updater = X_train_current#[-N_test:,:] # Get bottom of array (exclusing center)\n",
    "\n",
    "            # Get Current Classes\n",
    "            Classifer_Wasserstein_Centers_loop = np.zeros([N_test,N_Quantizers_to_parameterize])\n",
    "            Classifer_Wasserstein_Centers_loop[:, position_counter] =  1\n",
    "\n",
    "\n",
    "            # Updates Classes\n",
    "            if (x_i==0 and t_j==0):\n",
    "                # INITIALIZE: Classifiers\n",
    "                Classifer_Wasserstein_Centers = Classifer_Wasserstein_Centers_loop\n",
    "                # INITIALIZE: Training Data\n",
    "                X_test = X_test_updater\n",
    "                # INITIALIZE: Barycenters Array\n",
    "                Barycenters_Array = barycenter_at_current_location.reshape(-1,1)\n",
    "                # INITIALIZE: Measures and locations\n",
    "                measures_locations_test_list = measures_locations_list_current_test\n",
    "                measures_weights_test_list = measures_weights_list_current\n",
    "            else:\n",
    "                # UPDATE: Classifer\n",
    "                Classifer_Wasserstein_Centers = np.append(Classifer_Wasserstein_Centers,Classifer_Wasserstein_Centers_loop,axis=0)\n",
    "                # UPDATE: Training Data\n",
    "                X_test = np.append(X_test,X_test_updater,axis=0)\n",
    "                # UPDATE: Populate Barycenters Array\n",
    "                Barycenters_Array = np.append(Barycenters_Array,(barycenter_at_current_location.reshape(-1,1)),axis=-1)\n",
    "                # UPDATE: Measures and locations\n",
    "                ## Test\n",
    "                measures_locations_test_list = measures_locations_test_list + measures_locations_list_current_test\n",
    "                measures_weights_test_list = measures_locations_test_list + measures_weights_list_current\n",
    "\n",
    "            # Update Position\n",
    "            position_counter = position_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Monte-Carlo Timer\n",
    "test_DATA_MC = time.time() - test_DATA_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we train a deep (feed-forward) classifier:\n",
    "$$\n",
    "\\hat{f}\\triangleq \\operatorname{Softmax}_N\\circ W_J\\circ \\sigma \\bullet \\dots \\sigma \\bullet W_1,\n",
    "$$\n",
    "to identify which barycenter we are closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-Load Grid and Redefine Relevant Input/Output dimensions in dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Load Hyper-parameter Grid\n",
    "exec(open('CV_Grid.py').read())\n",
    "# Re-Load Classifier Function(s)\n",
    "exec(open('Helper_Functions.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==========================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Redefine (Dimension-related) Elements of Grid\n",
    "param_grid_Deep_Classifier['input_dim'] = [2]\n",
    "param_grid_Deep_Classifier['output_dim'] = [N_Quantizers_to_parameterize]\n",
    "\n",
    "# Train simple deep classifier\n",
    "predicted_classes_train, predicted_classes_test, N_params_deep_classifier, timer_output = build_simple_deep_classifier(n_folds = CV_folds, \n",
    "                                                                                                        n_jobs = n_jobs, \n",
    "                                                                                                        n_iter = n_iter, \n",
    "                                                                                                        param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                        X_train = X_train, \n",
    "                                                                                                        y_train = Classifer_Wasserstein_Centers,\n",
    "                                                                                                        X_test = X_test)\n",
    "\n",
    "print(\"=================================================\")\n",
    "print(\"Training Classifer Portion of Type-A Model: Done!\")\n",
    "print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Predicted Quantized Distributions\n",
    "- Each *row* of \"Predicted_Weights\" is the $\\beta\\in \\Delta_N$.\n",
    "- Each *Column* of \"Barycenters_Array\" denotes the $x_1,\\dots,x_N$ making up the points of the corresponding empirical measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Weights\n",
    "## Train\n",
    "print(\"#---------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): START\")\n",
    "print(\"#---------------------------------------#\")\n",
    "Predicted_Weights = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):    \n",
    "    b = np.repeat(np.array(predicted_classes_train[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b = b/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights = b\n",
    "    else:\n",
    "        Predicted_Weights = np.append(Predicted_Weights,b,axis=1)\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Training Set (Regression): END\")\n",
    "print(\"#-------------------------------------#\")\n",
    "\n",
    "## Test\n",
    "print(\"#-------------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): START\")\n",
    "print(\"#-------------------------------------#\")\n",
    "Predicted_Weights_test = np.array([])\n",
    "for i in tqdm(range(N_Quantizers_to_parameterize)):\n",
    "    b_test = np.repeat(np.array(predicted_classes_test[:,i],dtype='float').reshape(-1,1),N_Monte_Carlo_Samples,axis=-1)\n",
    "    b_test = b_test/N_Monte_Carlo_Samples\n",
    "    if i ==0 :\n",
    "        Predicted_Weights_test = b_test\n",
    "    else:\n",
    "        Predicted_Weights_test = np.append(Predicted_Weights_test,b_test,axis=1)\n",
    "print(\"#-----------------------------------#\")\n",
    "print(\"Building Test Set (Predictions): END\")\n",
    "print(\"#-----------------------------------#\")\n",
    "        \n",
    "# Format Points of Mass\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: START\")\n",
    "print(\"#-----------------------------#\")\n",
    "Barycenters_Array = Barycenters_Array.T.reshape(-1,)\n",
    "print(\"#-----------------------------#\")\n",
    "print(\"Building Barycenters Set: END\")\n",
    "print(\"#-----------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Timer\n",
    "Type_A_timer_end = time.time()\n",
    "# Compute Lapsed Time Needed For Training\n",
    "Time_Lapse_Model_A = Type_A_timer_end - Type_A_timer_Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Complexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Complexity = pd.DataFrame({\"N_Centers\":N_Quantizers_to_parameterize,\n",
    "                                 \"N_Q\":N_Monte_Carlo_Samples,\n",
    "                                 \"N_Params\":N_params_deep_classifier,\n",
    "                                 \"Training Time\":Time_Lapse_Model_A,\n",
    "                                 \"T_Test/T_Test-MC\": (timer_output/test_DATA_MC),\n",
    "                                 \"Time Test\": timer_output,\n",
    "                                 \"Time EM-MC\": test_DATA_MC},index=[\"Model_Complexity_metrics\"])\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "Model_Complexity.to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__ModelComplexities.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Moment Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Training Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors = np.array([])\n",
    "Mean_errors = np.array([])\n",
    "Var_errors = np.array([])\n",
    "Skewness_errors = np.array([])\n",
    "Kurtosis_errors = np.array([])\n",
    "predictions_mean = np.array([])\n",
    "true_mean = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_list)-1)):    \n",
    "    # Get Laws\n",
    "    W1_loop = ot.emd2_1d(Barycenters_Array,\n",
    "                         np.array(measures_locations_list[x_i]).reshape(-1,),\n",
    "                         Predicted_Weights[x_i,].reshape(-1,),\n",
    "                         (np.array(measures_weights_list[x_i])).reshape(-1,))\n",
    "    W1_errors = np.append(W1_errors,W1_loop)\n",
    "    # Get Means\n",
    "    Mu_hat = np.sum((Predicted_Weights[x_i])*(Barycenters_Array))\n",
    "    Mu = np.mean(np.array(measures_locations_list[x_i]))\n",
    "    Mean_errors =  np.append(Mean_errors,(Mu_hat-Mu))\n",
    "    ## Update Erros\n",
    "    predictions_mean = np.append(predictions_mean,Mu_hat)\n",
    "    true_mean = np.append(true_mean,Mu)\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat = np.sum((Barycenters_Array**2)*(Predicted_Weights[x_i]))\n",
    "    Var = np.mean(np.array(measures_locations_list[x_i])**2)\n",
    "    Var_errors = np.append(Var_errors,(Var_hat-Var)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat = np.sum((Barycenters_Array**3)*(Predicted_Weights[x_i]))\n",
    "    Skewness = np.mean(np.array(measures_locations_list[x_i])**3)\n",
    "    Skewness_errors = np.append(Skewness_errors,(abs(Skewness_hat-Skewness))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat = np.sum((Barycenters_Array**4)*(Predicted_Weights[x_i]))\n",
    "    Kurtosis = np.mean(np.array(measures_locations_list[x_i])**4)\n",
    "    Kurtosis_errors = np.append(Kurtosis_errors,(abs(Kurtosis_hat-Kurtosis))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "W1_95 = bootstrap(W1_errors, n=1000, func=np.mean)(.95)\n",
    "W1_99 = bootstrap(W1_errors, n=1000, func=np.mean)(.99)\n",
    "M_95 = bootstrap(predictions_mean, n=1000, func=np.mean)(.95)\n",
    "M_99 = bootstrap(predictions_mean, n=1000, func=np.mean)(.99)\n",
    "M_95_MC = bootstrap(true_mean, n=1000, func=np.mean)(.95)\n",
    "M_99_MC = bootstrap(true_mean, n=1000, func=np.mean)(.99)\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance = np.array([np.min(np.abs(W1_errors)),np.mean(np.abs(W1_errors)),np.max(np.abs(W1_errors))])\n",
    "Mean_prediction_Performance = np.array([np.min(np.abs(Mean_errors)),np.mean(np.abs(Mean_errors)),np.max(np.abs(Mean_errors))])\n",
    "Var_prediction_Performance = np.array([np.min(np.abs(Var_errors)),np.mean(np.abs(Var_errors)),np.max(np.abs(Var_errors))])\n",
    "Skewness_prediction_Performance = np.array([np.min(np.abs(Skewness_errors)),np.mean(np.abs(Skewness_errors)),np.max(np.abs(Skewness_errors))])\n",
    "Kurtosis_prediction_Performance = np.array([np.min(np.abs(Kurtosis_errors)),np.mean(np.abs(Kurtosis_errors)),np.max(np.abs(Kurtosis_errors))])\n",
    "\n",
    "Type_A_Prediction = pd.DataFrame({\"W1\":W1_Performance,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance},index=[\"Min\",\"MAE\",\"Max\"])\n",
    "Type_A_Predictions_and_confidence = pd.DataFrame({\"W1_99_Train\":W1_95,\n",
    "                                                  \"W1error_99_Train\":W1_99,\n",
    "                                                  \"M_95_Train\":M_95,\n",
    "                                                  \"M_99_Train\":M_99,\n",
    "                                                  \"MC_95_Train\":M_95_MC,\n",
    "                                                  \"MC_99_Train\":M_99_MC},index=[\"CL\",\"Mean\",\"CU\"])\n",
    "\n",
    "\n",
    "# Write Performance\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "Type_A_Prediction.to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Train.tex\"))\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "(Type_A_Predictions_and_confidence.T).to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Train_predictions_w_confidence_intervals.tex\"))\n",
    "\n",
    "# #---------------------------------------------------------------------------------------------#\n",
    "# # Update User\n",
    "# Type_A_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Result(s): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Test Set Performance Metrics\")\n",
    "\n",
    "# Initialize Wasserstein-1 Error Distribution\n",
    "W1_errors_test = np.array([])\n",
    "Mean_errors_test = np.array([])\n",
    "Var_errors_test = np.array([])\n",
    "Skewness_errors_test = np.array([])\n",
    "Kurtosis_errors_test = np.array([])\n",
    "# Initialize Prediction Metrics\n",
    "predictions_mean_test = np.array([])\n",
    "true_mean_test = np.array([])\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Populate Error Distribution\n",
    "for x_i in tqdm(range(len(measures_locations_test_list))):    \n",
    "    # Get Laws\n",
    "    W1_loop_test = ot.emd2_1d(Barycenters_Array,\n",
    "                         np.array(measures_locations_test_list[x_i]).reshape(-1,),\n",
    "                         Predicted_Weights_test[x_i,].reshape(-1,),\n",
    "                         (np.array(measures_weights_test_list[x_i])).reshape(-1,))\n",
    "    W1_errors_test = np.append(W1_errors_test,W1_loop_test)\n",
    "    # Get Means\n",
    "    Mu_hat_test = np.sum((Predicted_Weights_test[x_i])*(Barycenters_Array))\n",
    "    Mu_test = np.mean(np.array(measures_locations_test_list[x_i]))\n",
    "    Mean_errors_test = np.append(Mean_errors_test,(Mu_hat_test-Mu_test))\n",
    "    ## Update Predictions\n",
    "    predictions_mean_test = np.append(predictions_mean_test,Mu_hat_test)\n",
    "    true_mean_test = np.append(true_mean_test,Mu_test)\n",
    "    # Get Var (non-centered)\n",
    "    Var_hat_test = np.sum((Barycenters_Array**2)*(Predicted_Weights_test[x_i]))\n",
    "    Var_test = np.mean(np.array(measures_locations_test_list[x_i])**2)\n",
    "    Var_errors_test = np.append(Var_errors_test,(Var_hat_test-Var_test)**2)\n",
    "    # Get skewness (non-centered)\n",
    "    Skewness_hat_test = np.sum((Barycenters_Array**3)*(Predicted_Weights_test[x_i]))\n",
    "    Skewness_test = np.mean(np.array(measures_locations_test_list[x_i])**3)\n",
    "    Skewness_errors_test = np.append(Skewness_errors_test,(abs(Skewness_hat_test-Skewness_test))**(1/3))\n",
    "    # Get skewness (non-centered)\n",
    "    Kurtosis_hat_test = np.sum((Barycenters_Array**4)*(Predicted_Weights_test[x_i]))\n",
    "    Kurtosis_test = np.mean(np.array(measures_locations_test_list[x_i])**4)\n",
    "    Kurtosis_errors_test = np.append(Kurtosis_errors_test,(abs(Kurtosis_hat_test-Kurtosis_test))**.25)\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------#\n",
    "W1_95_test = bootstrap(W1_errors_test, n=1000, func=np.mean)(.95)\n",
    "W1_99_test = bootstrap(W1_errors_test, n=1000, func=np.mean)(.99)\n",
    "M_95_test = bootstrap(predictions_mean_test, n=1000, func=np.mean)(.95)\n",
    "M_99_test = bootstrap(predictions_mean_test, n=1000, func=np.mean)(.99)\n",
    "M_95_MC_test = bootstrap(true_mean_test, n=1000, func=np.mean)(.95)\n",
    "M_99_MC_test = bootstrap(true_mean_test, n=1000, func=np.mean)(.99)\n",
    "#---------------------------------------------------------------------------------------------#\n",
    "# Compute Error Statistics/Descriptors\n",
    "W1_Performance_test = np.array([np.min(np.abs(W1_errors_test)),np.mean(np.abs(W1_errors_test)),np.max(np.abs(W1_errors_test))])\n",
    "Mean_prediction_Performance_test = np.array([np.min(np.abs(Mean_errors_test)),np.mean(np.abs(Mean_errors_test)),np.max(np.abs(Mean_errors_test))])\n",
    "Var_prediction_Performance_test = np.array([np.min(np.abs(Var_errors_test)),np.mean(np.abs(Var_errors_test)),np.max(np.abs(Var_errors_test))])\n",
    "Skewness_prediction_Performance_test = np.array([np.min(np.abs(Skewness_errors_test)),np.mean(np.abs(Skewness_errors_test)),np.max(np.abs(Skewness_errors_test))])\n",
    "Kurtosis_prediction_Performance_test = np.array([np.min(np.abs(Kurtosis_errors_test)),np.mean(np.abs(Kurtosis_errors_test)),np.max(np.abs(Kurtosis_errors_test))])\n",
    "\n",
    "Type_A_Prediction_test = pd.DataFrame({\"W1\":W1_Performance_test,\n",
    "                                  \"E[X']-E[X]\":Mean_prediction_Performance_test,\n",
    "                                  \"(E[X'^2]-E[X^2])^.5\":Var_prediction_Performance_test,\n",
    "                                  \"(E[X'^3]-E[X^3])^(1/3)\":Skewness_prediction_Performance_test,\n",
    "                                  \"(E[X'^4]-E[X^4])^.25\":Kurtosis_prediction_Performance_test},index=[\"Min\",\"MAE\",\"Max\"])\n",
    "\n",
    "Type_A_Predictions_and_confidence_test = pd.DataFrame({\"W1_99_Test\":W1_95_test,\n",
    "                                                       \"W1error_99_Test\":W1_99_test,\n",
    "                                                       \"M_95_Test\":M_95_test,\n",
    "                                                       \"M_99_Test\":M_99_test,\n",
    "                                                       \"MC_95_Test\":M_95_MC_test,\n",
    "                                                       \"MC_99_Test\":M_99_MC_test},index=[\"CL\",\"Mean\",\"CU\"])\n",
    "\n",
    "# Write Performance\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "Type_A_Prediction_test.to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Test.tex\"))\n",
    "pd.set_option('display.float_format', '{:.4E}'.format)\n",
    "(Type_A_Predictions_and_confidence_test.T).to_latex((results_tables_path+str(\"Roughness_\")+str(Rougness)+str(\"__RatiofBM_\")+str(Ratio_fBM_to_typical_vol)+\n",
    " \"__TypeAPrediction_Test_predictions_w_confidence_intervals.tex\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Training-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(predictions_mean,label=\"prediction\",color=\"purple\")\n",
    "plt.plot(true_mean,label=\"true\",color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(predictions_mean_test,color=\"purple\")\n",
    "# # plt.plot(true_mean_test)\n",
    "\n",
    "# # Initialize Plot #\n",
    "# #-----------------#\n",
    "# plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# # Generate Plots #\n",
    "# #----------------#\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.plot_trisurf(X_train[:-1:,0], X_train[:-1:,1], true_mean, cmap='viridis',linewidth=0.5);\n",
    "# ax.plot_trisurf(X_train[:-1:,0], X_train[:-1:,1], predictions_mean, cmap='inferno',linewidth=0.5);\n",
    "\n",
    "\n",
    "# # Format Plot #\n",
    "# #-------------#\n",
    "# plt.legend(loc=\"upper left\",prop={'size': 10})\n",
    "# plt.title(\"Model Predictions\")\n",
    "\n",
    "# # Export #\n",
    "# #--------#\n",
    "# # SAVE Figure to .eps\n",
    "# plt.savefig('./outputs/plots/Test.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Test-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plt.plot(predictions_mean_test,color=\"purple\")\n",
    "# # plt.plot(true_mean_test)\n",
    "\n",
    "# # sns.set()\n",
    "# # Initialize Plot #\n",
    "# #-----------------#\n",
    "# plt.figure(num=None, figsize=(12, 12), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# # Generate Plots #\n",
    "# #----------------#\n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.plot_trisurf(X_test[:-1:,0], X_test[:-1:,1], true_mean_test, cmap='viridis',linewidth=0.5);\n",
    "# ax.plot_trisurf(X_train[:-1:,0], X_train[:-1:,1], predictions_mean_test, cmap='inferno',linewidth=0.5);\n",
    "\n",
    "\n",
    "# # Format Plot #\n",
    "# #-------------#\n",
    "# plt.legend(loc=\"upper left\",prop={'size': 10})\n",
    "# plt.title(\"Model Predictions\")\n",
    "\n",
    "# # Export #\n",
    "# #--------#\n",
    "# # SAVE Figure to .eps\n",
    "# plt.savefig('./outputs/plots/Test.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print for Terminal Legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#----------------------#\")\n",
    "print(\"Training-Set Performance\")\n",
    "print(\"#----------------------#\")\n",
    "print(Type_A_Prediction)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"#------------------#\")\n",
    "print(\"Test-Set Performance\")\n",
    "print(\"#------------------#\")\n",
    "print(Type_A_Prediction_test)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type_A_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type_A_Prediction_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
