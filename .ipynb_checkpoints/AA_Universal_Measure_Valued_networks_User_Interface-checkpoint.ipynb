{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Universal Regular Conditional Expectations:\n",
    "\n",
    "---\n",
    "This implements the universal deep neural model of $\\mathcal{NN}_{1_{\\mathbb{R}^n},\\mathcal{D}}^{\\sigma:\\star}$ [Anastasis Kratsios](https://people.math.ethz.ch/~kratsioa/) - 2021.\n",
    "\n",
    "---\n",
    "\n",
    "## What does this code do?\n",
    "1. Learn Heteroskedastic Non-Linear Regression Problem\n",
    "     - $Y\\sim f_{\\text{unkown}}(x) + \\epsilon$ where $f$ is an known function and $\\epsilon\\sim Laplace(0,\\|x\\|)$\n",
    "2. Learn Random Bayesian Network's Law:\n",
    "    - $Y = W_J Y^{J-1}, \\qquad Y^{j}\\triangleq \\sigma\\bullet A^{j}Y^{j-1} + b^{j}, \\qquad Y^0\\triangleq x$\n",
    "\n",
    "3. In the above example if $A_j = M_j\\odot \\tilde{A_j}$ where $\\tilde{A}_j$ is a deterministic matrix and $M_j$ is a \"mask\", that is, a random matrix with binary entries and $\\odot$ is the Hadamard product then we recover the dropout framework.\n",
    "4. Learn the probability distribution that the unique strong solution to the rough SDE with uniformly Lipschitz drivers driven by a factional Brownian motion with Hurst exponent $H \\in [\\frac1{2},1)$:\n",
    "$$\n",
    "X_t^x = x + \\int_0^t \\alpha(s,X_s^x)ds + \\int_0^t \\beta(s,X_s^x)dB_s^H\n",
    "$$\n",
    "belongs, at time $t=1$, to a ball about the initial point $x$ of random radius given by an independant exponential random-variable with shape parameter $\\lambda=2$\n",
    "5. Train a DNN to predict the returns of bitcoin with GD.  Since this has random initialization then each prediction of a given $x$ is stochastic...We learn the distribution of this conditional RV (conditioned on x in the input space).\n",
    "$$\n",
    "Y_x \\triangleq \\hat{f}_{\\theta_{T}}(x), \\qquad \\theta_{(t+1)}\\triangleq \\theta_{(t)} + \\lambda \\sum_{x \\in \\mathbb{X}} \\nabla_{\\theta}\\|\\hat{f}_{\\theta_t}(x) - f(x)\\|, \\qquad \\theta_0 \\sim N_d(0,1);\n",
    "$$\n",
    "$T\\in \\mathbb{N}$ is a fixed number of \"SGD\" iterations (typically identified by cross-validation on a single SGD trajectory for a single initialization) and where $\\theta \\in \\mathbb{R}^{(d_{J}+1)+\\sum_{j=0}^{J-1} (d_{j+1}d_j + 1)}$ and $d_j$ is the dimension of the \"bias\" vector $b_j$ defining each layer of the DNN with layer dimensions:\n",
    "$$\n",
    "\\hat{f}_{\\theta}(x)\\triangleq A^{(J)}x^{(J)} + b^{(J)},\\qquad x^{(j+1)}\\triangleq \\sigma\\bullet A^{j}x^{(j)} + b^{j},\\qquad x^{(0)}\\triangleq x\n",
    ".\n",
    "$$\n",
    "\n",
    "6. Extreme Learning Machines: \n",
    "    Just like the Bayesian network but then last layer is trained on the training set using KRidge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode:\n",
    "Software/Hardware Testing or Real-Deal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random DNN\n",
    "# f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# Random DNN internal noise\n",
    "## Real-world data version\n",
    "f_unknown_mode = \"Extreme_Learning_Machine\"\n",
    "### General Parameters\n",
    "# activation_function == 'thresholding'\n",
    "activation_function = 'sigmoid'\n",
    "### Dataset Option 1\n",
    "dataset_option = 'SnP'\n",
    "### Dataset Option 2\n",
    "# dataset_option = 'crypto'\n",
    "Depth_Bayesian_DNN = 1\n",
    "N_Random_Features = 10**3\n",
    "## Simulated Data version\n",
    "# f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "width = 10**3\n",
    "\n",
    "# Random Dropout applied to trained DNN\n",
    "# f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "Dropout_rate = 0.75\n",
    "\n",
    "# GD with Randomized Input\n",
    "# f_unknown_mode = \"GD_with_randomized_input\"\n",
    "# GD_epochs = 50\n",
    "\n",
    "# SDE with fractional Driver\n",
    "# f_unknown_mode = \"Rough_SDE\"\n",
    "N_Euler_Steps = 50\n",
    "Hurst_Exponent = 0.51\n",
    "\n",
    "# f_unknown_mode = \"Rough_SDE_Vanilla\"\n",
    "## Define Process' dynamics in (2) cell(s) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dim = 5\n",
    "if f_unknown_mode != 'Extreme_Learning_Machine':\n",
    "    width = int(2*(problem_dim+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla fractional SDE:\n",
    "If f_unknown_mode == \"Rough_SDE_Vanilla\" is selected, then we can specify the process's dynamics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------#\n",
    "# Define Process' Dynamics #\n",
    "#--------------------------#\n",
    "drift_constant = 0.1\n",
    "volatility_constant = 0.01\n",
    "\n",
    "# Define DNN Applier\n",
    "def f_unknown_drift_vanilla(x):\n",
    "#     x_internal = drift_constant*np.ones(problem_dim)\n",
    "    x_internal = drift_constant*np.sin(x)\n",
    "    return x_internal\n",
    "def f_unknown_vol_vanilla(x):\n",
    "#     x_internal = volatility_constant*np.diag(np.ones(problem_dim))\n",
    "    x_internal = volatility_constant*np.diag(np.cos(x))\n",
    "    return x_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: *Why the procedure is so computationally efficient*?\n",
    "---\n",
    " - The sample barycenters do not require us to solve for any new Wasserstein-1 Barycenters; which is much more computationally costly,\n",
    " - Our training procedure never back-propages through $\\mathcal{W}_1$ since steps 2 and 3 are full-decoupled.  Therefore, training our deep classifier is (comparatively) cheap since it takes values in the standard $N$-simplex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Hyperparameter(s)\n",
    "- Ratio $\\frac{\\text{Testing Datasize}}{\\text{Training Datasize}}$.\n",
    "- Number of Training Points to Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = .1\n",
    "N_train_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte-Carlo\n",
    "N_Monte_Carlo_Samples = 10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial radis of $\\delta$-bounded random partition of $\\mathcal{X}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters of Cover\n",
    "delta = 0.1\n",
    "Proportion_per_cluster = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Auxiliary Script(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# %run Loader.ipynb\n",
    "exec(open('Loader.py').read())\n",
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "import time as time #<- Note sure why...but its always seems to need 'its own special loading...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate or Parse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Beginning Data-Parsing/Simulation Phase\n",
      "---------------------------------------\n",
      "Deciding on Which Simulator/Parser To Load\n",
      "Setting/Defining: Internal Parameters\n",
      "Deciding on Which Type of Data to Get/Simulate\n",
      "Simulating Output Data for given input data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:10<00:00, 10.31s/it] \n",
      "100%|██████████| 10/10 [01:12<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Done Data-Parsing/Simulation Phase\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run Data_Simulator_and_Parser.ipynb\n",
    "exec(open('Data_Simulator_and_Parser.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Data\n",
    "This is especially important to avoid exploding gradient problems when training the ML-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Running script for main model!\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 9817.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "==========================================\n",
      "Training Classifer Portion of Type-A Model\n",
      "==========================================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   23.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   23.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 6.2220 - accuracy: 0.0010\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.9979 - accuracy: 0.0010\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.5355 - accuracy: 0.0110\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 5.0939 - accuracy: 0.0190\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.6604 - accuracy: 0.0270\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3238 - accuracy: 0.0340\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.0586 - accuracy: 0.0500\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.8716 - accuracy: 0.0490\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.7260 - accuracy: 0.0530\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.6048 - accuracy: 0.0620\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.5065 - accuracy: 0.0660\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.4055 - accuracy: 0.0740\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.3159 - accuracy: 0.0790\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.2419 - accuracy: 0.0860\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.1569 - accuracy: 0.0880\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 3.1117 - accuracy: 0.0990\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 3.0610 - accuracy: 0.0940\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9973 - accuracy: 0.0960\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9758 - accuracy: 0.0910\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.9315 - accuracy: 0.0960\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8961 - accuracy: 0.1130\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.8439 - accuracy: 0.1140\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7826 - accuracy: 0.1230\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7405 - accuracy: 0.1270\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.7354 - accuracy: 0.1250\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.7250 - accuracy: 0.1250\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6615 - accuracy: 0.1340\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6538 - accuracy: 0.1140\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.6047 - accuracy: 0.1290\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5887 - accuracy: 0.1550\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5734 - accuracy: 0.1450\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5472 - accuracy: 0.1470\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5144 - accuracy: 0.1460\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.5037 - accuracy: 0.1380\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4579 - accuracy: 0.1530\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.4678 - accuracy: 0.1620\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4395 - accuracy: 0.1530\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4088 - accuracy: 0.1580\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3855 - accuracy: 0.1590\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3799 - accuracy: 0.1620\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3713 - accuracy: 0.1610\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3498 - accuracy: 0.1770\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.3376 - accuracy: 0.1720\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3054 - accuracy: 0.1680\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3123 - accuracy: 0.1730\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2858 - accuracy: 0.1680\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2733 - accuracy: 0.1690\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2693 - accuracy: 0.1710\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2197 - accuracy: 0.1940\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.2200 - accuracy: 0.1680\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2190 - accuracy: 0.1770\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1947 - accuracy: 0.1940\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1659 - accuracy: 0.1890\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1715 - accuracy: 0.2010\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1603 - accuracy: 0.1850\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1303 - accuracy: 0.2100\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1404 - accuracy: 0.2040\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1098 - accuracy: 0.2220\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1195 - accuracy: 0.2000\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0958 - accuracy: 0.2020\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0823 - accuracy: 0.2140\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0713 - accuracy: 0.2080\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0616 - accuracy: 0.2030\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0590 - accuracy: 0.2050\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0636 - accuracy: 0.1960\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0324 - accuracy: 0.2240\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 2.0145 - accuracy: 0.2140\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0079 - accuracy: 0.2240\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0091 - accuracy: 0.2180\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 2.0254 - accuracy: 0.2130\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 2.0184 - accuracy: 0.2030\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.9768 - accuracy: 0.2350\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.9754 - accuracy: 0.2140\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9585 - accuracy: 0.2440\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9355 - accuracy: 0.2570\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9358 - accuracy: 0.2270\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9286 - accuracy: 0.2400\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9291 - accuracy: 0.2480\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9168 - accuracy: 0.2520\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.9032 - accuracy: 0.2400\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8937 - accuracy: 0.2680\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8823 - accuracy: 0.2550\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8771 - accuracy: 0.2340\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8754 - accuracy: 0.2400\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8812 - accuracy: 0.2540\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8552 - accuracy: 0.2670\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8533 - accuracy: 0.2390\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8338 - accuracy: 0.2940\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8457 - accuracy: 0.2540\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8343 - accuracy: 0.2720\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8085 - accuracy: 0.2640\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8338 - accuracy: 0.2570\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8046 - accuracy: 0.2860\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7863 - accuracy: 0.2860\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7956 - accuracy: 0.2700\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8057 - accuracy: 0.2730\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7917 - accuracy: 0.2980\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7779 - accuracy: 0.2770\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7810 - accuracy: 0.2830\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7332 - accuracy: 0.2890\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7409 - accuracy: 0.2950\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7756 - accuracy: 0.2600\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.7642 - accuracy: 0.2980\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7606 - accuracy: 0.2780\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7296 - accuracy: 0.2900\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7160 - accuracy: 0.2810\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7380 - accuracy: 0.2770\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7183 - accuracy: 0.3020\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6959 - accuracy: 0.3140\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6919 - accuracy: 0.3060\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6957 - accuracy: 0.3090\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6704 - accuracy: 0.3140\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7076 - accuracy: 0.2880\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.6958 - accuracy: 0.2980\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6749 - accuracy: 0.2950\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6896 - accuracy: 0.2960\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6560 - accuracy: 0.3370\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6506 - accuracy: 0.2920\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6468 - accuracy: 0.2980\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6682 - accuracy: 0.2970\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6231 - accuracy: 0.3280\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6448 - accuracy: 0.3060\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6410 - accuracy: 0.3150\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6414 - accuracy: 0.2960\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6142 - accuracy: 0.3220\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6277 - accuracy: 0.3050\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6241 - accuracy: 0.3260\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6123 - accuracy: 0.3130\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6068 - accuracy: 0.3130\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5979 - accuracy: 0.3380\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6016 - accuracy: 0.3460\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5891 - accuracy: 0.3190\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5966 - accuracy: 0.3350\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5852 - accuracy: 0.3250\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6017 - accuracy: 0.3260\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5480 - accuracy: 0.3340\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5515 - accuracy: 0.3460\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5450 - accuracy: 0.3550\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5574 - accuracy: 0.3580\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5653 - accuracy: 0.3380\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5780 - accuracy: 0.3270\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5445 - accuracy: 0.3460\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5550 - accuracy: 0.3360\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5321 - accuracy: 0.3350\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5375 - accuracy: 0.3270\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5233 - accuracy: 0.3430\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5489 - accuracy: 0.3310\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5345 - accuracy: 0.3320\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5025 - accuracy: 0.3560\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5039 - accuracy: 0.3590\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5157 - accuracy: 0.3450\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4867 - accuracy: 0.3620\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4968 - accuracy: 0.3660\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4685 - accuracy: 0.3590\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4988 - accuracy: 0.3550\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5100 - accuracy: 0.3350\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5164 - accuracy: 0.3390\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4817 - accuracy: 0.3950\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4598 - accuracy: 0.3540\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4356 - accuracy: 0.3920\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4502 - accuracy: 0.3810\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4342 - accuracy: 0.3820\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4926 - accuracy: 0.3590\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 1.5113 - accuracy: 0.3470\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4542 - accuracy: 0.3780\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4500 - accuracy: 0.3550\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4638 - accuracy: 0.3500\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4527 - accuracy: 0.3750\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4464 - accuracy: 0.3690\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4246 - accuracy: 0.3920\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4221 - accuracy: 0.3800\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4180 - accuracy: 0.3910\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4142 - accuracy: 0.3800\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4332 - accuracy: 0.3850\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3989 - accuracy: 0.3880\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4011 - accuracy: 0.3980\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4120 - accuracy: 0.3820\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3793 - accuracy: 0.3950\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3869 - accuracy: 0.3920\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3996 - accuracy: 0.3900\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3963 - accuracy: 0.3790\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.4314 - accuracy: 0.3700\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3516 - accuracy: 0.4080\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3883 - accuracy: 0.4070\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4153 - accuracy: 0.3980\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.4057 - accuracy: 0.3830\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3981 - accuracy: 0.3830\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3744 - accuracy: 0.3960\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3898 - accuracy: 0.3950\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.3556 - accuracy: 0.4080\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3700 - accuracy: 0.3980\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3384 - accuracy: 0.4170\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3520 - accuracy: 0.4060\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3317 - accuracy: 0.4300\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3338 - accuracy: 0.4060\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3244 - accuracy: 0.4270\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3053 - accuracy: 0.4390\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3458 - accuracy: 0.4140\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 1.3644 - accuracy: 0.4030\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 1.3645 - accuracy: 0.4080\n",
      "63/63 [==============================] - 0s 1ms/step\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "===============================================\n",
      "Training Classifer Portion of Type Model: Done!\n",
      "===============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------#\n",
      " Get Training Error(s)\n",
      "#--------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:55<00:00,  5.71it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "#----------------#\n",
      " Get Test Error(s)\n",
      "#----------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------------------#\n",
      " Get Testing Error(s): END\n",
      "#------------------------#\n",
      "                                         DNM  MC-Oracle\n",
      "W1-95L                           1214.463146   0.000000\n",
      "W1                               1415.753799   0.000000\n",
      "W1-95R                           1637.571751   0.000000\n",
      "M-95L                               0.388391   0.000000\n",
      "M                                   0.448332   0.000000\n",
      "M-95R                               0.500554   0.000000\n",
      "N_Par                          181500.000000   0.000000\n",
      "Train_Time                        261.456240  41.694699\n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000\n",
      "------------------------------------\n",
      "Done: Running script for main model!\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------\")\n",
    "print(\"Running script for main model!\")\n",
    "print(\"------------------------------\")\n",
    "%run Universal_Measure_Valued_Networks_Backend.ipynb\n",
    "# exec(open('Universal_Measure_Valued_Networks_Backend.py').read())\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Done: Running script for main model!\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Run: All Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) *Pointmass Benchmark(s)*\n",
    "These benchmarks consist of subsets of $C(\\mathbb{R}^d,\\mathbb{R})$ which we lift to models in $C(\\mathbb{R}^d,\\cap_{1\\leq q<\\infty}\\mathscr{P}_{q}(\\mathbb{R}))$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to f(x) \\to \\delta_{f(x)}\\in \\cap_{1\\leq q<\\infty}\\mathcal{P}_{q}(\\mathbb{R}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "--------------\n",
      "Training: ENET\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 412/1000 [00:00<00:00, 4118.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Training: ENET - Done\n",
      "---------------------\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4136.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3730.82it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0969s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.1s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET\n",
      "W1-95L                           1214.463146   0.000000            -\n",
      "W1                               1415.753799   0.000000            -\n",
      "W1-95R                           1637.571751   0.000000            -\n",
      "M-95L                               0.388391   0.000000      1033.72\n",
      "M                                   0.448332   0.000000      1033.77\n",
      "M-95R                               0.500554   0.000000      1033.82\n",
      "N_Par                          181500.000000   0.000000            4\n",
      "Train_Time                        261.456240  41.694699  1.62081e+09\n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05\n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "-----------------\n",
      "Training: K-Ridge\n",
      "-----------------\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.4s finished\n",
      " 27%|██▋       | 268/1000 [00:00<00:00, 2678.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3571.66it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3534.04it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.0369s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:    0.0s finished\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge  \n",
      "W1-95L                                  -  \n",
      "W1                                      -  \n",
      "W1-95R                                  -  \n",
      "M-95L                             1012.32  \n",
      "M                                 1031.95  \n",
      "M-95R                             1055.33  \n",
      "N_Par                                   0  \n",
      "Train_Time                       0.979093  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "--------------\n",
      "Training: GBRF\n",
      "--------------\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3679.30it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3637.45it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge        GBRF  \n",
      "W1-95L                                  -           -  \n",
      "W1                                      -           -  \n",
      "W1-95R                                  -           -  \n",
      "M-95L                             1012.32     1029.81  \n",
      "M                                 1031.95     1033.02  \n",
      "M-95R                             1055.33     1035.88  \n",
      "N_Par                                   0        4632  \n",
      "Train_Time                       0.979093    0.374792  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n",
      "-------------\n",
      "Training: DNN\n",
      "-------------\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   17.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   17.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.9170 - mse: 1366898.0000 - mae: 1054.9170 - mape: 100.0004\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.8987 - mse: 1366855.1250 - mae: 1054.8987 - mape: 100.0004\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.8795 - mse: 1366811.5000 - mae: 1054.8795 - mape: 99.9993\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.8586 - mse: 1366763.5000 - mae: 1054.8586 - mape: 99.9978\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.8357 - mse: 1366710.6250 - mae: 1054.8357 - mape: 99.9990\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.8098 - mse: 1366651.7500 - mae: 1054.8098 - mape: 99.9972\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.7810 - mse: 1366584.3750 - mae: 1054.7810 - mape: 99.9984\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.7473 - mse: 1366507.7500 - mae: 1054.7473 - mape: 99.9972\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.7086 - mse: 1366419.8750 - mae: 1054.7086 - mape: 99.9944\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.6639 - mse: 1366317.6250 - mae: 1054.6639 - mape: 99.9935\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.6123 - mse: 1366199.1250 - mae: 1054.6123 - mape: 99.9942\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.5525 - mse: 1366065.5000 - mae: 1054.5525 - mape: 99.9928\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.4832 - mse: 1365908.2500 - mae: 1054.4832 - mape: 99.9884\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1054.4028 - mse: 1365726.8750 - mae: 1054.4028 - mape: 99.9889\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.3108 - mse: 1365519.1250 - mae: 1054.3108 - mape: 99.9870\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.2054 - mse: 1365284.2500 - mae: 1054.2054 - mape: 99.9776\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1054.0848 - mse: 1365012.2500 - mae: 1054.0848 - mape: 99.9760\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1053.9474 - mse: 1364705.7500 - mae: 1053.9474 - mape: 99.9684\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1053.7919 - mse: 1364356.2500 - mae: 1053.7919 - mape: 99.9700\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1053.6166 - mse: 1363963.1250 - mae: 1053.6166 - mape: 99.9601\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1053.4198 - mse: 1363527.6250 - mae: 1053.4198 - mape: 99.9483\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1053.1996 - mse: 1363040.0000 - mae: 1053.1996 - mape: 99.9404\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1052.9547 - mse: 1362489.7500 - mae: 1052.9547 - mape: 99.9226\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1052.6830 - mse: 1361891.2500 - mae: 1052.6830 - mape: 99.9209\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1052.3811 - mse: 1361226.8750 - mae: 1052.3811 - mape: 99.9088\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1052.0496 - mse: 1360494.1250 - mae: 1052.0496 - mape: 99.8867\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1051.6832 - mse: 1359689.1250 - mae: 1051.6832 - mape: 99.8783\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1051.2815 - mse: 1358801.6250 - mae: 1051.2815 - mape: 99.8549\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.8418 - mse: 1357835.3750 - mae: 1050.8418 - mape: 99.8407\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1050.3627 - mse: 1356780.0000 - mae: 1050.3627 - mape: 99.8173\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1049.8413 - mse: 1355647.3750 - mae: 1049.8413 - mape: 99.7835\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1049.2762 - mse: 1354413.1250 - mae: 1049.2762 - mape: 99.7659\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1048.6639 - mse: 1353079.5000 - mae: 1048.6639 - mape: 99.7393\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1048.0024 - mse: 1351636.5000 - mae: 1048.0024 - mape: 99.7135\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1047.2930 - mse: 1350092.3750 - mae: 1047.2930 - mape: 99.7058\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1046.5308 - mse: 1348447.3750 - mae: 1046.5308 - mape: 99.6371\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1045.7169 - mse: 1346682.6250 - mae: 1045.7169 - mape: 99.5970\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1044.8512 - mse: 1344810.3750 - mae: 1044.8512 - mape: 99.5631\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1043.9287 - mse: 1342812.6250 - mae: 1043.9287 - mape: 99.5580\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1042.9542 - mse: 1340732.2500 - mae: 1042.9542 - mape: 99.4486\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1041.9249 - mse: 1338518.8750 - mae: 1041.9249 - mape: 99.4289\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1040.8433 - mse: 1336187.7500 - mae: 1040.8433 - mape: 99.4249\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1039.7083 - mse: 1333739.7500 - mae: 1039.7083 - mape: 99.3321\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1038.5199 - mse: 1331200.1250 - mae: 1038.5199 - mape: 99.3342\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1037.2743 - mse: 1328512.2500 - mae: 1037.2743 - mape: 99.2611\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1035.9744 - mse: 1325735.7500 - mae: 1035.9744 - mape: 99.2122\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1034.6204 - mse: 1322858.8750 - mae: 1034.6204 - mape: 99.2355\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1033.2139 - mse: 1319848.8750 - mae: 1033.2139 - mape: 99.1443\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1031.7550 - mse: 1316755.0000 - mae: 1031.7550 - mape: 99.1026\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1030.2427 - mse: 1313496.7500 - mae: 1030.2427 - mape: 99.0691\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1028.6741 - mse: 1310170.3750 - mae: 1028.6741 - mape: 99.1134\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1027.0503 - mse: 1306718.2500 - mae: 1027.0503 - mape: 98.9792\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1025.3707 - mse: 1303192.5000 - mae: 1025.3707 - mape: 98.9897\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1023.6335 - mse: 1299488.0000 - mae: 1023.6335 - mape: 98.9648\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1021.8400 - mse: 1295720.8750 - mae: 1021.8400 - mape: 98.9458\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1019.9863 - mse: 1291814.6250 - mae: 1019.9863 - mape: 98.8317\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1018.0784 - mse: 1287807.7500 - mae: 1018.0784 - mape: 98.8207\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1016.1039 - mse: 1283662.5000 - mae: 1016.1039 - mape: 98.8609\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1014.0693 - mse: 1279410.2500 - mae: 1014.0693 - mape: 98.7977\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 1011.9818 - mse: 1275078.3750 - mae: 1011.9818 - mape: 98.6961\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1009.8154 - mse: 1270573.5000 - mae: 1009.8154 - mape: 98.7193\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1007.5956 - mse: 1265957.7500 - mae: 1007.5956 - mape: 98.6596\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1005.2993 - mse: 1261200.2500 - mae: 1005.2993 - mape: 98.7206\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1002.9391 - mse: 1256339.3750 - mae: 1002.9391 - mape: 98.5495\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1000.5051 - mse: 1251397.3750 - mae: 1000.5051 - mape: 98.5510\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 997.9938 - mse: 1246236.8750 - mae: 997.9938 - mape: 98.5062\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 995.4174 - mse: 1240967.5000 - mae: 995.4174 - mape: 98.4429\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 992.7638 - mse: 1235517.0000 - mae: 992.7638 - mape: 98.4991\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 990.0353 - mse: 1230079.2500 - mae: 990.0353 - mape: 98.4303\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 987.2263 - mse: 1224409.6250 - mae: 987.2263 - mape: 98.4278\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 984.3337 - mse: 1218636.6250 - mae: 984.3337 - mape: 98.2765\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 981.3707 - mse: 1212747.0000 - mae: 981.3707 - mape: 98.4301\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 978.3290 - mse: 1206670.0000 - mae: 978.3290 - mape: 98.3010\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 975.1935 - mse: 1200448.7500 - mae: 975.1935 - mape: 98.1754\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 971.9734 - mse: 1194097.2500 - mae: 971.9734 - mape: 98.2007\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 968.6716 - mse: 1187636.5000 - mae: 968.6716 - mape: 98.1220\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 965.2637 - mse: 1180987.8750 - mae: 965.2637 - mape: 98.2121\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 961.7952 - mse: 1174243.3750 - mae: 961.7952 - mape: 98.0293\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 958.2131 - mse: 1167317.1250 - mae: 958.2131 - mape: 98.0836\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 954.5607 - mse: 1160315.3750 - mae: 954.5607 - mape: 98.1221\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 950.8120 - mse: 1153082.0000 - mae: 950.8120 - mape: 97.9716\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 946.9690 - mse: 1145814.7500 - mae: 946.9690 - mape: 97.9828\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 943.0217 - mse: 1138364.1250 - mae: 943.0217 - mape: 97.7800\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 938.9972 - mse: 1130810.3750 - mae: 938.9972 - mape: 97.7988\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 934.8641 - mse: 1123000.5000 - mae: 934.8641 - mape: 97.7549\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 930.6147 - mse: 1115149.0000 - mae: 930.6147 - mape: 97.7600\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 926.2751 - mse: 1107166.8750 - mae: 926.2751 - mape: 97.8051\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 921.8333 - mse: 1098980.0000 - mae: 921.8333 - mape: 97.5370\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 917.2835 - mse: 1090662.6250 - mae: 917.2835 - mape: 97.6132\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 912.6331 - mse: 1082293.5000 - mae: 912.6331 - mape: 97.6020\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 907.8618 - mse: 1073746.0000 - mae: 907.8618 - mape: 97.5251\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 902.9996 - mse: 1065082.8750 - mae: 902.9996 - mape: 97.3116\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 898.0317 - mse: 1056269.0000 - mae: 898.0317 - mape: 97.1900\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 892.9501 - mse: 1047366.6250 - mae: 892.9501 - mape: 97.1079\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 887.7938 - mse: 1038259.8750 - mae: 887.7938 - mape: 97.2910\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 882.4821 - mse: 1029253.5000 - mae: 882.4821 - mape: 97.1330\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 877.0723 - mse: 1019888.3750 - mae: 877.0723 - mape: 96.9294\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 871.5617 - mse: 1010545.8750 - mae: 871.5617 - mape: 96.9237\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 865.9350 - mse: 1001057.4375 - mae: 865.9350 - mape: 96.9861\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 860.1922 - mse: 991455.2500 - mae: 860.1922 - mape: 96.7475\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 854.3364 - mse: 981802.0625 - mae: 854.3364 - mape: 97.0247\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 848.3469 - mse: 972045.8125 - mae: 848.3469 - mape: 96.8296\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 842.2697 - mse: 962112.7500 - mae: 842.2697 - mape: 96.5884\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 836.0705 - mse: 952181.6250 - mae: 836.0705 - mape: 96.6918\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 829.7812 - mse: 942228.5000 - mae: 829.7812 - mape: 96.4997\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 823.3811 - mse: 932109.1250 - mae: 823.3811 - mape: 96.4779\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 816.8759 - mse: 922048.1250 - mae: 816.8759 - mape: 96.5264\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 810.2260 - mse: 911764.4375 - mae: 810.2260 - mape: 96.5178\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 803.4469 - mse: 901483.4375 - mae: 803.4469 - mape: 96.4561\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 796.5700 - mse: 891098.1250 - mae: 796.5700 - mape: 96.4550\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 789.5652 - mse: 880878.5625 - mae: 789.5652 - mape: 96.3531\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 782.4514 - mse: 870436.0625 - mae: 782.4514 - mape: 96.3020\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 775.1923 - mse: 859953.7500 - mae: 775.1923 - mape: 96.2902\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 767.8318 - mse: 849349.8750 - mae: 767.8318 - mape: 96.2958\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 760.3276 - mse: 838934.8125 - mae: 760.3276 - mape: 96.2623\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 752.6829 - mse: 828391.2500 - mae: 752.6829 - mape: 95.9461\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 744.9324 - mse: 817820.6875 - mae: 744.9324 - mape: 95.7984\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 737.0608 - mse: 807249.0000 - mae: 737.0608 - mape: 95.7240\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 729.1403 - mse: 796827.1250 - mae: 729.1403 - mape: 95.9327\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 721.1652 - mse: 786410.5625 - mae: 721.1652 - mape: 95.7258\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 713.1576 - mse: 775995.0625 - mae: 713.1576 - mape: 95.7522\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 705.0505 - mse: 765592.2500 - mae: 705.0505 - mape: 95.8599\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 696.9655 - mse: 755175.2500 - mae: 696.9655 - mape: 95.7852\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 688.9368 - mse: 745224.6875 - mae: 688.9368 - mape: 95.9369\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 681.0467 - mse: 735278.9375 - mae: 681.0467 - mape: 95.8460\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 673.2246 - mse: 725309.3125 - mae: 673.2246 - mape: 96.0825\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 665.5892 - mse: 715569.8750 - mae: 665.5892 - mape: 96.3534\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 658.1934 - mse: 706117.8125 - mae: 658.1934 - mape: 96.1486\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 651.0925 - mse: 696726.6250 - mae: 651.0925 - mape: 96.4567\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 644.3935 - mse: 687863.0000 - mae: 644.3935 - mape: 96.7455\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 638.3718 - mse: 679048.9375 - mae: 638.3718 - mape: 96.8325\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 632.6686 - mse: 670762.6875 - mae: 632.6686 - mape: 97.6044\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 627.1683 - mse: 662442.5000 - mae: 627.1683 - mape: 97.7289\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 621.8774 - mse: 654284.1875 - mae: 621.8774 - mape: 98.1999\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 616.7521 - mse: 646467.1250 - mae: 616.7521 - mape: 98.8268\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 611.7722 - mse: 638618.8750 - mae: 611.7722 - mape: 99.2285\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 606.9448 - mse: 630993.1250 - mae: 606.9448 - mape: 99.9277\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 602.2198 - mse: 623151.0625 - mae: 602.2198 - mape: 100.2185\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 597.5602 - mse: 615598.2500 - mae: 597.5602 - mape: 101.1469\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 593.0226 - mse: 608243.8750 - mae: 593.0226 - mape: 101.4268\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 588.5862 - mse: 600707.4375 - mae: 588.5862 - mape: 102.2937\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 584.2537 - mse: 593531.8750 - mae: 584.2537 - mape: 102.6864\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 580.2908 - mse: 586713.5625 - mae: 580.2908 - mape: 103.4960\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 576.6595 - mse: 580063.3750 - mae: 576.6595 - mape: 104.4899\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 573.2008 - mse: 573558.2500 - mae: 573.2008 - mape: 105.1634\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 569.7864 - mse: 567339.7500 - mae: 569.7864 - mape: 105.9096\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 566.4717 - mse: 560897.8750 - mae: 566.4717 - mape: 106.8410\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 563.3264 - mse: 554771.3125 - mae: 563.3264 - mape: 107.7893\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 560.3565 - mse: 548998.5000 - mae: 560.3565 - mape: 108.5778\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 557.5002 - mse: 543211.6875 - mae: 557.5002 - mape: 109.1644\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 554.7403 - mse: 537685.7500 - mae: 554.7403 - mape: 110.2882\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 552.0264 - mse: 532070.2500 - mae: 552.0264 - mape: 111.2342\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 549.2866 - mse: 526682.3125 - mae: 549.2866 - mape: 111.9664\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 546.5783 - mse: 521115.9688 - mae: 546.5783 - mape: 113.0577\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 543.8640 - mse: 515723.8438 - mae: 543.8640 - mape: 113.9153\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 541.1664 - mse: 510200.8125 - mae: 541.1664 - mape: 114.7308\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 538.4789 - mse: 504943.8750 - mae: 538.4789 - mape: 115.9433\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 535.7419 - mse: 499326.3750 - mae: 535.7419 - mape: 116.9260\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 533.0627 - mse: 494095.8125 - mae: 533.0627 - mape: 117.9535\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 530.3983 - mse: 488715.0000 - mae: 530.3983 - mape: 118.9739\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 527.7990 - mse: 483447.9062 - mae: 527.7990 - mape: 119.9293\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 525.2347 - mse: 478344.3750 - mae: 525.2347 - mape: 121.1868\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 522.7005 - mse: 473305.9375 - mae: 522.7005 - mape: 122.2357\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 520.1343 - mse: 468325.7188 - mae: 520.1343 - mape: 123.1020\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 517.6344 - mse: 463453.5625 - mae: 517.6344 - mape: 124.6266\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 515.1779 - mse: 458286.8750 - mae: 515.1779 - mape: 125.4668\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 512.7799 - mse: 453752.4062 - mae: 512.7799 - mape: 126.7176\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 510.4482 - mse: 449156.5938 - mae: 510.4482 - mape: 127.6091\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 508.2072 - mse: 444807.0625 - mae: 508.2072 - mape: 128.9759\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 506.1709 - mse: 440493.5312 - mae: 506.1709 - mape: 130.2121\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 504.3419 - mse: 436761.5625 - mae: 504.3419 - mape: 131.0292\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 502.6476 - mse: 433340.1562 - mae: 502.6476 - mape: 131.9877\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 500.9691 - mse: 429724.0625 - mae: 500.9691 - mape: 133.2524\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 499.3544 - mse: 426255.5625 - mae: 499.3544 - mape: 134.1539\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 497.8200 - mse: 422970.6875 - mae: 497.8200 - mape: 135.1397\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 496.3439 - mse: 419674.7812 - mae: 496.3439 - mape: 136.1497\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 494.9698 - mse: 416552.2500 - mae: 494.9698 - mape: 137.0868\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 493.7214 - mse: 413686.1875 - mae: 493.7214 - mape: 138.0812\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 492.4777 - mse: 410672.7500 - mae: 492.4777 - mape: 139.0854\n",
      "Epoch 180/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 491.2756 - mse: 408088.0625 - mae: 491.2756 - mape: 140.0202\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 490.0860 - mse: 405414.0938 - mae: 490.0860 - mape: 140.8280\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 488.9209 - mse: 402788.3438 - mae: 488.9209 - mape: 141.8735\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 487.8232 - mse: 400041.5312 - mae: 487.8232 - mape: 142.7639\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 486.6114 - mse: 397584.7500 - mae: 486.6114 - mape: 143.8314\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 485.4982 - mse: 395225.6875 - mae: 485.4982 - mape: 144.5961\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 484.3964 - mse: 392659.0000 - mae: 484.3964 - mape: 145.6020\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 483.2719 - mse: 390280.5938 - mae: 483.2719 - mape: 146.7302\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 482.1408 - mse: 388051.1875 - mae: 482.1408 - mape: 147.3525\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 481.0984 - mse: 385741.0938 - mae: 481.0984 - mape: 148.4435\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 480.0516 - mse: 383449.2500 - mae: 480.0516 - mape: 149.4087\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 479.0082 - mse: 381384.1250 - mae: 479.0082 - mape: 150.5279\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 477.9736 - mse: 379187.7812 - mae: 477.9736 - mape: 151.2652\n",
      "Epoch 193/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 476.9679 - mse: 377305.1562 - mae: 476.9679 - mape: 151.9548\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 475.9699 - mse: 375295.4688 - mae: 475.9699 - mape: 153.1047\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 475.0038 - mse: 373326.5938 - mae: 475.0038 - mape: 154.0513\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 474.0687 - mse: 371603.7812 - mae: 474.0687 - mape: 154.8720\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 473.1191 - mse: 369665.3438 - mae: 473.1191 - mape: 155.7132\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 472.1447 - mse: 367893.1875 - mae: 472.1447 - mape: 156.6134\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 471.2213 - mse: 365977.8125 - mae: 471.2213 - mape: 157.6916\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 470.3073 - mse: 364341.8750 - mae: 470.3073 - mape: 158.5754\n",
      "63/63 [==============================] - 0s 615us/step\n",
      "7/7 [==============================] - 0s 932us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3737.61it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 3649.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "#------------#\n",
      " Get Error(s) \n",
      "#------------#\n",
      "#-----------------#\n",
      " Get Error(s): END \n",
      "#-----------------#\n",
      "Updated DataFrame\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge        GBRF        DNN  \n",
      "W1-95L                                  -           -          -  \n",
      "W1                                      -           -          -  \n",
      "W1-95R                                  -           -          -  \n",
      "M-95L                             1012.32     1029.81    845.188  \n",
      "M                                 1031.95     1033.02    859.328  \n",
      "M-95R                             1055.33     1035.88    873.883  \n",
      "N_Par                                   0        4632      81201  \n",
      "Train_Time                       0.979093    0.374792    40.0515  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exec(open('CV_Grid.py').read())\n",
    "# Notebook Mode:\n",
    "# %run Evaluation.ipynb\n",
    "# %run Benchmarks_Model_Builder_Pointmass_Based.ipynb\n",
    "# Terminal Mode (Default):\n",
    "exec(open('Evaluation.py').read())\n",
    "exec(open('Benchmarks_Model_Builder_Pointmass_Based.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Point-Mass Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge        GBRF        DNN  \n",
      "W1-95L                                  -           -          -  \n",
      "W1                                      -           -          -  \n",
      "W1-95R                                  -           -          -  \n",
      "M-95L                             1012.32     1029.81    845.188  \n",
      "M                                 1031.95     1033.02    859.328  \n",
      "M-95R                             1055.33     1035.88    873.883  \n",
      "N_Par                                   0        4632      81201  \n",
      "Train_Time                       0.979093    0.374792    40.0515  \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>1214.463146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>1415.753799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>1637.571751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.388391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.72</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>1029.81</td>\n",
       "      <td>845.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.448332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.77</td>\n",
       "      <td>1031.95</td>\n",
       "      <td>1033.02</td>\n",
       "      <td>859.328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.500554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.82</td>\n",
       "      <td>1055.33</td>\n",
       "      <td>1035.88</td>\n",
       "      <td>873.883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>181500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4632</td>\n",
       "      <td>81201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>261.456240</td>\n",
       "      <td>41.694699</td>\n",
       "      <td>1.62081e+09</td>\n",
       "      <td>0.979093</td>\n",
       "      <td>0.374792</td>\n",
       "      <td>40.0515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.020070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.53532e-05</td>\n",
       "      <td>0.00190444</td>\n",
       "      <td>8.2907e-05</td>\n",
       "      <td>0.0129377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle         ENET  \\\n",
       "W1-95L                           1214.463146   0.000000            -   \n",
       "W1                               1415.753799   0.000000            -   \n",
       "W1-95R                           1637.571751   0.000000            -   \n",
       "M-95L                               0.388391   0.000000      1033.72   \n",
       "M                                   0.448332   0.000000      1033.77   \n",
       "M-95R                               0.500554   0.000000      1033.82   \n",
       "N_Par                          181500.000000   0.000000            4   \n",
       "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
       "\n",
       "                                   KRidge        GBRF        DNN  \n",
       "W1-95L                                  -           -          -  \n",
       "W1                                      -           -          -  \n",
       "W1-95R                                  -           -          -  \n",
       "M-95L                             1012.32     1029.81    845.188  \n",
       "M                                 1031.95     1033.02    859.328  \n",
       "M-95R                             1055.33     1035.88    873.883  \n",
       "N_Par                                   0        4632      81201  \n",
       "Train_Time                       0.979093    0.374792    40.0515  \n",
       "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Model Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                           1214.463146   0.000000  1.069317e+06   \n",
      "W1                               1415.753799   0.000000  1.069550e+06   \n",
      "W1-95R                           1637.571751   0.000000  1.069714e+06   \n",
      "M-95L                               0.388391   0.000000  1.033716e+03   \n",
      "M                                   0.448332   0.000000  1.033771e+03   \n",
      "M-95R                               0.500554   0.000000  1.033818e+03   \n",
      "N_Par                          181500.000000   0.000000  4.000000e+00   \n",
      "Train_Time                        261.456240  41.694699  1.620805e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.535316e-05   \n",
      "\n",
      "                                      KRidge           GBRF            DNN  \n",
      "W1-95L                         559600.334876  935988.346719  620719.227009  \n",
      "W1                             571237.862840  944404.197712  677916.434108  \n",
      "W1-95R                         580900.880499  954848.233050  718973.281049  \n",
      "M-95L                             744.742760     963.427472     761.152848  \n",
      "M                                 754.034038     970.744419     795.049301  \n",
      "M-95R                             761.306098     977.069192     831.838885  \n",
      "N_Par                               0.000000    4632.000000   81201.000000  \n",
      "Train_Time                          0.979093       0.374792      40.051524  \n",
      "Test_Time/MC-Oracle_Test_Time       0.001904       0.000083       0.012938  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>1214.463146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069317e+06</td>\n",
       "      <td>559600.334876</td>\n",
       "      <td>935988.346719</td>\n",
       "      <td>620719.227009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>1415.753799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069550e+06</td>\n",
       "      <td>571237.862840</td>\n",
       "      <td>944404.197712</td>\n",
       "      <td>677916.434108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>1637.571751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069714e+06</td>\n",
       "      <td>580900.880499</td>\n",
       "      <td>954848.233050</td>\n",
       "      <td>718973.281049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.388391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033716e+03</td>\n",
       "      <td>744.742760</td>\n",
       "      <td>963.427472</td>\n",
       "      <td>761.152848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.448332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033771e+03</td>\n",
       "      <td>754.034038</td>\n",
       "      <td>970.744419</td>\n",
       "      <td>795.049301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.500554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033818e+03</td>\n",
       "      <td>761.306098</td>\n",
       "      <td>977.069192</td>\n",
       "      <td>831.838885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>181500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4632.000000</td>\n",
       "      <td>81201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>261.456240</td>\n",
       "      <td>41.694699</td>\n",
       "      <td>1.620805e+09</td>\n",
       "      <td>0.979093</td>\n",
       "      <td>0.374792</td>\n",
       "      <td>40.051524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.020070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.535316e-05</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.012938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                           1214.463146   0.000000  1.069317e+06   \n",
       "W1                               1415.753799   0.000000  1.069550e+06   \n",
       "W1-95R                           1637.571751   0.000000  1.069714e+06   \n",
       "M-95L                               0.388391   0.000000  1.033716e+03   \n",
       "M                                   0.448332   0.000000  1.033771e+03   \n",
       "M-95R                               0.500554   0.000000  1.033818e+03   \n",
       "N_Par                          181500.000000   0.000000  4.000000e+00   \n",
       "Train_Time                        261.456240  41.694699  1.620805e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.535316e-05   \n",
       "\n",
       "                                      KRidge           GBRF            DNN  \n",
       "W1-95L                         559600.334876  935988.346719  620719.227009  \n",
       "W1                             571237.862840  944404.197712  677916.434108  \n",
       "W1-95R                         580900.880499  954848.233050  718973.281049  \n",
       "M-95L                             744.742760     963.427472     761.152848  \n",
       "M                                 754.034038     970.744419     795.049301  \n",
       "M-95R                             761.306098     977.069192     831.838885  \n",
       "N_Par                               0.000000    4632.000000   81201.000000  \n",
       "Train_Time                          0.979093       0.374792      40.051524  \n",
       "Test_Time/MC-Oracle_Test_Time       0.001904       0.000083       0.012938  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) *Gaussian Benchmarks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bencharm 1: [Gaussian Process Regressor](https://scikit-learn.org/stable/modules/gaussian_process.html)\n",
    "- Benchmark 2: Deep Gaussian Networks:\n",
    "These models train models which assume Gaussianity.  We may view these as models in $\\mathcal{P}_2(\\mathbb{R})$ via:\n",
    "$$\n",
    "\\mathbb{R}^d \\ni x \\to (\\hat{\\mu}(x),\\hat{\\Sigma}(x)\\hat{\\Sigma}^{\\top})\\triangleq f(x) \\in \\mathbb{R}\\times [0,\\infty) \\to \n",
    "(2\\pi)^{-\\frac{d}{2}}\\det(\\hat{\\Sigma}(x))^{-\\frac{1}{2}} \\, e^{ -\\frac{1}{2}(\\cdot - \\hat{\\mu}(x))^{{{\\!\\mathsf{T}}}} \\hat{\\Sigma}(x)^{-1}(\\cdot - \\hat{\\mu}(x)) } \\mu \\in \\mathcal{G}_d\\subset \\mathcal{P}_2(\\mathbb{R});\n",
    "$$\n",
    "where $\\mathcal{G}_1$ is the set of Gaussian measures on $\\mathbb{R}$ equipped with the relative Wasserstein-1 topology.\n",
    "\n",
    "Examples of this type of architecture are especially prevalent in uncertainty quantification; see ([Deep Ensembles](https://arxiv.org/abs/1612.01474)] or [NOMU: Neural Optimization-based Model Uncertainty](https://arxiv.org/abs/2102.13640).  Moreover, their universality in $C(\\mathbb{R}^d,\\mathcal{G}_2)$ is known, and has been shown in [Corollary 4.7](https://arxiv.org/abs/2101.05390)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.4s finished\n",
      "  0%|          | 2/1000 [00:00<00:54, 18.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering Parameters for Deep Gaussian Network to train on!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 45.90it/s]\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Getting Parameters for Deep Gaussian Network!\n",
      "===============================\n",
      "Training Deep Gaussian Network!\n",
      "===============================\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   19.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   2 | elapsed:   19.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.7375 - mse: 401.9442 - mae: 11.7375 - mape: 100.1577\n",
      "Epoch 2/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.7192 - mse: 401.4547 - mae: 11.7192 - mape: 100.5472\n",
      "Epoch 3/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.6996 - mse: 400.9362 - mae: 11.6996 - mape: 100.8055\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.6780 - mse: 400.3608 - mae: 11.6780 - mape: 101.1475\n",
      "Epoch 5/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.6536 - mse: 399.7065 - mae: 11.6536 - mape: 103.0769\n",
      "Epoch 6/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.6256 - mse: 398.9366 - mae: 11.6256 - mape: 103.4883\n",
      "Epoch 7/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.5935 - mse: 398.0435 - mae: 11.5935 - mape: 106.0059\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.5559 - mse: 396.9656 - mae: 11.5559 - mape: 107.8328\n",
      "Epoch 9/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.5109 - mse: 395.6497 - mae: 11.5109 - mape: 109.3518\n",
      "Epoch 10/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.4569 - mse: 394.0322 - mae: 11.4569 - mape: 112.7295\n",
      "Epoch 11/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.3922 - mse: 392.0131 - mae: 11.3922 - mape: 118.0222\n",
      "Epoch 12/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.3146 - mse: 389.4664 - mae: 11.3146 - mape: 123.3547\n",
      "Epoch 13/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.2210 - mse: 386.2097 - mae: 11.2210 - mape: 128.3522\n",
      "Epoch 14/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 11.1085 - mse: 382.0221 - mae: 11.1085 - mape: 139.8083\n",
      "Epoch 15/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 10.9887 - mse: 376.7250 - mae: 10.9887 - mape: 152.8078\n",
      "Epoch 16/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 10.8556 - mse: 369.7359 - mae: 10.8556 - mape: 164.4355\n",
      "Epoch 17/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 10.6798 - mse: 359.7610 - mae: 10.6798 - mape: 188.2320\n",
      "Epoch 18/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 10.4377 - mse: 344.8527 - mae: 10.4377 - mape: 214.4777\n",
      "Epoch 19/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 10.0730 - mse: 320.6928 - mae: 10.0730 - mape: 257.0174\n",
      "Epoch 20/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 9.4475 - mse: 277.0811 - mae: 9.4475 - mape: 306.6948\n",
      "Epoch 21/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 7.9732 - mse: 186.3688 - mae: 7.9732 - mape: 378.8570\n",
      "Epoch 22/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 4.3432 - mse: 49.7254 - mae: 4.3432 - mape: 485.3961\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.9392 - mse: 15.6812 - mae: 2.9392 - mape: 511.0666\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.5103 - mse: 9.3014 - mae: 2.5103 - mape: 514.5093\n",
      "Epoch 25/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.4134 - mse: 8.0198 - mae: 2.4134 - mape: 518.8925\n",
      "Epoch 26/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3923 - mse: 7.8831 - mae: 2.3923 - mape: 522.6557\n",
      "Epoch 27/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3838 - mse: 7.8710 - mae: 2.3838 - mape: 525.5938\n",
      "Epoch 28/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3764 - mse: 7.8138 - mae: 2.3764 - mape: 527.0968\n",
      "Epoch 29/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3678 - mse: 7.8216 - mae: 2.3678 - mape: 528.3920\n",
      "Epoch 30/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3692 - mse: 7.8612 - mae: 2.3692 - mape: 528.6235\n",
      "Epoch 31/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3644 - mse: 7.6577 - mae: 2.3644 - mape: 526.6447\n",
      "Epoch 32/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3555 - mse: 7.6640 - mae: 2.3555 - mape: 528.6804\n",
      "Epoch 33/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3533 - mse: 7.7238 - mae: 2.3533 - mape: 528.9677\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3505 - mse: 7.5639 - mae: 2.3505 - mape: 526.9720\n",
      "Epoch 35/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3456 - mse: 7.6218 - mae: 2.3456 - mape: 528.0719\n",
      "Epoch 36/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3390 - mse: 7.5235 - mae: 2.3390 - mape: 527.8712\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3395 - mse: 7.5459 - mae: 2.3395 - mape: 528.0787\n",
      "Epoch 38/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3382 - mse: 7.6301 - mae: 2.3382 - mape: 528.9437\n",
      "Epoch 39/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3326 - mse: 7.5232 - mae: 2.3326 - mape: 527.7385\n",
      "Epoch 40/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3295 - mse: 7.5062 - mae: 2.3295 - mape: 527.3238\n",
      "Epoch 41/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3266 - mse: 7.5217 - mae: 2.3266 - mape: 527.8956\n",
      "Epoch 42/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3257 - mse: 7.4558 - mae: 2.3257 - mape: 526.7480\n",
      "Epoch 43/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3142 - mse: 7.3423 - mae: 2.3142 - mape: 525.9416\n",
      "Epoch 44/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3153 - mse: 7.4094 - mae: 2.3153 - mape: 526.4383\n",
      "Epoch 45/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3107 - mse: 7.3777 - mae: 2.3107 - mape: 527.1048\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3051 - mse: 7.2633 - mae: 2.3051 - mape: 525.9979\n",
      "Epoch 47/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3040 - mse: 7.3184 - mae: 2.3040 - mape: 527.0340\n",
      "Epoch 48/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3048 - mse: 7.3769 - mae: 2.3048 - mape: 526.9955\n",
      "Epoch 49/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2956 - mse: 7.2632 - mae: 2.2956 - mape: 526.1679\n",
      "Epoch 50/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2946 - mse: 7.3379 - mae: 2.2946 - mape: 525.4243\n",
      "Epoch 51/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2950 - mse: 7.2700 - mae: 2.2950 - mape: 525.2697\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2905 - mse: 7.1707 - mae: 2.2905 - mape: 524.2995\n",
      "Epoch 53/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2810 - mse: 7.2171 - mae: 2.2810 - mape: 525.3173\n",
      "Epoch 54/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2832 - mse: 7.2199 - mae: 2.2832 - mape: 525.4266\n",
      "Epoch 55/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2773 - mse: 7.1314 - mae: 2.2773 - mape: 525.3618\n",
      "Epoch 56/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2736 - mse: 7.0733 - mae: 2.2736 - mape: 525.0541\n",
      "Epoch 57/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2678 - mse: 7.1152 - mae: 2.2678 - mape: 525.7065\n",
      "Epoch 58/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2666 - mse: 7.0574 - mae: 2.2666 - mape: 525.0262\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2652 - mse: 7.0192 - mae: 2.2652 - mape: 524.8313\n",
      "Epoch 60/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2635 - mse: 7.0749 - mae: 2.2635 - mape: 523.8843\n",
      "Epoch 61/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2582 - mse: 6.9536 - mae: 2.2582 - mape: 523.8915\n",
      "Epoch 62/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2526 - mse: 7.0425 - mae: 2.2526 - mape: 524.6733\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2516 - mse: 6.8972 - mae: 2.2516 - mape: 523.5020\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2467 - mse: 6.9378 - mae: 2.2467 - mape: 524.0535\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2408 - mse: 6.8280 - mae: 2.2408 - mape: 522.9078\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.2400 - mse: 6.8864 - mae: 2.2400 - mape: 523.1110\n",
      "Epoch 67/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2400 - mse: 6.8726 - mae: 2.2400 - mape: 523.2786\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2344 - mse: 6.8441 - mae: 2.2344 - mape: 522.0871\n",
      "Epoch 69/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2278 - mse: 6.8205 - mae: 2.2278 - mape: 523.0106\n",
      "Epoch 70/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2270 - mse: 6.7709 - mae: 2.2270 - mape: 522.2084\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2211 - mse: 6.7565 - mae: 2.2211 - mape: 522.4023\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2212 - mse: 6.8930 - mae: 2.2212 - mape: 522.9700\n",
      "Epoch 73/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2136 - mse: 6.6722 - mae: 2.2136 - mape: 521.2144\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2073 - mse: 6.6568 - mae: 2.2073 - mape: 521.5800\n",
      "Epoch 75/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2061 - mse: 6.6180 - mae: 2.2061 - mape: 521.5573\n",
      "Epoch 76/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2045 - mse: 6.7298 - mae: 2.2045 - mape: 522.6044\n",
      "Epoch 77/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1954 - mse: 6.5264 - mae: 2.1954 - mape: 520.7248\n",
      "Epoch 78/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1927 - mse: 6.6742 - mae: 2.1927 - mape: 521.7745\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1868 - mse: 6.4902 - mae: 2.1868 - mape: 520.6326\n",
      "Epoch 80/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1889 - mse: 6.5339 - mae: 2.1889 - mape: 520.8232\n",
      "Epoch 81/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1802 - mse: 6.5075 - mae: 2.1802 - mape: 521.0660\n",
      "Epoch 82/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.1857 - mse: 6.5104 - mae: 2.1857 - mape: 519.5233\n",
      "Epoch 83/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1808 - mse: 6.4801 - mae: 2.1808 - mape: 520.2872\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1736 - mse: 6.4775 - mae: 2.1736 - mape: 519.8632\n",
      "Epoch 85/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1695 - mse: 6.4549 - mae: 2.1695 - mape: 520.5446\n",
      "Epoch 86/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1621 - mse: 6.3661 - mae: 2.1621 - mape: 519.0197\n",
      "Epoch 87/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1638 - mse: 6.4534 - mae: 2.1638 - mape: 519.2177\n",
      "Epoch 88/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1584 - mse: 6.3472 - mae: 2.1584 - mape: 518.6851\n",
      "Epoch 89/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1512 - mse: 6.3184 - mae: 2.1512 - mape: 519.2343\n",
      "Epoch 90/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1476 - mse: 6.3100 - mae: 2.1476 - mape: 519.6921\n",
      "Epoch 91/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1451 - mse: 6.2044 - mae: 2.1451 - mape: 518.3508\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1417 - mse: 6.2408 - mae: 2.1417 - mape: 518.0952\n",
      "Epoch 93/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1352 - mse: 6.2231 - mae: 2.1352 - mape: 518.6964\n",
      "Epoch 94/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1322 - mse: 6.2442 - mae: 2.1322 - mape: 518.8186\n",
      "Epoch 95/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1287 - mse: 6.1593 - mae: 2.1287 - mape: 517.6582\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1283 - mse: 6.1347 - mae: 2.1283 - mape: 517.0732\n",
      "Epoch 97/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1206 - mse: 6.0854 - mae: 2.1206 - mape: 516.8749\n",
      "Epoch 98/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1181 - mse: 6.1699 - mae: 2.1181 - mape: 518.2485\n",
      "Epoch 99/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1129 - mse: 6.0637 - mae: 2.1129 - mape: 517.6569\n",
      "Epoch 100/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1047 - mse: 6.0084 - mae: 2.1047 - mape: 516.1925\n",
      "Epoch 101/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1019 - mse: 6.0343 - mae: 2.1019 - mape: 516.8121\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0971 - mse: 5.9698 - mae: 2.0971 - mape: 515.5153\n",
      "Epoch 103/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0932 - mse: 5.9734 - mae: 2.0932 - mape: 515.6031\n",
      "Epoch 104/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0899 - mse: 5.9347 - mae: 2.0899 - mape: 516.0528\n",
      "Epoch 105/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0855 - mse: 5.8890 - mae: 2.0855 - mape: 516.1800\n",
      "Epoch 106/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0774 - mse: 5.8548 - mae: 2.0774 - mape: 515.5706\n",
      "Epoch 107/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0793 - mse: 5.8603 - mae: 2.0793 - mape: 515.3091\n",
      "Epoch 108/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0710 - mse: 5.8476 - mae: 2.0710 - mape: 515.5820\n",
      "Epoch 109/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0640 - mse: 5.8010 - mae: 2.0640 - mape: 515.2618\n",
      "Epoch 110/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0678 - mse: 5.8447 - mae: 2.0678 - mape: 515.2222\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0568 - mse: 5.7590 - mae: 2.0568 - mape: 514.2115\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0493 - mse: 5.6779 - mae: 2.0493 - mape: 513.4586\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0487 - mse: 5.7281 - mae: 2.0487 - mape: 513.9274\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0429 - mse: 5.6805 - mae: 2.0429 - mape: 514.0440\n",
      "Epoch 115/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0421 - mse: 5.6742 - mae: 2.0421 - mape: 513.7600\n",
      "Epoch 116/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0350 - mse: 5.6111 - mae: 2.0350 - mape: 513.1409\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0299 - mse: 5.5540 - mae: 2.0299 - mape: 512.2740\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0278 - mse: 5.5473 - mae: 2.0278 - mape: 511.9164\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0157 - mse: 5.6042 - mae: 2.0157 - mape: 513.7403\n",
      "Epoch 120/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0134 - mse: 5.5373 - mae: 2.0134 - mape: 512.5003\n",
      "Epoch 121/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0083 - mse: 5.4453 - mae: 2.0083 - mape: 511.5277\n",
      "Epoch 122/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 2.0047 - mse: 5.4548 - mae: 2.0047 - mape: 512.0169\n",
      "Epoch 123/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9961 - mse: 5.3830 - mae: 1.9961 - mape: 510.8525\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9922 - mse: 5.4185 - mae: 1.9922 - mape: 511.3679\n",
      "Epoch 125/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9870 - mse: 5.4082 - mae: 1.9870 - mape: 511.6449\n",
      "Epoch 126/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9819 - mse: 5.3278 - mae: 1.9819 - mape: 510.8527\n",
      "Epoch 127/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9772 - mse: 5.2823 - mae: 1.9772 - mape: 510.0750\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9725 - mse: 5.2873 - mae: 1.9725 - mape: 510.4743\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9708 - mse: 5.2731 - mae: 1.9708 - mape: 509.2828\n",
      "Epoch 130/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9571 - mse: 5.1851 - mae: 1.9571 - mape: 509.6579\n",
      "Epoch 131/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9533 - mse: 5.1944 - mae: 1.9533 - mape: 509.4765\n",
      "Epoch 132/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9485 - mse: 5.1990 - mae: 1.9485 - mape: 509.0207\n",
      "Epoch 133/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9446 - mse: 5.1256 - mae: 1.9446 - mape: 509.1353\n",
      "Epoch 134/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9389 - mse: 5.1176 - mae: 1.9389 - mape: 508.8774\n",
      "Epoch 135/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9329 - mse: 5.0803 - mae: 1.9329 - mape: 509.3358\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9276 - mse: 5.0688 - mae: 1.9276 - mape: 508.6911\n",
      "Epoch 137/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9203 - mse: 5.0440 - mae: 1.9203 - mape: 508.4889\n",
      "Epoch 138/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9167 - mse: 4.9734 - mae: 1.9167 - mape: 506.5787\n",
      "Epoch 139/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9046 - mse: 4.9234 - mae: 1.9046 - mape: 507.2433\n",
      "Epoch 140/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9005 - mse: 4.9287 - mae: 1.9005 - mape: 507.6776\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8952 - mse: 4.9034 - mae: 1.8952 - mape: 506.8021\n",
      "Epoch 142/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8893 - mse: 4.8497 - mae: 1.8893 - mape: 505.9190\n",
      "Epoch 143/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8832 - mse: 4.8489 - mae: 1.8832 - mape: 505.8456\n",
      "Epoch 144/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8751 - mse: 4.7772 - mae: 1.8751 - mape: 505.4887\n",
      "Epoch 145/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8694 - mse: 4.7591 - mae: 1.8694 - mape: 505.6925\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8620 - mse: 4.6965 - mae: 1.8620 - mape: 504.2801\n",
      "Epoch 147/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8550 - mse: 4.7137 - mae: 1.8550 - mape: 504.6698\n",
      "Epoch 148/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8493 - mse: 4.6356 - mae: 1.8493 - mape: 503.7868\n",
      "Epoch 149/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8399 - mse: 4.6132 - mae: 1.8399 - mape: 504.0454\n",
      "Epoch 150/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8352 - mse: 4.5965 - mae: 1.8352 - mape: 503.2838\n",
      "Epoch 151/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8283 - mse: 4.5921 - mae: 1.8283 - mape: 502.8817\n",
      "Epoch 152/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8253 - mse: 4.5313 - mae: 1.8253 - mape: 502.2140\n",
      "Epoch 153/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8122 - mse: 4.4897 - mae: 1.8122 - mape: 502.2185\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8055 - mse: 4.4564 - mae: 1.8055 - mape: 501.7423\n",
      "Epoch 155/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7982 - mse: 4.4383 - mae: 1.7982 - mape: 502.2120\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7894 - mse: 4.3849 - mae: 1.7894 - mape: 501.2521\n",
      "Epoch 157/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7851 - mse: 4.3943 - mae: 1.7851 - mape: 501.9927\n",
      "Epoch 158/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7743 - mse: 4.3326 - mae: 1.7743 - mape: 500.7113\n",
      "Epoch 159/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7677 - mse: 4.2987 - mae: 1.7677 - mape: 500.8442\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7678 - mse: 4.3084 - mae: 1.7678 - mape: 499.8276\n",
      "Epoch 161/200\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 1.7510 - mse: 4.2084 - mae: 1.7510 - mape: 498.7226\n",
      "Epoch 162/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7507 - mse: 4.2203 - mae: 1.7507 - mape: 498.3812\n",
      "Epoch 163/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7367 - mse: 4.1594 - mae: 1.7367 - mape: 498.4900\n",
      "Epoch 164/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7320 - mse: 4.1356 - mae: 1.7320 - mape: 497.8731\n",
      "Epoch 165/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7198 - mse: 4.1012 - mae: 1.7198 - mape: 497.8338\n",
      "Epoch 166/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7060 - mse: 4.0550 - mae: 1.7060 - mape: 497.5606\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7014 - mse: 4.0078 - mae: 1.7014 - mape: 496.4089\n",
      "Epoch 168/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6918 - mse: 3.9815 - mae: 1.6918 - mape: 496.5407\n",
      "Epoch 169/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6849 - mse: 3.9637 - mae: 1.6849 - mape: 496.4916\n",
      "Epoch 170/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6733 - mse: 3.9021 - mae: 1.6733 - mape: 494.9845\n",
      "Epoch 171/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6644 - mse: 3.8675 - mae: 1.6644 - mape: 494.6823\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6560 - mse: 3.8467 - mae: 1.6560 - mape: 494.2685\n",
      "Epoch 173/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6503 - mse: 3.8156 - mae: 1.6503 - mape: 494.4099\n",
      "Epoch 174/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6369 - mse: 3.7772 - mae: 1.6369 - mape: 493.2211\n",
      "Epoch 175/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6321 - mse: 3.7605 - mae: 1.6321 - mape: 492.7816\n",
      "Epoch 176/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6175 - mse: 3.6984 - mae: 1.6175 - mape: 492.2692\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6059 - mse: 3.6561 - mae: 1.6059 - mape: 491.7846\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5979 - mse: 3.6180 - mae: 1.5979 - mape: 491.3416\n",
      "Epoch 179/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5913 - mse: 3.6165 - mae: 1.5913 - mape: 491.1683\n",
      "Epoch 180/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5774 - mse: 3.5461 - mae: 1.5774 - mape: 489.9517\n",
      "Epoch 181/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5706 - mse: 3.5286 - mae: 1.5706 - mape: 489.2896\n",
      "Epoch 182/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5618 - mse: 3.4987 - mae: 1.5618 - mape: 489.1492\n",
      "Epoch 183/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5450 - mse: 3.4474 - mae: 1.5450 - mape: 488.7682\n",
      "Epoch 184/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5350 - mse: 3.4093 - mae: 1.5350 - mape: 488.1939\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5259 - mse: 3.3726 - mae: 1.5259 - mape: 486.8684\n",
      "Epoch 186/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5165 - mse: 3.3434 - mae: 1.5165 - mape: 486.6416\n",
      "Epoch 187/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5053 - mse: 3.3166 - mae: 1.5053 - mape: 486.5712\n",
      "Epoch 188/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4946 - mse: 3.2646 - mae: 1.4946 - mape: 485.4899\n",
      "Epoch 189/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4796 - mse: 3.2304 - mae: 1.4796 - mape: 484.7762\n",
      "Epoch 190/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4707 - mse: 3.2125 - mae: 1.4707 - mape: 484.9440\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4673 - mse: 3.1750 - mae: 1.4673 - mape: 484.0392\n",
      "Epoch 192/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4535 - mse: 3.1297 - mae: 1.4535 - mape: 482.7201\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4407 - mse: 3.1068 - mae: 1.4407 - mape: 481.9124\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4298 - mse: 3.0762 - mae: 1.4298 - mape: 481.7091\n",
      "Epoch 195/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4164 - mse: 3.0318 - mae: 1.4164 - mape: 481.0110\n",
      "Epoch 196/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4051 - mse: 2.9979 - mae: 1.4051 - mape: 480.3904\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3966 - mse: 2.9765 - mae: 1.3966 - mape: 479.3772\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3833 - mse: 2.9457 - mae: 1.3833 - mape: 478.9330\n",
      "Epoch 199/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3730 - mse: 2.9187 - mae: 1.3730 - mape: 478.7442\n",
      "Epoch 200/200\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.3670 - mse: 2.8956 - mae: 1.3670 - mape: 477.2701\n",
      "63/63 [==============================] - 0s 726us/step\n",
      "7/7 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Training Deep Gaussian Network!: END\n",
      "====================================\n",
      "#---------------------------------------#\n",
      " Get Training Errors for: Gaussian Models\n",
      "#---------------------------------------#\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1367.31it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1267.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "#--------------------------------------#\n",
      " Get Testing Errors for: Gaussian Models\n",
      "#--------------------------------------#\n",
      "#-------------------------#\n",
      " Get Training Error(s): END\n",
      "#-------------------------#\n",
      "-------------------------------------------------\n",
      "Updating Performance Metrics Dataframe and Saved!\n",
      "-------------------------------------------------\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge        GBRF        DNN  \\\n",
      "W1-95L                                  -           -          -   \n",
      "W1                                      -           -          -   \n",
      "W1-95R                                  -           -          -   \n",
      "M-95L                             1012.32     1029.81    845.188   \n",
      "M                                 1031.95     1033.02    859.328   \n",
      "M-95R                             1055.33     1035.88    873.883   \n",
      "N_Par                                   0        4632      81201   \n",
      "Train_Time                       0.979093    0.374792    40.0515   \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         1.312783e+06    554.462998  \n",
      "W1                             1.365065e+06    620.431901  \n",
      "W1-95R                         1.425543e+06    792.035201  \n",
      "M-95L                          1.025159e+03      2.300499  \n",
      "M                              1.053884e+03      2.561677  \n",
      "M-95R                          1.087146e+03      2.781415  \n",
      "N_Par                          0.000000e+00  81201.000000  \n",
      "Train_Time                     3.558847e+01     44.929648  \n",
      "Test_Time/MC-Oracle_Test_Time  2.706415e-02      0.013243  \n",
      "------------------------------------------------\n",
      "Updated Performance Metrics Dataframe and Saved!\n",
      "------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %run Benchmarks_Model_Builder_Mean_Var.ipynb\n",
    "exec(open('Benchmarks_Model_Builder_Mean_Var.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Quality (Updated): Test\n",
      "                                         DNM  MC-Oracle          ENET  \\\n",
      "W1-95L                           1214.463146   0.000000  1.069317e+06   \n",
      "W1                               1415.753799   0.000000  1.069550e+06   \n",
      "W1-95R                           1637.571751   0.000000  1.069714e+06   \n",
      "M-95L                               0.388391   0.000000  1.033716e+03   \n",
      "M                                   0.448332   0.000000  1.033771e+03   \n",
      "M-95R                               0.500554   0.000000  1.033818e+03   \n",
      "N_Par                          181500.000000   0.000000  4.000000e+00   \n",
      "Train_Time                        261.456240  41.694699  1.620805e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.535316e-05   \n",
      "\n",
      "                                      KRidge           GBRF            DNN  \\\n",
      "W1-95L                         559600.334876  935988.346719  620719.227009   \n",
      "W1                             571237.862840  944404.197712  677916.434108   \n",
      "W1-95R                         580900.880499  954848.233050  718973.281049   \n",
      "M-95L                             744.742760     963.427472     761.152848   \n",
      "M                                 754.034038     970.744419     795.049301   \n",
      "M-95R                             761.306098     977.069192     831.838885   \n",
      "N_Par                               0.000000    4632.000000   81201.000000   \n",
      "Train_Time                          0.979093       0.374792      40.051524   \n",
      "Test_Time/MC-Oracle_Test_Time       0.001904       0.000083       0.012938   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         65796.993153    466.508161  \n",
      "W1                             78425.161808    620.431901  \n",
      "W1-95R                         92228.338539    792.035201  \n",
      "M-95L                            217.027898      2.300499  \n",
      "M                                242.422895      2.561677  \n",
      "M-95R                            265.653035      2.781415  \n",
      "N_Par                              0.000000  81201.000000  \n",
      "Train_Time                        35.588466     44.929648  \n",
      "Test_Time/MC-Oracle_Test_Time      0.027064      0.013243  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "      <th>GPR</th>\n",
       "      <th>DGN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>1214.463146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069317e+06</td>\n",
       "      <td>559600.334876</td>\n",
       "      <td>935988.346719</td>\n",
       "      <td>620719.227009</td>\n",
       "      <td>65796.993153</td>\n",
       "      <td>466.508161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>1415.753799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069550e+06</td>\n",
       "      <td>571237.862840</td>\n",
       "      <td>944404.197712</td>\n",
       "      <td>677916.434108</td>\n",
       "      <td>78425.161808</td>\n",
       "      <td>620.431901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>1637.571751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.069714e+06</td>\n",
       "      <td>580900.880499</td>\n",
       "      <td>954848.233050</td>\n",
       "      <td>718973.281049</td>\n",
       "      <td>92228.338539</td>\n",
       "      <td>792.035201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.388391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033716e+03</td>\n",
       "      <td>744.742760</td>\n",
       "      <td>963.427472</td>\n",
       "      <td>761.152848</td>\n",
       "      <td>217.027898</td>\n",
       "      <td>2.300499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.448332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033771e+03</td>\n",
       "      <td>754.034038</td>\n",
       "      <td>970.744419</td>\n",
       "      <td>795.049301</td>\n",
       "      <td>242.422895</td>\n",
       "      <td>2.561677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.500554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.033818e+03</td>\n",
       "      <td>761.306098</td>\n",
       "      <td>977.069192</td>\n",
       "      <td>831.838885</td>\n",
       "      <td>265.653035</td>\n",
       "      <td>2.781415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>181500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4632.000000</td>\n",
       "      <td>81201.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>261.456240</td>\n",
       "      <td>41.694699</td>\n",
       "      <td>1.620805e+09</td>\n",
       "      <td>0.979093</td>\n",
       "      <td>0.374792</td>\n",
       "      <td>40.051524</td>\n",
       "      <td>35.588466</td>\n",
       "      <td>44.929648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.020070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.535316e-05</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.012938</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.013243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle          ENET  \\\n",
       "W1-95L                           1214.463146   0.000000  1.069317e+06   \n",
       "W1                               1415.753799   0.000000  1.069550e+06   \n",
       "W1-95R                           1637.571751   0.000000  1.069714e+06   \n",
       "M-95L                               0.388391   0.000000  1.033716e+03   \n",
       "M                                   0.448332   0.000000  1.033771e+03   \n",
       "M-95R                               0.500554   0.000000  1.033818e+03   \n",
       "N_Par                          181500.000000   0.000000  4.000000e+00   \n",
       "Train_Time                        261.456240  41.694699  1.620805e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.535316e-05   \n",
       "\n",
       "                                      KRidge           GBRF            DNN  \\\n",
       "W1-95L                         559600.334876  935988.346719  620719.227009   \n",
       "W1                             571237.862840  944404.197712  677916.434108   \n",
       "W1-95R                         580900.880499  954848.233050  718973.281049   \n",
       "M-95L                             744.742760     963.427472     761.152848   \n",
       "M                                 754.034038     970.744419     795.049301   \n",
       "M-95R                             761.306098     977.069192     831.838885   \n",
       "N_Par                               0.000000    4632.000000   81201.000000   \n",
       "Train_Time                          0.979093       0.374792      40.051524   \n",
       "Test_Time/MC-Oracle_Test_Time       0.001904       0.000083       0.012938   \n",
       "\n",
       "                                        GPR           DGN  \n",
       "W1-95L                         65796.993153    466.508161  \n",
       "W1                             78425.161808    620.431901  \n",
       "W1-95R                         92228.338539    792.035201  \n",
       "M-95L                            217.027898      2.300499  \n",
       "M                                242.422895      2.561677  \n",
       "M-95R                            265.653035      2.781415  \n",
       "N_Par                              0.000000  81201.000000  \n",
       "Train_Time                        35.588466     44.929648  \n",
       "Test_Time/MC-Oracle_Test_Time      0.027064      0.013243  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prediction Quality (Updated): Test\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "Summary_pred_Qual_models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Quality (Updated): Train\n",
      "                                         DNM  MC-Oracle         ENET  \\\n",
      "W1-95L                           1214.463146   0.000000            -   \n",
      "W1                               1415.753799   0.000000            -   \n",
      "W1-95R                           1637.571751   0.000000            -   \n",
      "M-95L                               0.388391   0.000000      1033.72   \n",
      "M                                   0.448332   0.000000      1033.77   \n",
      "M-95R                               0.500554   0.000000      1033.82   \n",
      "N_Par                          181500.000000   0.000000            4   \n",
      "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
      "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
      "\n",
      "                                   KRidge        GBRF        DNN  \\\n",
      "W1-95L                                  -           -          -   \n",
      "W1                                      -           -          -   \n",
      "W1-95R                                  -           -          -   \n",
      "M-95L                             1012.32     1029.81    845.188   \n",
      "M                                 1031.95     1033.02    859.328   \n",
      "M-95R                             1055.33     1035.88    873.883   \n",
      "N_Par                                   0        4632      81201   \n",
      "Train_Time                       0.979093    0.374792    40.0515   \n",
      "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377   \n",
      "\n",
      "                                        GPR           DGN  \n",
      "W1-95L                         1.312783e+06    554.462998  \n",
      "W1                             1.365065e+06    620.431901  \n",
      "W1-95R                         1.425543e+06    792.035201  \n",
      "M-95L                          1.025159e+03      2.300499  \n",
      "M                              1.053884e+03      2.561677  \n",
      "M-95R                          1.087146e+03      2.781415  \n",
      "N_Par                          0.000000e+00  81201.000000  \n",
      "Train_Time                     3.558847e+01     44.929648  \n",
      "Test_Time/MC-Oracle_Test_Time  2.706415e-02      0.013243  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNM</th>\n",
       "      <th>MC-Oracle</th>\n",
       "      <th>ENET</th>\n",
       "      <th>KRidge</th>\n",
       "      <th>GBRF</th>\n",
       "      <th>DNN</th>\n",
       "      <th>GPR</th>\n",
       "      <th>DGN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W1-95L</th>\n",
       "      <td>1214.463146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.312783e+06</td>\n",
       "      <td>554.462998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1</th>\n",
       "      <td>1415.753799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.365065e+06</td>\n",
       "      <td>620.431901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W1-95R</th>\n",
       "      <td>1637.571751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.425543e+06</td>\n",
       "      <td>792.035201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95L</th>\n",
       "      <td>0.388391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.72</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>1029.81</td>\n",
       "      <td>845.188</td>\n",
       "      <td>1.025159e+03</td>\n",
       "      <td>2.300499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.448332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.77</td>\n",
       "      <td>1031.95</td>\n",
       "      <td>1033.02</td>\n",
       "      <td>859.328</td>\n",
       "      <td>1.053884e+03</td>\n",
       "      <td>2.561677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M-95R</th>\n",
       "      <td>0.500554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1033.82</td>\n",
       "      <td>1055.33</td>\n",
       "      <td>1035.88</td>\n",
       "      <td>873.883</td>\n",
       "      <td>1.087146e+03</td>\n",
       "      <td>2.781415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N_Par</th>\n",
       "      <td>181500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4632</td>\n",
       "      <td>81201</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>81201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Time</th>\n",
       "      <td>261.456240</td>\n",
       "      <td>41.694699</td>\n",
       "      <td>1.62081e+09</td>\n",
       "      <td>0.979093</td>\n",
       "      <td>0.374792</td>\n",
       "      <td>40.0515</td>\n",
       "      <td>3.558847e+01</td>\n",
       "      <td>44.929648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test_Time/MC-Oracle_Test_Time</th>\n",
       "      <td>0.020070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.53532e-05</td>\n",
       "      <td>0.00190444</td>\n",
       "      <td>8.2907e-05</td>\n",
       "      <td>0.0129377</td>\n",
       "      <td>2.706415e-02</td>\n",
       "      <td>0.013243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         DNM  MC-Oracle         ENET  \\\n",
       "W1-95L                           1214.463146   0.000000            -   \n",
       "W1                               1415.753799   0.000000            -   \n",
       "W1-95R                           1637.571751   0.000000            -   \n",
       "M-95L                               0.388391   0.000000      1033.72   \n",
       "M                                   0.448332   0.000000      1033.77   \n",
       "M-95R                               0.500554   0.000000      1033.82   \n",
       "N_Par                          181500.000000   0.000000            4   \n",
       "Train_Time                        261.456240  41.694699  1.62081e+09   \n",
       "Test_Time/MC-Oracle_Test_Time       0.020070   1.000000  1.53532e-05   \n",
       "\n",
       "                                   KRidge        GBRF        DNN  \\\n",
       "W1-95L                                  -           -          -   \n",
       "W1                                      -           -          -   \n",
       "W1-95R                                  -           -          -   \n",
       "M-95L                             1012.32     1029.81    845.188   \n",
       "M                                 1031.95     1033.02    859.328   \n",
       "M-95R                             1055.33     1035.88    873.883   \n",
       "N_Par                                   0        4632      81201   \n",
       "Train_Time                       0.979093    0.374792    40.0515   \n",
       "Test_Time/MC-Oracle_Test_Time  0.00190444  8.2907e-05  0.0129377   \n",
       "\n",
       "                                        GPR           DGN  \n",
       "W1-95L                         1.312783e+06    554.462998  \n",
       "W1                             1.365065e+06    620.431901  \n",
       "W1-95R                         1.425543e+06    792.035201  \n",
       "M-95L                          1.025159e+03      2.300499  \n",
       "M                              1.053884e+03      2.561677  \n",
       "M-95R                          1.087146e+03      2.781415  \n",
       "N_Par                          0.000000e+00  81201.000000  \n",
       "Train_Time                     3.558847e+01     44.929648  \n",
       "Test_Time/MC-Oracle_Test_Time  2.706415e-02      0.013243  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Prediction Quality (Updated): Train\")\n",
    "print(Summary_pred_Qual_models)\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) The natural Universal Benchmark: [Bishop's Mixture Density Network](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)\n",
    "\n",
    "This implementation is as follows:\n",
    "- For every $x$ in the trainingdata-set we fit a GMM $\\hat{\\nu}_x$, using the [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm), with the same number of centers as the deep neural model in $\\mathcal{NN}_{1_{\\mathbb{R}^d},\\mathcal{D}}^{\\sigma:\\star}$ which we are evaluating.  \n",
    "- A Mixture density network is then trained to predict the infered parameters; given any $x \\in \\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Preparing Training Outputs for MDNs using EM-Algorithm\n",
      "======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 277/1000 [02:59<07:47,  1.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1ec106463759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# %run Mixture_Density_Network.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mixture_Density_Network.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# fit_predict(X) are always consistent with fit(X).predict(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# for any value of max_iter and tol (and any random_state).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mof\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \"\"\"\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_prob_resp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_resp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_estimate_log_prob_resp\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mlogarithm\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresponsibilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mweighted_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_weighted_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mlog_prob_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_log_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_base.py\u001b[0m in \u001b[0;36m_estimate_weighted_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mweighted_log_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_component\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \"\"\"\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         return _estimate_log_gaussian_prob(\n\u001b[0;32m--> 681\u001b[0;31m             X, self.means_, self.precisions_cholesky_, self.covariance_type)\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_estimate_log_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/mixture/_gaussian_mixture.py\u001b[0m in \u001b[0;36m_estimate_log_gaussian_prob\u001b[0;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions_chol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_chol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mlog_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcovariance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tied'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if output_dim == 1:\n",
    "    # %run Mixture_Density_Network.ipynb\n",
    "    exec(open('Mixture_Density_Network.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Final Outputs\n",
    "Now we piece together all the numerical experiments and report a nice summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Final Results\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prasing Quality Metric Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing Saving\n",
    "**Note:** *We do it in two steps since the grid sometimes does not want to write nicely...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Performance Metrics\n",
    "### Incase caption breaks\n",
    "Summary_pred_Qual_models.to_latex((results_tables_path+\"/Final_Results/\"+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"),\n",
    "                                 float_format=\"{:0.3g}\".format)\n",
    "text_file = open((results_tables_path+\"/Final_Results/\"+\"ZZZ_CAPTION_Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS___CAPTION.tex\"), \"w\")\n",
    "text_file.write(\"Quality Metrics; d:\"+str(problem_dim)+\", D:\"+str(output_dim)+\", Depth:\"+str(Depth_Bayesian_DNN)+\", Width:\"+str(width)+\", Dropout rate:\"+str(Dropout_rate)+\".\")\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "### Incase caption does not break\n",
    "Summary_pred_Qual_models.to_latex((results_tables_path+\"/Final_Results/\"+\"Performance_metrics_Problem_Type_\"+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"),\n",
    "                                 caption=(\"Quality Metrics; d:\"+str(problem_dim)+\", D:\"+str(output_dim)+\", Depth:\"+str(Depth_Bayesian_DNN)+\", Width:\"+str(width)+\", Dropout rate:\"+str(Dropout_rate)+\".\"),\n",
    "                                 float_format=\"{:0.3g}\".format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Terminal Runner(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Terminal Running\n",
    "print(\"===================\")\n",
    "print(\"Predictive Quality:\")\n",
    "print(\"===================\")\n",
    "print(Summary_pred_Qual_models)\n",
    "print(\"===================\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Kernel_Used_in_GPR: \"+str(GPR_trash.kernel))\n",
    "print(\"🙃🙃 Have a wonderful day! 🙃🙃\")\n",
    "Summary_pred_Qual_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
