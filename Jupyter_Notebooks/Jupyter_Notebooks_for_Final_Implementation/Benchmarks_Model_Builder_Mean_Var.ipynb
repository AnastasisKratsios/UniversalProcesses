{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Model(s)\n",
    "\n",
    "**Note:** *NB, this means that this script *must* be run after the point-mass benchmarks script!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### DEGUBBING MENU:\n",
    "# trial_run = True\n",
    "# N_train_size= 20\n",
    "# train_test_ratio = .5\n",
    "# N_Monte_Carlo_Samples = 10**3\n",
    "# # # Random DNN\n",
    "# # f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# # # Random DNN internal noise\n",
    "# f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "# Depth_Bayesian_DNN = 2\n",
    "# width = 50\n",
    "\n",
    "# # # Random Dropout applied to trained DNN\n",
    "# # f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "# Dropout_rate = 0.1\n",
    "\n",
    "# # GD with Randomized Input\n",
    "# # f_unknown_mode = \"GD_with_randomized_input\"\n",
    "# GD_epochs = 100\n",
    "\n",
    "# # SDE with fractional Driver\n",
    "# # f_unknown_mode = \"Rough_SDE\"\n",
    "# N_Euler_Steps = 10**1\n",
    "# Hurst_Exponent = 0.5\n",
    "# problem_dim = 3\n",
    "\n",
    "# # Hyper-parameters of Cover\n",
    "# delta = 0.01\n",
    "# Proportion_per_cluster = .75\n",
    "\n",
    "# # %run Loader.ipynb\n",
    "# exec(open('Loader.py').read())\n",
    "# # Load Packages/Modules\n",
    "# exec(open('Init_Dump.py').read())\n",
    "# import time as time #<- Note sure why...but its always seems to need 'its own special loading...'\n",
    "\n",
    "# # %run Data_Simulator_and_Parser.ipynb\n",
    "# exec(open('Data_Simulator_and_Parser.py').read())\n",
    "\n",
    "# print(\"------------------------------\")\n",
    "# print(\"Running script for main model!\")\n",
    "# print(\"------------------------------\")\n",
    "# # %run Universal_Measure_Valued_Networks_Backend.ipynb\n",
    "# exec(open('Universal_Measure_Valued_Networks_Backend.py').read())\n",
    "\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"Done: Running script for main model!\")\n",
    "# print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GPR(X_train_in,X_test_in,y_means_in):\n",
    "    # Initialize Cross-Vlidator of GPR\n",
    "    CV_GPR = RandomizedSearchCV(estimator=GaussianProcessRegressor(),\n",
    "                                n_jobs=n_jobs,\n",
    "                                cv=KFold(2, random_state=2020, shuffle=True),\n",
    "                                param_distributions=param_grid_GAUSSIAN,\n",
    "                                n_iter=n_iter,\n",
    "                                return_train_score=True,\n",
    "                                random_state=2021,\n",
    "                                verbose=10)\n",
    "\n",
    "    CV_GPR.fit(X_train,Y_train_mean_emp)\n",
    "    # Get Best Model\n",
    "    best_GPR = CV_GPR.best_estimator_\n",
    "\n",
    "    # Get Training-Set Prediction\n",
    "    GPR_means = best_GPR.predict(X_train,return_std=True)[0]\n",
    "    GPR_vars = (best_GPR.predict(X_train,return_std=True)[1])**2\n",
    "\n",
    "    # Get Test-Set Predictions\n",
    "    GPR_test_time_prediction = time.time()\n",
    "    GPR_means_test = best_GPR.predict(X_test,return_std=True)[0]\n",
    "    GPR_vars_test = (best_GPR.predict(X_test,return_std=True)[1])**2\n",
    "    GPR_test_time_prediction = time.time() - GPR_test_time_prediction\n",
    "    \n",
    "    # Return Trained Predictions + Model\n",
    "    GPR_means_test = best_GPR.predict(X_test,return_std=True)[0]\n",
    "    return GPR_means,GPR_vars, GPR_means_test, GPR_vars_test, best_GPR, GPR_test_time_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Gaussian DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps $\\varrho:\\mathbb{R}^d\\ni \\to (\\hat{\\mu},\\sigma)\\in \\mathbb{R}\\times (0,\\infty)$.  \n",
    "\n",
    "Implictly:\n",
    "$\n",
    "\\rho:\\mathbb{R}^d\\ni \\to \\nu\\circ \\varrho(x)\\in \\mathcal{P}_2(\\mathbb{R})\n",
    ".\n",
    "$\n",
    "\n",
    "The universal approximation theorem for this architecture is given in [Corollary 7: Quantitative Rates and Fundamental Obstructions to Non-EuclideanUniversal Approximation with Deep Narrow Feed-Forward Networks](https://arxiv.org/pdf/2101.05390.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1a64b37a734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Affine Readout post-composed with UAP-preserving readout map to G_d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mGaussian_Splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dim' is not defined"
     ]
    }
   ],
   "source": [
    "if output_dim == 1:\n",
    "    # Affine Readout post-composed with UAP-preserving readout map to G_d\n",
    "    class Gaussian_Splitter(tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, units=16, input_dim=32):\n",
    "            super(Gaussian_Splitter, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                     shape=(input_shape[-1], self.units),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True)\n",
    "            self.b = self.add_weight(name='bias_ffNN',\n",
    "                                     shape=(self.units,),\n",
    "                                   initializer='random_normal',\n",
    "                                   trainable=True)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            parameters = tf.matmul(inputs, self.w) + self.b\n",
    "            mean_and_cov = tf.concat([parameters,tf.math.exp(parameters)],-1)\n",
    "            return mean_and_cov\n",
    "else:\n",
    "    # Affine Readout post-composed with UAP-preserving readout map to G_d\n",
    "    class Gaussian_Splitter(tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, units=16, input_dim=32):\n",
    "            super(Gaussian_Splitter, self).__init__()\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.w = self.add_weight(name='Weights_ffNN',\n",
    "                                     shape=(input_shape[-1], self.units),\n",
    "                                     initializer='random_normal',\n",
    "                                     trainable=True)\n",
    "            self.b = self.add_weight(name='bias_ffNN',\n",
    "                                     shape=(self.units,),\n",
    "                                     initializer='random_normal',\n",
    "                                     trainable=True)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            parameters = tf.matmul(inputs, self.w) + self.b\n",
    "            mean_and_cov = tf.concat([parameters,tf.math.exp(parameters)],-1)\n",
    "            return mean_and_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dim == 1:\n",
    "    def get_ffNN_Gaussian(height, depth, learning_rate, input_dim, output_dim):\n",
    "        #----------------------------#\n",
    "        # Maximally Interacting Layer #\n",
    "        #-----------------------------#\n",
    "        # Initialize Inputs\n",
    "        input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "\n",
    "        #------------------#\n",
    "        #   Core Layers    #\n",
    "        #------------------#\n",
    "        core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "        # Activation\n",
    "        core_layers = tf.nn.swish(core_layers)\n",
    "        # Train additional Depth?\n",
    "        if depth>1:\n",
    "            # Add additional deep layer(s)\n",
    "            for depth_i in range(1,depth):\n",
    "                core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "                # Activation\n",
    "                core_layers = tf.nn.swish(core_layers)\n",
    "\n",
    "        #------------------#\n",
    "        #  Readout Layers  #\n",
    "        #------------------# \n",
    "        # Gaussian Splitter Layer\n",
    "        output_layers = Gaussian_Splitter(output_dim)(core_layers)  \n",
    "        # Define Input/Output Relationship (Arch.)\n",
    "        trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "\n",
    "\n",
    "        #----------------------------------#\n",
    "        # Define Optimizer & Compile Archs.\n",
    "        #----------------------------------#\n",
    "        opt = Adam(lr=learning_rate)\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "        return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "    def build_ffNN_Gaussian(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "        # Update Dictionary\n",
    "        param_grid_in_internal = param_grid_in\n",
    "        param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "        # Deep Feature Network\n",
    "        ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN_Gaussian, \n",
    "                                                                verbose=True)\n",
    "\n",
    "        # Randomized CV\n",
    "        ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                        n_jobs=n_jobs,\n",
    "                                        cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                        param_distributions=param_grid_in_internal,\n",
    "                                        n_iter=n_iter,\n",
    "                                        return_train_score=True,\n",
    "                                        random_state=2020,\n",
    "                                        verbose=10)\n",
    "\n",
    "        # Fit Model #\n",
    "        #-----------#\n",
    "        ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "        # Write Predictions #\n",
    "        #-------------------#\n",
    "        y_hat_train = ffNN_CVer.predict(X_train)\n",
    "\n",
    "        eval_time_ffNN = time.time()\n",
    "        y_hat_test = ffNN_CVer.predict(X_test)\n",
    "        eval_time_ffNN = time.time() - eval_time_ffNN\n",
    "\n",
    "        # Counter number of parameters #\n",
    "        #------------------------------#\n",
    "        # Extract Best Model\n",
    "        best_model = ffNN_CVer.best_estimator_\n",
    "        # Count Number of Parameters\n",
    "        N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "\n",
    "\n",
    "        # Return Values #\n",
    "        #---------------#\n",
    "        return y_hat_train, y_hat_test, N_params_best_ffNN, eval_time_ffNN\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('Deep Feature Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_dim > 1:\n",
    "    def get_ffNN(height, depth, learning_rate, input_dim, output_dim):\n",
    "        #----------------------------#\n",
    "        # Maximally Interacting Layer #\n",
    "        #-----------------------------#\n",
    "        # Initialize Inputs\n",
    "        input_layer = tf.keras.Input(shape=(input_dim,))\n",
    "\n",
    "\n",
    "        #------------------#\n",
    "        #   Core Layers    #\n",
    "        #------------------#\n",
    "        core_layers = fullyConnected_Dense(height)(input_layer)\n",
    "        # Activation\n",
    "        core_layers = tf.nn.swish(core_layers)\n",
    "        # Train additional Depth?\n",
    "        if depth>1:\n",
    "            # Add additional deep layer(s)\n",
    "            for depth_i in range(1,depth):\n",
    "                core_layers = fullyConnected_Dense(height)(core_layers)\n",
    "                # Activation\n",
    "                core_layers = tf.nn.swish(core_layers)\n",
    "\n",
    "        #------------------#\n",
    "        #  Readout Layers  #\n",
    "        #------------------# \n",
    "        # Affine (Readout) Layer (Dense Fully Connected)\n",
    "        output_layers = fullyConnected_Dense(output_dim)(core_layers)  \n",
    "        # Define Input/Output Relationship (Arch.)\n",
    "        trainable_layers_model = tf.keras.Model(input_layer, output_layers)\n",
    "\n",
    "\n",
    "        #----------------------------------#\n",
    "        # Define Optimizer & Compile Archs.\n",
    "        #----------------------------------#\n",
    "        opt = Adam(lr=learning_rate)\n",
    "        trainable_layers_model.compile(optimizer=opt, loss=\"mae\", metrics=[\"mse\", \"mae\", \"mape\"])\n",
    "\n",
    "        return trainable_layers_model\n",
    "\n",
    "\n",
    "\n",
    "    def build_ffNN(n_folds , n_jobs, n_iter, param_grid_in, X_train, y_train,X_test):\n",
    "        # Update Dictionary\n",
    "        param_grid_in_internal = param_grid_in\n",
    "        param_grid_in_internal['input_dim'] = [(X_train.shape[1])]\n",
    "\n",
    "        # Deep Feature Network\n",
    "        ffNN_CV = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn=get_ffNN, \n",
    "                                                                verbose=True)\n",
    "\n",
    "        # Randomized CV\n",
    "        ffNN_CVer = RandomizedSearchCV(estimator=ffNN_CV, \n",
    "                                        n_jobs=n_jobs,\n",
    "                                        cv=KFold(n_folds, random_state=2020, shuffle=True),\n",
    "                                        param_distributions=param_grid_in_internal,\n",
    "                                        n_iter=n_iter,\n",
    "                                        return_train_score=True,\n",
    "                                        random_state=2020,\n",
    "                                        verbose=10)\n",
    "\n",
    "        # Fit Model #\n",
    "        #-----------#\n",
    "        ffNN_CVer.fit(X_train,y_train)\n",
    "\n",
    "        # Write Predictions #\n",
    "        #-------------------#\n",
    "        y_hat_train = ffNN_CVer.predict(X_train)\n",
    "\n",
    "        eval_time_ffNN = time.time()\n",
    "        y_hat_test = ffNN_CVer.predict(X_test)\n",
    "        eval_time_ffNN = time.time() - eval_time_ffNN\n",
    "\n",
    "        # Counter number of parameters #\n",
    "        #------------------------------#\n",
    "        # Extract Best Model\n",
    "        best_model = ffNN_CVer.best_estimator_\n",
    "        # Count Number of Parameters\n",
    "        N_params_best_ffNN = np.sum([np.prod(v.get_shape().as_list()) for v in best_model.model.trainable_variables])\n",
    "\n",
    "\n",
    "        # Return Values #\n",
    "        #---------------#\n",
    "        return y_hat_train, y_hat_test, N_params_best_ffNN, eval_time_ffNN\n",
    "\n",
    "    # Update User\n",
    "    #-------------#\n",
    "    print('DNN Builder - Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRP_time = time.time()\n",
    "GPR_means, GPR_vars, GPR_means_test, GPR_vars_test, GPR_trash, GPR_test_time_prediction = get_GPR(X_train,\n",
    "                                                                                                  X_test,\n",
    "                                                                                                  Y_train_mean_emp) \n",
    "GRP_time = time.time() - GRP_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Deep Gaussian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer Parameters to train on *(for training-set)* for deep Gaussian Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations #\n",
    "#-----------------#\n",
    "print(\"Infering Parameters for Deep Gaussian Network to train on!\")\n",
    "# Start timer:\n",
    "timeBuilding_Training_Set_DGN = time.time()\n",
    "# Set Gaussian Dimension\n",
    "dim_Gaussian_space = output_dim*(1+output_dim)\n",
    "\n",
    "\n",
    "# Get Optimized Parameters to train Deep Gaussian Network On\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    # Define Function Defining log-likelihood of Gaussian dist.\n",
    "    if problem_dim == 1:\n",
    "        ## Count Data-set (outputed-samples) size\n",
    "        n = Y_train.shape[1]\n",
    "        ## Dummy Initialized Parameters\n",
    "        initParams = [1, 1]\n",
    "        def gaussian_log_like(parameters_in):\n",
    "            mean = parameters_in[0]   \n",
    "            sigma = parameters_in[1]\n",
    "\n",
    "            # Calculate negative log likelihood\n",
    "            negative_log_likelihood = -np.sum(stats.norm.logpdf(Y_train[i,], loc=mean, scale=sigma))\n",
    "            return negative_log_likelihood\n",
    "        \n",
    "        # Search for MAE Gaussian Parameters\n",
    "        results_loop = ((minimize(gaussian_log_like, initParams, method='Nelder-Mead')).x).reshape(1,-1)\n",
    "    \n",
    "    else:\n",
    "        # Get Sample Means\n",
    "        mean_loop = np.mean(Y_train[i,],axis=0)\n",
    "        # Get (regularized Cholesky) Squareroot of Sample Covariance\n",
    "        cov_loop = np.tril(np.linalg.cholesky(np.cov(Y_train[i,].T)+(10**-6)*np.diag(np.ones(output_dim)))).reshape(-1,)\n",
    "        \n",
    "        # Coercion\n",
    "        results_loop = np.append(mean_loop,cov_loop).reshape(-1,dim_Gaussian_space)\n",
    "    # Update Targets #\n",
    "    #----------------#\n",
    "    if i == 0:\n",
    "        Y_train_var_emp = results_loop\n",
    "    else:\n",
    "        Y_train_var_emp = np.append(Y_train_var_emp,results_loop,axis=0)\n",
    "# Stop timer:\n",
    "time.Building_Training_Set_DGN = time.time() - timeBuilding_Training_Set_DGN\n",
    "print(\"Done Getting Parameters for Deep Gaussian Network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Deep Network on Infered Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================\")\n",
    "print(\"Training Deep Gaussian Network!\")\n",
    "print(\"===============================\")\n",
    "# Train simple deep classifier\n",
    "timer_DGN = time.time()\n",
    "if output_dim == 1:\n",
    "    # Redefine (Dimension-related) Elements of Grid\n",
    "    param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "    param_grid_Deep_Classifier['output_dim'] = [output_dim]\n",
    "    Deep_Gaussian_train_parameters, Deep_Gaussian_test_parameters, N_params_deep_Gaussian, timer_output_Deep_Gaussian = build_ffNN_Gaussian(n_folds = CV_folds, \n",
    "                                                                                                                                            n_jobs = n_jobs, \n",
    "                                                                                                                                            n_iter = n_iter, \n",
    "                                                                                                                                            param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                                                            X_train = X_train, \n",
    "                                                                                                                                            y_train = Y_train_var_emp,\n",
    "                                                                                                                                            X_test = X_test)\n",
    "else:\n",
    "    # Redefine (Dimension-related) Elements of Grid\n",
    "    param_grid_Deep_Classifier['input_dim'] = [problem_dim]\n",
    "    param_grid_Deep_Classifier['output_dim'] = [Y_train_var_emp.shape[1]]\n",
    "    Deep_Gaussian_train_parameters, Deep_Gaussian_test_parameters, N_params_deep_Gaussian, timer_output_Deep_Gaussian = build_ffNN(n_folds = CV_folds, \n",
    "                                                                                                                                   n_jobs = n_jobs, \n",
    "                                                                                                                                   n_iter = n_iter, \n",
    "                                                                                                                                   param_grid_in=param_grid_Deep_Classifier, \n",
    "                                                                                                                                   X_train = X_train, \n",
    "                                                                                                                                   y_train = Y_train_var_emp,\n",
    "                                                                                                                                   X_test = X_test)\n",
    "# Format as float\n",
    "Deep_Gaussian_train_parameters = np.array(Deep_Gaussian_train_parameters,dtype=float)\n",
    "Deep_Gaussian_test_parameters = np.array(Deep_Gaussian_test_parameters,dtype=float)\n",
    "timer_DGN = time.time() - timer_DGN\n",
    "print(\"====================================\")\n",
    "print(\"Training Deep Gaussian Network!: END\")\n",
    "print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Quality and Prediction Metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Bootstraps = N_Boostraps_BCA\n",
    "print(\"#---------------------------------------#\")\n",
    "print(\" Get Training Errors for: Gaussian Models\")\n",
    "print(\"#---------------------------------------#\")\n",
    "for i in tqdm(range((X_train.shape[0]))):        \n",
    "    if output_dim == 1:\n",
    "        # Get Samples\n",
    "        ## From: Deep Gaussian Network (DGN)\n",
    "        hat_mu_DGaussianNet = Deep_Gaussian_train_parameters[i][0]\n",
    "        hat_sd_DGaussianNet = np.sqrt(Deep_Gaussian_train_parameters[i][1])\n",
    "        sample_DGaussianNet = np.random.normal(hat_mu_DGaussianNet,\n",
    "                                               hat_sd_DGaussianNet,\n",
    "                                               N_Monte_Carlo_Samples)\n",
    "\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP = GPR_means[i]\n",
    "        hat_sd_GPR = np.sqrt(GPR_vars[i])\n",
    "        sample_GRP = np.random.normal(hat_mu_GRP,\n",
    "                                      hat_sd_GPR,\n",
    "                                      N_Monte_Carlo_Samples)\n",
    "\n",
    "        # Compute Error(s)\n",
    "        ## W1\n",
    "        ### DGN\n",
    "        W1_loop_DGN = ot.emd2_1d(sample_DGaussianNet,\n",
    "                                 np.array(Y_train[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ### GPR\n",
    "        W1_loop_GPR = ot.emd2_1d(sample_GRP,\n",
    "                                 np.array(Y_train[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ## M1\n",
    "        Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "        if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "            Mu = direct_facts[i,]\n",
    "        else:\n",
    "            Mu = Mu_MC\n",
    "        ### Error(s)\n",
    "        Mean_loop_DGN = np.sum(np.abs(hat_mu_DGaussianNet-Mu))\n",
    "        Mean_loop_GPR = np.sum(np.abs(hat_mu_GRP-Mu_MC))\n",
    "    else:\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP = GPR_means[i,]\n",
    "        hat_sd_GPR = np.sqrt(GPR_vars[i])\n",
    "        sample_GRP = np.random.multivariate_normal(hat_mu_GRP,\n",
    "                                                   hat_sd_GPR*np.diag(np.ones(output_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate Gaussian Process Regressor's Prediction\n",
    "        sample_GRP = np.random.multivariate_normal(GPR_means[i,],\n",
    "                                                   np.diag(np.repeat(GPR_vars[i],problem_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate deep Gaussian Network's prediction\n",
    "        # Extract Prediction(s)\n",
    "        ## Get Mean\n",
    "        mean_loop = Deep_Gaussian_train_parameters[0,:problem_dim]\n",
    "        ## Get Covariance for Predicted Cholesky Root\n",
    "        cov_sqrt_chol_loop = Deep_Gaussian_train_parameters[0,problem_dim:]\n",
    "        cov_sqrt_chol_loop = cov_sqrt_chol_loop.reshape(output_dim,output_dim)\n",
    "        cov_sqrt_chol_loop = (np.matmul(cov_sqrt_chol_loop,cov_sqrt_chol_loop.T))\n",
    "        ## Get Empirical Samples\n",
    "        sample_DGaussianNet = np.random.multivariate_normal(mean_loop,\n",
    "                                                            cov_sqrt_chol_loop,\n",
    "                                                            N_Monte_Carlo_Samples)\n",
    "        \n",
    "        ## W1\n",
    "        W1_loop_GPR = ot.sliced.sliced_wasserstein_distance(X_s = sample_GRP, \n",
    "                                                            X_t = Y_train[i,],\n",
    "                                                            seed = 2020)\n",
    "        W1_loop_DGN = ot.sliced.sliced_wasserstein_distance(X_s = sample_DGaussianNet, \n",
    "                                                            X_t = Y_train[i,],\n",
    "                                                            seed = 2020)\n",
    "        \n",
    "        ## M1\n",
    "        Mu_MC = np.mean(np.array(Y_train[i,]))\n",
    "        Mu = Mu_MC\n",
    "        Mean_loop_GPR = np.sum(np.abs(hat_mu_GRP-Mu_MC))\n",
    "        Mean_loop_DGN = np.sum(np.abs(cov_sqrt_chol_loop-Mu_MC))\n",
    "   \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_Errors_GPR = W1_loop_GPR\n",
    "        W1_Errors_DGN = W1_loop_DGN\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR =  Mean_loop_GPR\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN =  Mean_loop_DGN\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_Errors_GPR = np.append(W1_Errors_GPR,W1_loop_GPR)\n",
    "        W1_Errors_DGN = np.append(W1_Errors_DGN,W1_loop_DGN)\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR =  np.append(Mean_Errors_GPR,Mean_loop_GPR)\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN =  np.append(Mean_Errors_DGN,Mean_loop_DGN)\n",
    "        \n",
    "    \n",
    "# Compute Error Metrics with Bootstrapped Confidence Intervals\n",
    "W1_Errors_GPR = np.array(bootstrap(np.abs(W1_Errors_GPR),n=N_Bootstraps)(.95))\n",
    "W1_Errors_DGN = np.array(bootstrap(np.abs(W1_Errors_DGN),n=N_Bootstraps)(.95))\n",
    "M1_Errors_GPR = np.array(bootstrap(np.abs(Mean_Errors_GPR),n=N_Bootstraps)(.95))\n",
    "M1_Errors_DGN = np.array(bootstrap(np.abs(Mean_Errors_DGN),n=N_Bootstraps)(.95))\n",
    "\n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Bootstraps = N_Boostraps_BCA\n",
    "print(\"#--------------------------------------#\")\n",
    "print(\" Get Testing Errors for: Gaussian Models\")\n",
    "print(\"#--------------------------------------#\")\n",
    "for i in tqdm(range((X_test.shape[0]))):        \n",
    "    if output_dim == 1:\n",
    "        # Get Samples\n",
    "        ## From: Deep Gaussian Network (DGN)\n",
    "        hat_mu_DGaussianNet_test = Deep_Gaussian_test_parameters[i,][0]\n",
    "        hat_sd_DGaussianNet_test = np.sqrt(Deep_Gaussian_test_parameters[i,][1])\n",
    "        sample_DGaussianNet_test = np.random.normal(hat_mu_DGaussianNet_test,\n",
    "                                               hat_sd_DGaussianNet_test,\n",
    "                                               N_Monte_Carlo_Samples)\n",
    "\n",
    "        ## From: Gaussian Process Regressor (GPR)\n",
    "        hat_mu_GRP_test = GPR_means_test[i]\n",
    "        hat_sd_GPR_test = np.sqrt(GPR_vars_test[i])\n",
    "        sample_GRP_test = np.random.normal(hat_mu_GRP_test,\n",
    "                                      hat_sd_GPR_test,\n",
    "                                      N_Monte_Carlo_Samples)\n",
    "\n",
    "        # Compute Error(s)\n",
    "        ## W1\n",
    "        ### DGN\n",
    "        W1_loop_DGN_test = ot.emd2_1d(sample_DGaussianNet_test,\n",
    "                                 np.array(Y_test[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ### GPR\n",
    "        W1_loop_GPR_test = ot.emd2_1d(sample_GRP_test,\n",
    "                                 np.array(Y_test[i,]).reshape(-1,),\n",
    "                                 empirical_weights,\n",
    "                                 empirical_weights)\n",
    "        ## M1\n",
    "        Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "        if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "            Mu_test = direct_facts_test[i,]\n",
    "        else:\n",
    "            Mu_test = Mu_MC_test\n",
    "        ### Error(s)\n",
    "        Mean_loop_DGN_test = np.sum(np.abs(hat_mu_DGaussianNet_test-Mu_test))\n",
    "        Mean_loop_GPR_test = np.sum(np.abs(hat_mu_GRP_test-Mu_test))\n",
    "    else:\n",
    "        ## Get Multivariate Gaussian Process Regressor's Prediction\n",
    "        hat_mu_GRP_test = GPR_means_test[i,]\n",
    "        hat_sd_GPR_test = np.sqrt(GPR_vars_test[i])\n",
    "        sample_GRP_test = np.random.multivariate_normal(GPR_means_test[i,],\n",
    "                                                   np.diag(np.repeat(GPR_vars_test[i],problem_dim)),\n",
    "                                                   N_Monte_Carlo_Samples)\n",
    "        ## Get Multivariate deep Gaussian Network's prediction\n",
    "        # Extract Prediction(s)\n",
    "        ## Get Mean\n",
    "        mean_loop = Deep_Gaussian_test_parameters[0,:problem_dim]\n",
    "        ## Get Covariance for Predicted Cholesky Root\n",
    "        cov_sqrt_chol_loop = Deep_Gaussian_test_parameters[0,problem_dim:]\n",
    "        cov_sqrt_chol_loop = cov_sqrt_chol_loop.reshape(output_dim,output_dim)\n",
    "        cov_sqrt_chol_loop = (np.matmul(cov_sqrt_chol_loop,cov_sqrt_chol_loop.T))\n",
    "        ## Get Empirical Samples\n",
    "        sample_DGaussianNet_test = np.random.multivariate_normal(mean_loop,\n",
    "                                                                 cov_sqrt_chol_loop,\n",
    "                                                                 N_Monte_Carlo_Samples)\n",
    "        \n",
    "        ## W1\n",
    "        W1_loop_GPR_test = ot.sliced.sliced_wasserstein_distance(X_s = sample_GRP_test, \n",
    "                                                                 X_t = Y_test[i,],\n",
    "                                                                 seed = 2020)\n",
    "        W1_loop_DGN_test = ot.sliced.sliced_wasserstein_distance(X_s = sample_DGaussianNet_test, \n",
    "                                                                 X_t = Y_test[i,],\n",
    "                                                                 seed = 2020)\n",
    "        \n",
    "        ## M1\n",
    "        Mu_MC_test = np.mean(np.array(Y_test[i,]))\n",
    "        Mu_test = Mu_MC_test\n",
    "        Mean_loop_GPR_test = np.sum(np.abs(hat_mu_GRP_test-Mu_MC_test))\n",
    "        Mean_loop_DGN_test = np.sum(np.abs(cov_sqrt_chol_loop-Mu_MC_test))\n",
    "   \n",
    "    \n",
    "    # Update\n",
    "    if i == 0:\n",
    "        W1_Errors_GPR_test = W1_loop_GPR_test\n",
    "        W1_Errors_DGN_test = W1_loop_DGN_test\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR_test =  Mean_loop_GPR_test\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN_test =  Mean_loop_DGN_test\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        W1_Errors_GPR_test = np.append(W1_Errors_GPR_test,\n",
    "                                       W1_loop_GPR_test)\n",
    "        W1_Errors_DGN_test = np.append(W1_Errors_DGN_test,\n",
    "                                       W1_loop_DGN_test)\n",
    "        # Moments\n",
    "        ## GPR\n",
    "        Mean_Errors_GPR_test =  np.append(Mean_Errors_GPR_test,Mean_loop_GPR_test)\n",
    "        ## DGN\n",
    "        Mean_Errors_DGN_test =  np.append(Mean_Errors_DGN_test,Mean_loop_DGN_test)\n",
    "        \n",
    "    \n",
    "# Compute Error Metrics with Bootstrapped Confidence Intervals\n",
    "W1_Errors_GPR_test = np.array(bootstrap(np.abs(W1_Errors_GPR_test),n=N_Bootstraps)(.95))\n",
    "W1_Errors_DGN_test = np.array(bootstrap(np.abs(W1_Errors_DGN_test),n=N_Bootstraps)(.95))\n",
    "M1_Errors_GPR_test = np.array(bootstrap(np.abs(Mean_Errors_GPR_test),n=N_Bootstraps)(.95))\n",
    "M1_Errors_DGN_test = np.array(bootstrap(np.abs(Mean_Errors_DGN_test),n=N_Bootstraps)(.95))\n",
    "\n",
    "print(\"#-------------------------#\")\n",
    "print(\" Get Training Error(s): END\")\n",
    "print(\"#-------------------------#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Performance Metrics:\n",
    "NB, this means that this script *must* be run after the point-mass benchmarks script!\n",
    "\n",
    "## Update Prediction-Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------------------------------\")\n",
    "print(\"Updating Performance Metrics Dataframe and Saved!\")\n",
    "print(\"-------------------------------------------------\")\n",
    "# Append Gaussian Process Regressor Performance\n",
    "# Train\n",
    "Summary_pred_Qual_models[\"GPR\"] = pd.Series(np.append(np.append(W1_Errors_GPR,\n",
    "                                                                M1_Errors_GPR),\n",
    "                                                         np.array([0,\n",
    "                                                                   GRP_time,\n",
    "                                                                   (GPR_test_time_prediction/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "## Test\n",
    "Summary_pred_Qual_models_test[\"GPR\"] = pd.Series(np.append(np.append(W1_Errors_GPR_test,\n",
    "                                                                M1_Errors_GPR_test),\n",
    "                                                         np.array([0,\n",
    "                                                                   GRP_time,\n",
    "                                                                   (GPR_test_time_prediction/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "# Append Deep Gaussian Network Performance\n",
    "Summary_pred_Qual_models[\"DGN\"] = pd.Series(np.append(np.append(W1_Errors_DGN,\n",
    "                                                                M1_Errors_DGN),\n",
    "                                                      np.array([N_params_deep_Gaussian,\n",
    "                                                                timer_DGN,\n",
    "                                                                (timer_output_Deep_Gaussian/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models.index)\n",
    "## Test\n",
    "Summary_pred_Qual_models_test[\"DGN\"] = pd.Series(np.append(np.append(W1_Errors_DGN_test,\n",
    "                                                                     M1_Errors_DGN_test),\n",
    "                                                           np.array([N_params_deep_Gaussian,\n",
    "                                                                     timer_DGN,\n",
    "                                                                     (timer_output_Deep_Gaussian/Test_Set_PredictionTime_MC)])), index=Summary_pred_Qual_models_test.index)\n",
    "# Update Performance Metrics\n",
    "## Train\n",
    "Summary_pred_Qual_models.to_latex((results_tables_path+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS.tex\"))\n",
    "print(\"Training Results to date:\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "## Test\n",
    "Summary_pred_Qual_models_test.to_latex((results_tables_path+str(f_unknown_mode)+\"Problemdimension\"+str(problem_dim)+\"__SUMMARY_METRICS_test.tex\"))\n",
    "print(\"Test Results to date:\")\n",
    "print(Summary_pred_Qual_models_test)\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Updated Performance Metrics Dataframe and Saved!\")\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Fin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
