{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulator and/or Parser\n",
    "**Description:** *This scripts prepares, preprocesses, and parses and any datasets used in for the main script*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Beginning Data-Parsing/Simulation Phase\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------------------------\")\n",
    "print(\"Beginning Data-Parsing/Simulation Phase\")\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "Deep Feature Builder - Ready\n"
     ]
    }
   ],
   "source": [
    "# trial_run = True\n",
    "\n",
    "# problem_dim = 3\n",
    "\n",
    "\n",
    "# train_test_ratio = .2\n",
    "# N_train_size = 5\n",
    "\n",
    "\n",
    "# ## Monte-Carlo\n",
    "# N_Monte_Carlo_Samples = 10**2\n",
    "# N_Euler_Steps = 50\n",
    "# Hurst_Exponent = .5\n",
    "\n",
    "\n",
    "# # Hyper-parameters of Cover\n",
    "# delta = 0.01\n",
    "# Proportion_per_cluster = .5\n",
    "\n",
    "\n",
    "# # Random DNN\n",
    "# # f_unknown_mode = \"Heteroskedastic_NonLinear_Regression\"\n",
    "\n",
    "# # Random DNN internal noise\n",
    "# # Real-world data version\n",
    "# # f_unknown_mode = \"Extreme_Learning_Machine\"\n",
    "# dataset_option = 'crypto'\n",
    "# N_Random_Features = 10**2\n",
    "# # Simulated Data version\n",
    "# # f_unknown_mode = \"DNN_with_Random_Weights\"\n",
    "# Depth_Bayesian_DNN = 2\n",
    "# width = 20\n",
    "\n",
    "# # Random Dropout applied to trained DNN\n",
    "# # f_unknown_mode = \"DNN_with_Bayesian_Dropout\"\n",
    "# Dropout_rate = 0.1\n",
    "\n",
    "# # Rough SDE (time 1)\n",
    "# f_unknown_mode = \"Rough_SDE\"\n",
    "\n",
    "# # GD with Randomized Input\n",
    "# # f_unknown_mode = \"GD_with_randomized_input\"\n",
    "# GD_epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "# exec(open('Loader.py').read())\n",
    "# # Load Packages/Modules\n",
    "# exec(open('Init_Dump.py').read())\n",
    "# trial_run = True\n",
    "# # Load Hyper-parameter Grid\n",
    "# exec(open('CV_Grid.py').read())\n",
    "# # Load Helper Function(s)\n",
    "# exec(open('Helper_Functions.py').read())\n",
    "# # Architecture Builder\n",
    "# exec(open('Benchmarks_Model_Builder.py').read())\n",
    "# # Import time separately\n",
    "# import time\n",
    "# #os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "# # load dataset\n",
    "# results_path = \"./outputs/models/\"\n",
    "# results_tables_path = \"./outputs/results/\"\n",
    "# raw_data_path_folder = \"./inputs/raw/\"\n",
    "# data_path_folder = \"./inputs/data/\"\n",
    "\n",
    "\n",
    "# ### Set Seed\n",
    "# random.seed(2021)\n",
    "# np.random.seed(2021)\n",
    "# tf.random.set_seed(2021)\n",
    "\n",
    "# N_test_size = int(np.round(N_train_size*train_test_ratio,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Set_PredictionTime_MC = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide on Which Simulator/Parser To Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deciding on Which Simulator/Parser To Load\n"
     ]
    }
   ],
   "source": [
    "print(\"Deciding on Which Simulator/Parser To Load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedastic_NonLinear_Regression:\n",
    "$$\n",
    "Y_x \\sim f(x) + \\text{Laplace}\\left(\\tilde{f}(x),\\|x\\|\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Heteroskedastic_NonLinear_Regression\":\n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    # Define Simulator\n",
    "    def Simulator(x_in):\n",
    "        var = np.sqrt(np.sum(x_in**2))\n",
    "        # Pushforward\n",
    "        f_x = f_unknown(x_in)\n",
    "        # Apply Noise After\n",
    "        noise = np.random.laplace(0,var,N_Monte_Carlo_Samples)\n",
    "        f_x_noise = np.cos(f_x) + noise\n",
    "        return f_x_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Random_Weights\":\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,) \n",
    "        # Feature Map Layer\n",
    "        W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "    #     Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla DNN with MC-Droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"DNN_with_Bayesian_Dropout\":\n",
    "    # Initialize Drouput Parameters\n",
    "    N_Dropout = int(np.maximum(1,round(width*Dropout_rate)))\n",
    "    \n",
    "    #-----------#\n",
    "    # Build DNN #\n",
    "    #-----------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([1,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            # Apply Random Dropout\n",
    "            random_mask_coordinates_i = np.random.choice(range(width),N_Dropout)\n",
    "            random_mask_coordinates_j = np.random.choice(range(width),N_Dropout)\n",
    "            W_internal[random_mask_coordinates_i,random_mask_coordinates_j] = 0\n",
    "            # Apply Dropped-out layer\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (fractional) SDE:\n",
    "$$\n",
    "(x,t)\\mapsto \\frac1{S}\\sum_{s=1}^S\\, \\delta_{X_t^{x:s}}; \n",
    "$$\n",
    "where $H\\in (0,1)$, $\\alpha,\\beta$ are DNNs of correct dimensions, and the $X_t^{x,s}$ are i.i.d. copies of: \n",
    "$$\n",
    "X_t^x \\triangleq x + \\int_0^t\\alpha(s,X_s)ds + \\int_0^t \\beta(s,X_s)dB_s^H\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    #-------------------#\n",
    "    # Build DNN (Drift) #\n",
    "    #-------------------#\n",
    "    W_feature = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout = np.random.uniform(size=np.array([problem_dim,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list = [W_hidden_loop]\n",
    "        else:\n",
    "            W_hidden_list.append(W_hidden_loop)\n",
    "    # Define DNN Applier\n",
    "    def f_unknown_drift(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        return x_internal\n",
    "    \n",
    "    #-----------------#\n",
    "    # Build DNN (Vol) #\n",
    "    #-----------------#\n",
    "    W_feature_vol = np.random.uniform(size=np.array([width,problem_dim]),low=-.5,high=.5)\n",
    "    W_readout_vol = np.random.uniform(size=np.array([problem_dim,width]),low=-.5,high=.5)\n",
    "    # Generate Matrices\n",
    "    for i_weights in range(Depth_Bayesian_DNN):\n",
    "        W_hidden_loop_vol = np.random.uniform(size=np.array([width,width]),low=-.5,high=.5)\n",
    "        if i_weights == 0:\n",
    "            W_hidden_list_vol = [W_hidden_loop_vol]\n",
    "        else:\n",
    "            W_hidden_list_vol.append(W_hidden_loop_vol)\n",
    "    def f_unknown_vol(x):\n",
    "        x_internal = x.reshape(-1,)\n",
    "        x_internal = np.matmul(W_feature,x)\n",
    "        #Deep Layer(s)\n",
    "        for i in range(Depth_Bayesian_DNN):\n",
    "            W_internal = W_hidden_list[i]\n",
    "            x_internal = np.matmul(W_internal,x_internal)\n",
    "            x_internal = np.maximum(0,x_internal)    \n",
    "        # Readout Layer\n",
    "        x_internal = np.matmul(W_readout,x_internal)\n",
    "        x_internal = np.outer(x_internal,x_internal)\n",
    "        x_internal = np.tanh(x_internal)\n",
    "        return x_internal\n",
    "    \n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------#   \n",
    "#------------------------------------------------------------------------------#   \n",
    "# Note: The simulator is a bit more complicated in this case that the others.\n",
    "    def Simulator(x):\n",
    "        #-------------------#\n",
    "        # Initialization(s) #\n",
    "        #-------------------#\n",
    "        x_init = x.reshape(-1,)\n",
    "\n",
    "        #--------------------------------#\n",
    "        # Perform Monte-Carlo Simulation #\n",
    "        #--------------------------------#\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            # (re) Coerce input_data fBM Path\n",
    "            x_internal = x_init\n",
    "            # Get fBM path\n",
    "            for d in range(problem_dim):\n",
    "                fBM_gen_loop = (((FBM(n=N_Euler_Steps, hurst=Hurst_Exponent, length=1, method='daviesharte')).fbm())[1:]).reshape(-1,1)\n",
    "                if d == 0:\n",
    "                    fBM_gen = fBM_gen_loop\n",
    "                else:\n",
    "                    fBM_gen = np.append(fBM_gen,fBM_gen_loop,axis=-1)\n",
    "\n",
    "\n",
    "            #---------------#\n",
    "            # Generate Path #\n",
    "            #---------------#\n",
    "            for t in range(N_Euler_Steps):\n",
    "                # Coerce\n",
    "                x_internal = x_internal.reshape(-1,)\n",
    "                # Evolve Path\n",
    "                drift_update = f_unknown_drift(x_internal)/N_Euler_Steps\n",
    "                vol_update = f_unknown_vol(x_internal)\n",
    "                x_internal = (x_internal + drift_update + np.matmul(vol_update,fBM_gen[t,])).reshape(1,-1,problem_dim)\n",
    "                # Coerce\n",
    "                x_internal = x_internal.reshape(1,-1,problem_dim)\n",
    "                # Update Sample path\n",
    "                if t == 0:\n",
    "                    x_sample_path_loop = x_internal\n",
    "                else:\n",
    "                    x_sample_path_loop = np.append(x_sample_path_loop,x_internal,axis=0)\n",
    "            # Update Sample Path\n",
    "            if i_MC == 0:\n",
    "                x_sample_path = x_sample_path_loop\n",
    "            else:\n",
    "                x_sample_path = np.append(x_sample_path,x_sample_path_loop,axis=1)\n",
    "\n",
    "        #------------------------------------------#\n",
    "        # Get Inputs for These Monte-Carlo Outputs #\n",
    "        #------------------------------------------#\n",
    "        ## Generate Path in time\n",
    "        t_steps = (np.linspace(start = 0, stop = 1, num = N_Euler_Steps)).reshape(-1,1)\n",
    "        ## Generate x paired with this t\n",
    "        x_position_initialization = (np.repeat(x.reshape(1,-1),N_Euler_Steps,axis=0)).reshape(-1,problem_dim)\n",
    "        ## Create (t,x) pairs\n",
    "        X_inputs_to_return = np.append(t_steps,x_position_initialization,axis=1)\n",
    "\n",
    "\n",
    "        #------------------------------------------------------#\n",
    "        # Return Monte-Carlo Sample and Dataset update to User #\n",
    "        #------------------------------------------------------#\n",
    "        return X_inputs_to_return, x_sample_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (fractional) SDE - Vanilla Version:\n",
    "$$\n",
    "(x,t)\\mapsto \\frac1{S}\\sum_{s=1}^S\\, \\delta_{X_t^{x:s}}; \n",
    "$$\n",
    "where $H\\in (0,1)$, $\\alpha,\\beta$ are known \"classical\" functions and the $X_t^{x,s}$ are i.i.d. copies of: \n",
    "$$\n",
    "X_t^x \\triangleq x + \\int_0^t\\alpha(s,X_s)ds + \\int_0^t \\beta(s,X_s)dB_s^H\n",
    ".\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE_Vanilla\": \n",
    "#------------------------------------------------------------------------------#   \n",
    "#------------------------------------------------------------------------------#   \n",
    "# Note: The simulator is a bit more complicated in this case that the others.\n",
    "    def Simulator(x):\n",
    "        #-------------------#\n",
    "        # Initialization(s) #\n",
    "        #-------------------#\n",
    "        x_init = x.reshape(-1,)\n",
    "\n",
    "        #--------------------------------#\n",
    "        # Perform Monte-Carlo Simulation #\n",
    "        #--------------------------------#\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            # (re) Coerce input_data fBM Path\n",
    "            x_internal = x_init\n",
    "            # Get fBM path\n",
    "            for d in range(problem_dim):\n",
    "                fBM_gen_loop = (((FBM(n=N_Euler_Steps, hurst=Hurst_Exponent, length=1, method='daviesharte')).fbm())[1:]).reshape(-1,1)\n",
    "                if d == 0:\n",
    "                    fBM_gen = fBM_gen_loop\n",
    "                else:\n",
    "                    fBM_gen = np.append(fBM_gen,fBM_gen_loop,axis=-1)\n",
    "\n",
    "\n",
    "            #---------------#\n",
    "            # Generate Path #\n",
    "            #---------------#\n",
    "            for t in range(N_Euler_Steps):\n",
    "                # Coerce\n",
    "                x_internal = x_internal.reshape(-1,)\n",
    "                # Evolve Path\n",
    "                drift_update = f_unknown_drift_vanilla(x_internal)/N_Euler_Steps\n",
    "                vol_update = f_unknown_vol_vanilla(x_internal)\n",
    "                x_internal = (x_internal + drift_update + np.matmul(vol_update,fBM_gen[t,])).reshape(1,-1,problem_dim)\n",
    "                # Coerce\n",
    "                x_internal = x_internal.reshape(1,-1,problem_dim)\n",
    "                # Update Sample path\n",
    "                if t == 0:\n",
    "                    x_sample_path_loop = x_internal\n",
    "                else:\n",
    "                    x_sample_path_loop = np.append(x_sample_path_loop,x_internal,axis=0)\n",
    "            # Update Sample Path\n",
    "            if i_MC == 0:\n",
    "                x_sample_path = x_sample_path_loop\n",
    "            else:\n",
    "                x_sample_path = np.append(x_sample_path,x_sample_path_loop,axis=1)\n",
    "\n",
    "        #------------------------------------------#\n",
    "        # Get Inputs for These Monte-Carlo Outputs #\n",
    "        #------------------------------------------#\n",
    "        ## Generate Path in time\n",
    "        t_steps = (np.linspace(start = 0, stop = 1, num = N_Euler_Steps)).reshape(-1,1)\n",
    "        ## Generate x paired with this t\n",
    "        x_position_initialization = (np.repeat(x.reshape(1,-1),N_Euler_Steps,axis=0)).reshape(-1,problem_dim)\n",
    "        ## Create (t,x) pairs\n",
    "        X_inputs_to_return = np.append(t_steps,x_position_initialization,axis=1)\n",
    "\n",
    "\n",
    "        #------------------------------------------------------#\n",
    "        # Return Monte-Carlo Sample and Dataset update to User #\n",
    "        #------------------------------------------------------#\n",
    "        return X_inputs_to_return, x_sample_path\n",
    "    \n",
    "    # Set Model to rough_SDE since the rest of the code is identical in that case:\n",
    "    f_unknown_mode = \"Rough_SDE\"\n",
    "    #Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set/Define: Internal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting/Defining: Internal Parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting/Defining: Internal Parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of outputs space $\\mathcal{Y}=\\mathbb{R}^D$.\n",
    "\n",
    "**Note:** *This is only relevant for (fractional) SDE Example which is multi-dimensional in the output space.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode != \"Rough_SDE\":\n",
    "    output_dim = 1\n",
    "else: \n",
    "    output_dim = problem_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on Testing Set's Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test_size = int(np.round(N_train_size*train_test_ratio,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Decide on Which Type of Data to Get/Simulate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deciding on Which Type of Data to Get/Simulate\n"
     ]
    }
   ],
   "source": [
    "print(\"Deciding on Which Type of Data to Get/Simulate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Inputs (Training & Testing) for: \n",
    "*Non-SDE and non GD with random inputs examples*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode != \"GD_with_randomized_input\":\n",
    "    # Get Training Set\n",
    "    X_train = np.random.uniform(size=np.array([N_train_size,problem_dim]),low=.5,high=1.5)\n",
    "\n",
    "    # Get Testing Set\n",
    "    test_set_indices = np.random.choice(range(X_train.shape[0]),N_test_size)\n",
    "    X_test = X_train[test_set_indices,]\n",
    "    X_test = X_test + np.random.uniform(low=-(delta/np.sqrt(problem_dim)), \n",
    "                                        high = -(delta/np.sqrt(problem_dim)),\n",
    "                                        size = X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relabel if fSDE is used instead\n",
    "**Explanation:** *The \"lowercase x\" is used to highlight that the X is made of time-space pairs: (t,x).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    x_train = X_train\n",
    "    x_test = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prase Inputs for: \n",
    "### Gradient Descent with random initialization:\n",
    "$$\n",
    "Y_x\\triangleq \\hat{f}_{\\theta_T}(x),\\qquad \\theta_{t+1} \\triangleq \\theta_t - \\nabla \\sum_{x\\in \\mathbb{X}} \\|\\hat{f}_{\\theta_t}(x)-f(x)\\|, \\qquad \\theta_0 \\sim N_d(0,1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"GD_with_randomized_input\":\n",
    "    # Auxiliary Initialization(s)\n",
    "    Train_step_proportion = 1-train_test_ratio\n",
    "\n",
    "    \n",
    "    if dataset_option == \"crypto\":\n",
    "        #--------------#\n",
    "        # Prepare Data #\n",
    "        #--------------#\n",
    "        # Read Dataset\n",
    "        crypto_data = pd.read_csv('inputs/data/cryptocurrencies/Cryptos_All_in_one.csv')\n",
    "        # Format Date-Time\n",
    "        crypto_data['Date'] = pd.to_datetime(crypto_data['Date'],infer_datetime_format=True)\n",
    "        crypto_data.set_index('Date', drop=True, inplace=True)\n",
    "        crypto_data.index.names = [None]\n",
    "\n",
    "        # Remove Missing Data\n",
    "        crypto_data = crypto_data[crypto_data.isna().any(axis=1)==False]\n",
    "\n",
    "        # Get Returns\n",
    "        crypto_returns = crypto_data.diff().iloc[1:]\n",
    "\n",
    "        # Parse Regressors from Targets\n",
    "        ## Get Regression Targets\n",
    "        crypto_target_data = pd.DataFrame({'BITCOIN-closing':crypto_returns['BITCOIN-Close']})\n",
    "        ## Get Regressors\n",
    "        crypto_data_returns = crypto_returns.drop('BITCOIN-Close', axis=1)  \n",
    "\n",
    "        #-------------#\n",
    "        # Subset Data #\n",
    "        #-------------#\n",
    "        # Get indices\n",
    "        N_train_step = int(round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        N_test_set = int(crypto_data_returns.shape[0] - round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        # # Get Datasets\n",
    "        X_train = crypto_data_returns[:N_train_step]\n",
    "        X_test = crypto_data_returns[-N_test_set:]\n",
    "\n",
    "        ## Coerce into format used in benchmark model(s)\n",
    "        data_x = X_train\n",
    "        data_x_test = X_test\n",
    "        # Get Targets \n",
    "        data_y = crypto_target_data[:N_train_step]\n",
    "        data_y_test = crypto_target_data[-N_test_set:]\n",
    "\n",
    "        # Scale Data\n",
    "        scaler = StandardScaler()\n",
    "        data_x = scaler.fit_transform(data_x)\n",
    "        data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "        # # Update User\n",
    "        print('#================================================#')\n",
    "        print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "        print('#================================================#')\n",
    "    \n",
    "    if dataset_option == \"SnP\":\n",
    "        #--------------#\n",
    "        # Get S&P Data #\n",
    "        #--------------#\n",
    "        #=# SnP Constituents #=#\n",
    "        # Load Data\n",
    "        snp_data = pd.read_csv('inputs/data/snp500_data/snp500-adjusted-close.csv')\n",
    "        # Format Data\n",
    "        ## Index by Time\n",
    "        snp_data['date'] = pd.to_datetime(snp_data['date'],infer_datetime_format=True)\n",
    "        #-------------------------------------------------------------------------------#\n",
    "\n",
    "        #=# SnP Index #=#\n",
    "        ## Read Regression Target\n",
    "        snp_index_target_data = pd.read_csv('inputs/data/snp500_data/GSPC.csv')\n",
    "        ## Get (Reference) Dates\n",
    "        dates_temp = pd.to_datetime(snp_data['date'],infer_datetime_format=True).tail(600)\n",
    "        ## Format Target\n",
    "        snp_index_target_data = pd.DataFrame({'SnP_Index': snp_index_target_data['Close'],'date':dates_temp.reset_index(drop=True)})\n",
    "        snp_index_target_data['date'] = pd.to_datetime(snp_index_target_data['date'],infer_datetime_format=True)\n",
    "        snp_index_target_data.set_index('date', drop=True, inplace=True)\n",
    "        snp_index_target_data.index.names = [None]\n",
    "        #-------------------------------------------------------------------------------#\n",
    "\n",
    "        ## Get Rid of Rubbish\n",
    "        snp_data.set_index('date', drop=True, inplace=True)\n",
    "        snp_data.index.names = [None]\n",
    "        ## Get Rid of NAs and Expired Trends\n",
    "        snp_data = (snp_data.tail(600)).dropna(axis=1).fillna(0)\n",
    "\n",
    "        # Apple\n",
    "        snp_index_target_data = snp_data[{'AAPL'}]\n",
    "        snp_data = snp_data[{'IBM','QCOM','MSFT','CSCO','ADI','MU','MCHP','NVR','NVDA','GOOGL','GOOG'}]\n",
    "        # Get Return(s)\n",
    "        snp_data_returns = snp_data.diff().iloc[1:]\n",
    "        snp_index_target_data_returns = snp_index_target_data.diff().iloc[1:]\n",
    "        #--------------------------------------------------------#\n",
    "\n",
    "        #-------------#\n",
    "        # Subset Data #\n",
    "        #-------------#\n",
    "        # Get indices\n",
    "        N_train_step = int(round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        N_test_set = int(snp_index_target_data_returns.shape[0] - round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        # # Get Datasets\n",
    "        X_train = snp_data_returns[:N_train_step]\n",
    "        X_test = snp_data_returns[-N_test_set:]\n",
    "        ## Coerce into format used in benchmark model(s)\n",
    "        data_x = X_train\n",
    "        data_x_test = X_test\n",
    "        # Get Targets \n",
    "        data_y = snp_index_target_data_returns[:N_train_step]\n",
    "        data_y_test = snp_index_target_data_returns[-N_test_set:]\n",
    "\n",
    "        # Scale Data\n",
    "        scaler = StandardScaler()\n",
    "        data_x = scaler.fit_transform(data_x)\n",
    "        data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "        # # Update User\n",
    "        print('#================================================#')\n",
    "        print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "        print('#================================================#')\n",
    "\n",
    "    \n",
    "    # # Set First Run to Off\n",
    "    First_run = False\n",
    "\n",
    "#     #-----------#\n",
    "#     # Plot Data #\n",
    "#     #-----------#\n",
    "#     fig = crypto_data_returns.plot(figsize=(16, 16))\n",
    "#     fig.get_legend().remove()\n",
    "#     plt.title(\"Crypto_Market Returns\")\n",
    "\n",
    "#     # SAVE Figure to .eps\n",
    "#     plt.savefig('./outputs/plots/'+str(dataset_option)+'_returns.pdf', format='pdf')\n",
    "\n",
    "    # Redefine Meta-Parameters #\n",
    "    #--------------------------#\n",
    "    # Redefine Training Set inputs and ys to train DNN:\n",
    "    data_y_to_train_DNN_on = (data_y.to_numpy()).reshape(-1,)\n",
    "    X_train = data_x\n",
    "    X_test = data_x_test\n",
    "    problem_dim=data_x.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize Target Function #\n",
    "    #----------------------------#\n",
    "    # Initialize DNN to train\n",
    "    f_model = get_ffNN(width, Depth_Bayesian_DNN, 0.001, problem_dim, 1)\n",
    "\n",
    "    # Define Stochastic Prediction Function:\n",
    "    def f_unknown():\n",
    "        f_model.fit(data_x,data_y_to_train_DNN_on,epochs = GD_epochs)\n",
    "        f_x_trained_with_random_initialization_x_train = f_model.predict(X_train)\n",
    "        f_x_trained_with_random_initialization_x_test = f_model.predict(X_test)\n",
    "        return f_x_trained_with_random_initialization_x_train, f_x_trained_with_random_initialization_x_test\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Learning-Machine Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Extreme_Learning_Machine\":\n",
    "    # Auxiliary Initialization(s)\n",
    "    Train_step_proportion = 1-train_test_ratio\n",
    "    \n",
    "    # Vectorized Sigmoid\n",
    "    \n",
    "    # custom function\n",
    "    def sigmoid_univariate(x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    sigmoid = np.vectorize(sigmoid_univariate)\n",
    "    \n",
    "    # Get Data\n",
    "    if dataset_option == \"crypto\":\n",
    "        #--------------#\n",
    "        # Prepare Data #\n",
    "        #--------------#\n",
    "        # Read Dataset\n",
    "        crypto_data = pd.read_csv('inputs/data/cryptocurrencies/Cryptos_All_in_one.csv')\n",
    "        # Format Date-Time\n",
    "        crypto_data['Date'] = pd.to_datetime(crypto_data['Date'],infer_datetime_format=True)\n",
    "        crypto_data.set_index('Date', drop=True, inplace=True)\n",
    "        crypto_data.index.names = [None]\n",
    "\n",
    "        # Remove Missing Data\n",
    "        crypto_data = crypto_data[crypto_data.isna().any(axis=1)==False]\n",
    "\n",
    "        # Get Returns\n",
    "        crypto_returns = crypto_data.diff().iloc[1:]\n",
    "\n",
    "        # Parse Regressors from Targets\n",
    "        ## Get Regression Targets\n",
    "        crypto_target_data = pd.DataFrame({'BITCOIN-closing':crypto_returns['BITCOIN-Close']})\n",
    "        ## Get Regressors\n",
    "        crypto_data_returns = crypto_returns.drop('BITCOIN-Close', axis=1)  \n",
    "\n",
    "        #-------------#\n",
    "        # Subset Data #\n",
    "        #-------------#\n",
    "        # Get indices\n",
    "        N_train_step = int(round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        N_test_set = int(crypto_data_returns.shape[0] - round(crypto_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        # # Get Datasets\n",
    "        X_train = crypto_data_returns[:N_train_step]\n",
    "        X_test = crypto_data_returns[-N_test_set:]\n",
    "\n",
    "        ## Coerce into format used in benchmark model(s)\n",
    "        data_x = X_train\n",
    "        data_x_test = X_test\n",
    "        # Get Targets \n",
    "        data_y = crypto_target_data[:N_train_step]\n",
    "        data_y_test = crypto_target_data[-N_test_set:]\n",
    "\n",
    "        # Scale Data\n",
    "        scaler = StandardScaler()\n",
    "        data_x = scaler.fit_transform(data_x)\n",
    "        data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "        # # Update User\n",
    "        print('#================================================#')\n",
    "        print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "        print('#================================================#')\n",
    "    \n",
    "    if dataset_option == \"SnP\":\n",
    "        #--------------#\n",
    "        # Get S&P Data #\n",
    "        #--------------#\n",
    "        #=# SnP Constituents #=#\n",
    "        # Load Data\n",
    "        snp_data = pd.read_csv('inputs/data/snp500_data/snp500-adjusted-close.csv')\n",
    "        # Format Data\n",
    "        ## Index by Time\n",
    "        snp_data['date'] = pd.to_datetime(snp_data['date'],infer_datetime_format=True)\n",
    "        #-------------------------------------------------------------------------------#\n",
    "\n",
    "        #=# SnP Index #=#\n",
    "        ## Read Regression Target\n",
    "        snp_index_target_data = pd.read_csv('inputs/data/snp500_data/GSPC.csv')\n",
    "        ## Get (Reference) Dates\n",
    "        dates_temp = pd.to_datetime(snp_data['date'],infer_datetime_format=True).tail(600)\n",
    "        ## Format Target\n",
    "        snp_index_target_data = pd.DataFrame({'SnP_Index': snp_index_target_data['Close'],'date':dates_temp.reset_index(drop=True)})\n",
    "        snp_index_target_data['date'] = pd.to_datetime(snp_index_target_data['date'],infer_datetime_format=True)\n",
    "        snp_index_target_data.set_index('date', drop=True, inplace=True)\n",
    "        snp_index_target_data.index.names = [None]\n",
    "        #-------------------------------------------------------------------------------#\n",
    "\n",
    "        ## Get Rid of Rubbish\n",
    "        snp_data.set_index('date', drop=True, inplace=True)\n",
    "        snp_data.index.names = [None]\n",
    "        ## Get Rid of NAs and Expired Trends\n",
    "        snp_data = (snp_data.tail(600)).dropna(axis=1).fillna(0)\n",
    "\n",
    "        # Apple\n",
    "        snp_index_target_data = snp_data[{'AAPL'}]\n",
    "        snp_data = snp_data[{'IBM','QCOM','MSFT','CSCO','ADI','MU','MCHP','NVR','NVDA','GOOGL','GOOG'}]\n",
    "        # Get Return(s)\n",
    "        snp_data_returns = snp_data.diff().iloc[1:]\n",
    "        snp_index_target_data_returns = snp_index_target_data.diff().iloc[1:]\n",
    "        #--------------------------------------------------------#\n",
    "\n",
    "        #-------------#\n",
    "        # Subset Data #\n",
    "        #-------------#\n",
    "        # Get indices\n",
    "        N_train_step = int(round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        N_test_set = int(snp_index_target_data_returns.shape[0] - round(snp_index_target_data_returns.shape[0]*Train_step_proportion,0))\n",
    "        # # Get Datasets\n",
    "        X_train = snp_data_returns[:N_train_step]\n",
    "        X_test = snp_data_returns[-N_test_set:]\n",
    "        ## Coerce into format used in benchmark model(s)\n",
    "        data_x = X_train\n",
    "        data_x_test = X_test\n",
    "        # Get Targets \n",
    "        data_y = snp_index_target_data_returns[:N_train_step]\n",
    "        data_y_test = snp_index_target_data_returns[-N_test_set:]\n",
    "\n",
    "        # Scale Data\n",
    "        scaler = StandardScaler()\n",
    "        data_x = scaler.fit_transform(data_x)\n",
    "        data_x_test = scaler.transform(data_x_test)\n",
    "\n",
    "        # # Update User\n",
    "        print('#================================================#')\n",
    "        print(' Training Datasize: '+str(X_train.shape[0])+' and test datasize: ' + str(X_test.shape[0]) + '.  ')\n",
    "        print('#================================================#')\n",
    "\n",
    "    \n",
    "    # # Set First Run to Off\n",
    "    First_run = False\n",
    "\n",
    "#     #-----------#\n",
    "#     # Plot Data #\n",
    "#     #-----------#\n",
    "#     fig = crypto_data_returns.plot(figsize=(16, 16))\n",
    "#     fig.get_legend().remove()\n",
    "#     plt.title(\"Crypto_Market Returns\")\n",
    "\n",
    "#     # SAVE Figure to .eps\n",
    "#     plt.savefig('./outputs/plots/'+str(dataset_option)+'_returns.pdf', format='pdf')\n",
    "\n",
    "    # Redefine Meta-Parameters #\n",
    "    #--------------------------#\n",
    "    # Redefine Training Set inputs and ys to train DNN:\n",
    "    data_y_to_train_DNN_on = (data_y.to_numpy()).reshape(-1,)\n",
    "    X_train = data_x\n",
    "    X_test = data_x_test\n",
    "    problem_dim=data_x.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize Target Function #\n",
    "    #----------------------------#\n",
    "    # Initialize DNN to train\n",
    "    f_model = get_ffNN(width, Depth_Bayesian_DNN, 0.001, problem_dim, 1)\n",
    "\n",
    "    # Define Stochastic Prediction Function:\n",
    "    def f_unknown():\n",
    "        f_model.fit(data_x,data_y_to_train_DNN_on,epochs = GD_epochs)\n",
    "        f_x_trained_with_random_initialization_x_train = f_model.predict(X_train)\n",
    "        f_x_trained_with_random_initialization_x_test = f_model.predict(X_test)\n",
    "        return f_x_trained_with_random_initialization_x_train, f_x_trained_with_random_initialization_x_test\n",
    "\n",
    "    def Simulator(x_in):\n",
    "        for i_MC in range(N_Monte_Carlo_Samples):\n",
    "            y_MC_loop = f_unknown(x_in)\n",
    "            if i_MC == 0:\n",
    "                y_MC = y_MC_loop\n",
    "            else:\n",
    "                y_MC = np.append(y_MC,y_MC_loop)\n",
    "        return y_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating Output Data for given input data\n"
     ]
    }
   ],
   "source": [
    "print(\"Simulating Output Data for given input data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get outputs for all cases besides Gradient-Descent or fractional SDEs:\n",
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (f_unknown_mode != \"Rough_SDE\") and (f_unknown_mode != \"GD_with_randomized_input\") and (f_unknown_mode != 'Extreme_Learning_Machine'):\n",
    "    for i in tqdm(range(X_train.shape[0])):\n",
    "        # Put Datum\n",
    "        x_loop = X_train[i,]\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_train = y_loop\n",
    "            Y_train_mean_emp = np.mean(y_loop)\n",
    "    #         Y_train_var_emp = np.mean((y_loop - np.mean(y_loop))**2)\n",
    "        else:\n",
    "            Y_train = np.append(Y_train,y_loop,axis=0)\n",
    "            Y_train_mean_emp = np.append(Y_train_mean_emp,np.mean(y_loop))\n",
    "    #         Y_train_var_emp = np.append(Y_train_var_emp,np.mean((y_loop - np.mean(y_loop))**2))\n",
    "    # Join mean and Variance Training Data\n",
    "    # Y_train_var_emp = np.append(Y_train_mean_emp.reshape(-1,1),Y_train_var_emp.reshape(-1,1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (f_unknown_mode != \"Rough_SDE\") and (f_unknown_mode != \"GD_with_randomized_input\") and (f_unknown_mode != 'Extreme_Learning_Machine'):\n",
    "    # Start Timer\n",
    "    Test_Set_PredictionTime_MC = time.time()\n",
    "\n",
    "    # Generate Data\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        # Put Datum\n",
    "        x_loop = X_test[i,]\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = (Simulator(x_loop)).reshape(1,-1)\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_test = y_loop\n",
    "        else:\n",
    "            Y_test = np.append(Y_test,y_loop,axis=0)\n",
    "\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Cases (Simulated Differently for higher efficiency...since it is possible):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For: \"GD_with_randomized_input\":\n",
    "This variant is more efficient in the case of the gradient-descent with randomized initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implemention of the GD algorithm is more efficient (but this only holds for the GD Monte-Carlo method):\n",
    "if f_unknown_mode == \"GD_with_randomized_input\":\n",
    "    # Start Timer\n",
    "    Test_Set_PredictionTime_MC = time.time()\n",
    "    for j_MC in range(N_Monte_Carlo_Samples):\n",
    "        # MC of SGD\n",
    "        Y_train_loop,Y_test_loop = f_unknown()\n",
    "        # Update Dataset\n",
    "        if j_MC == 0:\n",
    "            Y_train = Y_train_loop\n",
    "            Y_test = Y_test_loop\n",
    "        else:\n",
    "            Y_train = np.append(Y_train,Y_train_loop,axis=1)\n",
    "            Y_test = np.append(Y_test,Y_test_loop,axis=1)\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC\n",
    "    \n",
    "## Get means for mean-prediction models\n",
    "    ## Training\n",
    "    for i in tqdm(range(X_train.shape[0])):\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop = Y_train[i,]\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_train_mean_emp = np.mean(y_loop)\n",
    "        else:\n",
    "            Y_train_mean_emp = np.append(Y_train_mean_emp,np.mean(y_loop))\n",
    "    ## Testing\n",
    "    ### Continue Timer\n",
    "    Test_Set_PredictionTime_MC2 = time.time()\n",
    "    for i in tqdm(range(X_test.shape[0])):\n",
    "        # Product Monte-Carlo Sample for Input\n",
    "        y_loop_test = Y_test[i,]\n",
    "\n",
    "        # Update Dataset\n",
    "        if i == 0:\n",
    "            Y_test_mean_emp = np.mean(y_loop_test)\n",
    "        else:\n",
    "            Y_test_mean_emp = np.append(Y_test_mean_emp,np.mean(y_loop_test))\n",
    "    \n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = (time.time() - Test_Set_PredictionTime_MC2) + Test_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == 'Extreme_Learning_Machine':\n",
    "    \n",
    "    # Initialization(s) #\n",
    "    #-------------------#\n",
    "    # Timer(s)\n",
    "    Test_time_elapse = 0\n",
    "    Train_time_elapse = 0\n",
    "    # Features\n",
    "    X_train_rand_features = X_train\n",
    "    X_test_rand_features = X_test\n",
    "\n",
    "    for j_loop in tqdm(range(N_Monte_Carlo_Samples)):\n",
    "        #--------------------#\n",
    "        ## Perform Learning ##\n",
    "        #--------------------#    \n",
    "        for d_loop in range(Depth_Bayesian_DNN):\n",
    "            # Timer\n",
    "            Update_train_time_elapse = time.time()\n",
    "            # Get Random Features\n",
    "            #---------------------------------------------------------------------------------------------------#\n",
    "            Weights_rand = randsp(m=(X_train_rand_features.shape[1]),n=N_Random_Features,density = 0.75)\n",
    "            biases_rand = np.random.uniform(low=-.5,high=.5,size = N_Random_Features)\n",
    "            ## Get Random Features\n",
    "            #---------------------------------------------------------------------------------------------------#\n",
    "            ### Training\n",
    "            #### Apply Random (hidden) Weights\n",
    "            X_train_rand_features = sparse.csr_matrix.dot(X_train_rand_features,Weights_rand)\n",
    "            #### Apply Random (hidden) Biases\n",
    "            X_train_rand_features = X_train_rand_features + biases_rand\n",
    "            #### Apply Discontinuous (Step) Activation function\n",
    "            if activation_function == 'thresholding':\n",
    "                X_train_rand_features[X_train_rand_features>0] = 1\n",
    "                X_train_rand_features[X_train_rand_features<=0] = 0\n",
    "            else:\n",
    "                X_train_rand_features = sigmoid(X_train_rand_features)\n",
    "            #### Compress\n",
    "            X_train_rand_features = sparse.csr_matrix(X_train_rand_features)\n",
    "            # TIMER\n",
    "            Update_train_time_elapse = time.time() - Update_train_time_elapse\n",
    "            Train_time_elapse = Train_time_elapse + Update_train_time_elapse\n",
    "            #---------------------------------------------------------------------------------------------------#\n",
    "            ### Testing\n",
    "\n",
    "            # TIMER\n",
    "            Update_test_time_elapse = time.time()\n",
    "\n",
    "            #### Apply Random (hidden) Weights\n",
    "            X_test_rand_features = sparse.csr_matrix.dot(X_test_rand_features,Weights_rand) \n",
    "            #### Apply Random (hidden) Biases\n",
    "            X_test_rand_features = X_test_rand_features + biases_rand\n",
    "            #### Apply Discontinuous (Step) Activation function\n",
    "            if activation_function == 'thresholding':\n",
    "                X_test_rand_features[X_test_rand_features>0] = 1\n",
    "                X_test_rand_features[X_test_rand_features<=0] = 0\n",
    "            else:\n",
    "                X_test_rand_features = sigmoid(X_test_rand_features)\n",
    "            #### Compress\n",
    "            X_train_rand_features = sparse.csr_matrix(X_train_rand_features)\n",
    "\n",
    "            # TIMER\n",
    "            Update_test_time_elapse = time.time() - Update_test_time_elapse\n",
    "            Test_time_elapse = Test_time_elapse + Update_test_time_elapse\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------#\n",
    "        # Timer\n",
    "        Update_train_time_elapse = time.time()\n",
    "        # Train Extreme Learning Machine\n",
    "        ExLM_reg = Ridge(alpha=(np.random.uniform(low=0,high=1,size=1)[0]))\n",
    "        ExLM_reg.fit(X_train_rand_features,data_y)\n",
    "        # Get Predictions\n",
    "        ## Training Set\n",
    "        ExLM_predict_train = ExLM_reg.predict(X_train_rand_features)\n",
    "        # TIMER\n",
    "        Update_train_time_elapse = time.time() - Update_train_time_elapse\n",
    "        Train_time_elapse = Train_time_elapse + Update_train_time_elapse\n",
    "\n",
    "\n",
    "\n",
    "        # TIMER\n",
    "        Update_test_time_elapse = time.time()\n",
    "        ## Get Test-Set Prediction(s)\n",
    "        ExLM_predict_test = ExLM_reg.predict(X_test_rand_features)\n",
    "        # TIMER\n",
    "        Update_test_time_elapse = time.time() - Update_test_time_elapse\n",
    "        Test_time_elapse = Test_time_elapse + Update_test_time_elapse\n",
    "\n",
    "\n",
    "        # Update Prediction(s) #\n",
    "        #----------------------#\n",
    "        if j_loop == 0:\n",
    "            Y_train = ExLM_predict_train\n",
    "            Y_test = ExLM_predict_test\n",
    "        else:\n",
    "            Y_train = np.append(Y_train,ExLM_predict_train,axis=1)\n",
    "            Y_test = np.append(Y_test,ExLM_predict_test,axis=1)\n",
    "        \n",
    "    # Update MC Training Time\n",
    "    Train_Set_PredictionTime_MC = Train_time_elapse\n",
    "    Test_Set_PredictionTime_MC = Test_time_elapse\n",
    "    \n",
    "    # Get Mean Training Data\n",
    "    Y_train_mean_emp = np.mean(Y_train,axis=1)\n",
    "    Y_test_mean_emp = np.mean(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for (f)SDE Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    for x_i in tqdm(range(x_train.shape[0])):\n",
    "        # Extrain current initial condition\n",
    "        x_init_loop = x_train[x_i,]\n",
    "        # Monte-Carlo Simulate\n",
    "        X_inputs_to_return_loop, x_sample_path_loop = Simulator(x_init_loop)\n",
    "        \n",
    "        # Update Training dataset (both input(s) and output(s))\n",
    "        if x_i == 0:\n",
    "            # Update Input(s)\n",
    "            X_train = X_inputs_to_return_loop\n",
    "            # Update Output(s)\n",
    "            Y_train = x_sample_path_loop\n",
    "        else:\n",
    "            # Update Input(s)\n",
    "            X_train = np.append(X_train,X_inputs_to_return_loop,axis=0)\n",
    "            # Update Output(s)\n",
    "            Y_train = np.append(Y_train,x_sample_path_loop,axis=0)\n",
    "    # Get Mean Training Data\n",
    "    Y_train_mean_emp = np.mean(Y_train,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time()\n",
    "    for x_i in tqdm(range(x_test.shape[0])):\n",
    "        # Extrain current initial condition\n",
    "        x_init_loop = x_test[x_i,]\n",
    "        # Monte-Carlo Simulate\n",
    "        X_inputs_to_return_loop, x_sample_path_loop = Simulator(x_init_loop)\n",
    "        \n",
    "        # Update Training dataset (both input(s) and output(s))\n",
    "        if x_i == 0:\n",
    "            # Update Input(s)\n",
    "            X_test = X_inputs_to_return_loop\n",
    "            # Update Output(s)\n",
    "            Y_test = x_sample_path_loop\n",
    "        else:\n",
    "            # Update Input(s)\n",
    "            X_test = np.append(X_test,X_inputs_to_return_loop,axis=0)\n",
    "            # Update Output(s)\n",
    "            Y_test = np.append(Y_test,x_sample_path_loop,axis=0)\n",
    "    # Get Testing Mean Data\n",
    "    Y_test_mean_emp = np.mean(Y_test,axis=1)\n",
    "    # End Timer\n",
    "    Test_Set_PredictionTime_MC = time.time() - Test_Set_PredictionTime_MC\n",
    "    \n",
    "    f_unknown_mode = \"GD_with_randomized_input\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if f_unknown_mode == \"Rough_SDE\":\n",
    "    Y_train_mean_emp = np.sum(Y_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------------------------\")\n",
    "print(\"Done Data-Parsing/Simulation Phase\")\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Set_PredictionTime_MC = time.time() - Train_Set_PredictionTime_MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
